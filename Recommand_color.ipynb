{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.imageprocessing import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_codis_color_info =  pd.read_csv(\"data/codis_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "color1 = df_codis_color_info.filter(regex = \"color1_[RGB]\").values\n",
    "color2 = df_codis_color_info.filter(regex = \"color2_[RGB]\").values\n",
    "color3 = df_codis_color_info.filter(regex = \"color3_[RGB]\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = np.concatenate([color1, color2], axis = 1)\n",
    "y = color3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X0_train, X0_test, y_train, y_test = train_test_split(X0, y, test_size = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1114, 6), (2, 6), (1114, 3), (2, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0_train.shape, X0_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change form (sample, timesteps, input_dim)\n",
    "X_train = X0_train.reshape((X0_train.shape[0], 2, 3)) /256\n",
    "X_test = X0_test.reshape((X0_test.shape[0], 2, 3)) /256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change form (sample, timesteps, input_dim)\n",
    "X = X0.reshape((X0.shape[0], 2, 3)) /256\n",
    "y = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling(RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.rand(0)\n",
    "# model = Sequential()\n",
    "# model.add(SimpleRNN(10, input_shape=(2,3)))\n",
    "# model.add(Dense(10, activation=\"relu\"))\n",
    "# model.add(Dense(3, activation=\"linear\"))\n",
    "# model.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(X, y, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history[\"loss\"])\n",
    "# plt.title(\"Loss\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 225.54908553  231.38313609  234.0563475 ]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARdJREFUeJzt3TFqQkEUQFENaewlEAi4EUkfXISF\nkEZJFu8CJlv4YPgj3HM28F4xXKYZZjvGGBuAoJfZCwDMIoBAlgACWQIIZAkgkCWAQJYAAlkCCGQJ\nIJD1usaQ/el7jTEPu3weZ6+wyPX8NXuFRQ7vb5u1Hho5Y//rY3efvcIiP7+3h86YGyCQJYBAlgAC\nWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGRt\nx1q/1gA8GTdAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIE\nEMgSQCBLAIEsAQSyBBDIEkAgSwCBrD/JmxVlgGX7SQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef17436828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 111.74219517  129.04810524  134.95169841]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARpJREFUeJzt3TFqQkEUQFEN2YOdXXaSBWQJVlYh\n9iKktnSfIZX9ZAsfDH+Ee84G3iuGyzTDbMcYYwMQ9DJ7AYBZBBDIEkAgSwCBLAEEsgQQyBJAIEsA\ngSwBBLJe1xjy83tfY8zD9u+X2SsscvzYzV5hkdv3ebPWQyNn7H9dD2+zV1jk6/T50BlzAwSyBBDI\nEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsA\ngaztWOvXGoAn4wYIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWA\nQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQ9QcaNhplPueJRgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1837e048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 242.45129398  243.41465542  245.62925269]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARdJREFUeJzt3cFNQkEUQFEw7kBLkBCLcEcRFMFK\nsAaroNyxhZ9g/pDccxp4bzG5mc1ktmOMsQEIepm9AMAsAghkCSCQJYBAlgACWQIIZAkgkCWAQJYA\nAlmvawz5OH6uMeZhp/Nl9gqL3H+vs1dY5P1tt1nroZEz9r++DvvZKyxy+/l+6Iy5AQJZAghkCSCQ\nJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQNZ2\nrPVrDcCTcQMEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsA\ngSwBBLIEEMgSQCBLAIEsAQSyBBDI+gPjFBRlkmPXKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef183f97f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 98.89863867  98.49581762  67.59914712]\n",
      "[ 146.84449768  145.24125671  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARZJREFUeJzt3TFqQkEUQFEVN2KXwiqFjetJpSTg\ngtxJVqYw2cIH5U/gnrOB94rhMs0w2zHG2AAE7WYvADCLAAJZAghkCSCQJYBAlgACWQIIZAkgkCWA\nQNZ+jSHPx2ONMS87fn7MXmGR8+kwe4VF7vffzVoPjZyx97p83WavsMj3z/WlM+YGCGQJIJAlgECW\nAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWdux\n1q81AP+MGyCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQII\nZAkgkCWAQJYAAlkCCGQJIJAlgEDWH7xPGmUchIIRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef18306978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 228.9212612   229.31030324  230.06082012]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARNJREFUeJzt3bFtQjEUQFGIGCBNmkDHZFBHyQYs\nxWc+s8KXiL6R7jkLvFdYV24s78cYYwcQ9DF7AYBZBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIO\nWwy5nc5bjHnZ8vU5e4VV7o9l9gqrnI7fu60eGjlj/+tyuc5eYZXfv5+XzpgbIJAlgECWAAJZAghk\nCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZO3HVr/W\nALwZN0AgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJA\nIEsAgSwBBLIEEMgSQCBLAIGsJzM4FmUgwgc6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef18491198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 27.72964417  28.16867165  29.4408003 ]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARVJREFUeJzt3cFpQkEUQFG/2E62ImnGBkJSheW4\ncGc/xjomLXww/BHuOQ28txgusxlmGWOMHUDQfvYCALMIIJAlgECWAAJZAghkCSCQJYBAlgACWQII\nZB22GPK8n7cY87Ll4zJ7hVWOp8/ZK6zy/H3stnpo5Iz9r+v1NnuFVb5/vl46Y26AQJYAAlkCCGQJ\nIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQtYyt\nfq0BeDNugECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQ\nJYBAlgACWQIIZAkgkCWAQJYAAll/+/oaZTOJOTIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1951f400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 102.09026727  105.15330308   82.77357539]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARhJREFUeJzt3bFpQzEUQFE7eK9UXiFTBP8B0mQC\nt4H0Jm4cyITKCh9svgz3nAXeK8RFjdB+jDF2AEEvsxcAmEUAgSwBBLIEEMgSQCBLAIEsAQSyBBDI\nEkAg67DFkNvlvMWYu70e32avsMrH5zJ7hVW+v353Wz00csYe6+f6N3uFVU7L+11nzA0QyBJAIEsA\ngSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSy\n9mOrX2sAnowbIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZ\nAghkCSCQJYBAlgACWQIIZAkgkCWAQNY/bEEaZXe4FkEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef195abdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 213.24451349  212.95847512  212.06174483]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAQlJREFUeJzt3cEJQjEUAEEV2xDxrKWJnrQzQauL\nLXxQDLIzDeRBwpJLyHqMMVYAQZvZAwDMIoBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJC1/cUi//LY\nZL87zB5hkefrMXuERY6n08/23hn7rtv9NnuERS7X80d77wYIZAkgkCWAQJYAAlkCCGQJIJAlgECW\nAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZ6/Evv8kAfJkbIJAlgECW\nAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQII\nZAkgkCWAQNYbmN4aZUBAjToAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1963d240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 44.62516146  60.91035908  76.53293723]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARpJREFUeJzt3bFpQzEUQFE7ZDN3bgLJCi5NMo4b\ng8tgcJkRMoL3UVb44PBluOcs8F4hLmqEtmOMsQEIepm9AMAsAghkCSCQJYBAlgACWQIIZAkgkCWA\nQJYAAlmvaww5nS9rjHnYx9t+9gqL7N4Ps1dY5P77s1nroZEz9r+u37fZKyzy+XV86Iy5AQJZAghk\nCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWA\nQNZ2rPVrDcCTcQMEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJA\nIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDI+gMXmRplYpTBqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef19629198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223.19782214  221.73756806  227.9800363 ]\n",
      "[ 118.03396606  117.28910065  119.64029694]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARlJREFUeJzt3bFtQjEUQFFArEGB6CiQKDJC1mGB\nTACUTMQAzOSsgAT5P+ies4CfJevKjeXlGGMsAIJWcw8AMBcBBLIEEMgSQCBLAIEsAQSyBBDIEkAg\nSwCBrPUUi+wOxymWednt53vuEZ6y/zrNPcJTtrvNYqqHRs7Ye90fn3E3ulzPL52xz9glwB8QQCBL\nAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEE\nspZjql9rAP4ZN0AgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEE\nsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIGsX5EDFmXkJtdNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef18402b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 219.14476807  207.72847896  195.61035599]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARtJREFUeJzt3bFtQjEUQFGIsgoDMED2SRFBnyJQ\n0aZnBqQskAHYylnhS0TfSPecBd4rrCs3lrdjjLEBCHqZvQDALAIIZAkgkCWAQJYAAlkCCGQJIJAl\ngECWAAJZr2sMef/8XmPMw27Xy+wVFrn//sxeYZHd/m2z1kMjZ+x/nb7Os1dY5HD8eOiMuQECWQII\nZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAl\ngEDWdqz1aw3Ak3EDBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgS\nQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyPoDYHAaZcacZioAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef181e8748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 240.16676185  236.29868646  228.2055968 ]\n",
      "[ 118.03396606  117.28910065  119.64029694]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARhJREFUeJzt3bFpQzEUQFE7eIAULhPwHuF3KT1I\niiziGE+VmVIrK3yw+TLccxZ4rxAXNUL7McbYAQS9zF4AYBYBBLIEEMgSQCBLAIEsAQSyBBDIEkAg\nSwCBrMMWQ86fH1uMudvX8j57hVWW79vsFVZ5Pb7ttnpo5Iw91u/fafYKq/xcL3edMTdAIEsAgSwB\nBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDI\n2o+tfq0BeDJugECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghk\nCSCQJYBAlgACWQIIZAkgkCWAQJYAAln/p/sWZYVpWtgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef18122390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 210.01039919  213.8024153   219.45957732]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARpJREFUeJzt3cFpQkEUQFENacWVkD5SRRoQ00HU\nJlKJBQjZBJuatPDB8Ee45zTw3mK4zGaY7RhjbACCXmYvADCLAAJZAghkCSCQJYBAlgACWQIIZAkg\nkCWAQNbrGkPeP77WGPOw+/V79gqL3H5+Z6+wyNt+t1nroZEz9r/Op8vsFRY5fh4eOmNugECWAAJZ\nAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkg\nkLUda/1aA/Bk3ACBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQ\nyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSy/gAH2BplKun3IQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1810aac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 171.63467132  171.71727749  172.55031995]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARRJREFUeJzt3cFpQkEUQFGVdJJgRdpF0KokkCYU\nwYVVjS18MPwJ3HMaeG8xXGYzzHaMMTYAQbvZCwDMIoBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJD1\nscaQz6/9GmPe9nw+Zq+wyO16n73CIsfjYbPWQyNn7G/9XH5nr7DI6fz91hlzAwSyBBDIEkAgSwCB\nLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgaztWOvX\nGoB/xg0QyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIE\nEMgSQCBLAIEsAQSyBBDIEkAg6wUmChpl1YjT+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef18380630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 79.96018203  64.6894198   54.49544937]\n",
      "[ 118.03396606  117.28910065  119.64029694]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARhJREFUeJzt3TFqQkEUQFENWUL2LDELSRdwAQGx\ntLCxdgf2lpMtfDD8Ee45G3ivGC7TDLMdY4wNQNDb7AUAZhFAIEsAgSwBBLIEEMgSQCBLAIEsAQSy\nBBDIel9jyO3yu8aYp90P37NXWGR3us5eYZHD8bxZ66GRM/a/fh4fs1dYZP/1+dQZcwMEsgQQyBJA\nIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIGs\n7Vjr1xqAF+MGCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECW\nAAJZAghkCSCQJYBAlgACWQIIZAkgkPUH/xcfZY/6urcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef195a9e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 252.63177493  252.50393049  251.60239967]\n",
      "[ 146.84449768  145.24125671  148.03344727]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARZJREFUeJzt3bFpA0EQQFGdORcoXIFTYaxAoSoT\n2LkjhW5n3cKBzK3gv9fATLB8Nll2GWOMA0DQy+wFAGYRQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQ\nyFr3GPJ7v+0x5mGny3X2Cpt8f/3MXmGTdX097PXQyBn7X2/H99krbPJ5/njojLkBAlkCCGQJIJAl\ngECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBA1jL2\n+rUG4Mm4AQJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBA\nlgACWQIIZAkgkCWAQJYAAlkCCGT9AYu+GmV/J5TAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef182ad5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 34.16685309  32.52647278  29.87602535]\n",
      "[ 146.84451294  145.24125671  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARZJREFUeJzt3cFpQkEUQFEVCwrYg9WIqSTYkqSB\nuHIpQgqZtPDB8Ee45zTw3mK4zGaY7RhjbACCdrMXAJhFAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJA\nIGu/xpD77brGmJd9XS6zV1jk+v0ze4VFHs/fzVoPjZyx//VxOM5eYZHz5+mlM+YGCGQJIJAlgECW\nAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWdux\n1q81AG/GDRDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEE\nsgQQyBJAIEsAgSwBBLIEEMgSQCDrD+eAHWWCTd3dAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef18376630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 127.41901408  130.33978873  129.70422535]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARhJREFUeJzt3cFpQkEUQFEN6cOVpLkQOzCkC3e6\nz0KwgCxiYZMWPhj+CPecBt5bDJfZDLMdY4wNQNDL7AUAZhFAIEsAgSwBBLIEEMgSQCBLAIEsAQSy\nBBDIel1jyG7/tsaYh/3+3GevsMj5cpq9wiJfx8/NWg+NnLH/dbt+z15hkY/D+0NnzA0QyBJAIEsA\ngSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSy\ntmOtX2sAnowbIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZ\nAghkCSCQJYBAlgACWQIIZAkgkCWAQNYfsnsaZfo7964AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef196a2c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 117.07059938  135.94195688  162.28784648]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARhJREFUeJzt3TFqQkEUQFEN7iuFVRZga5sUQUGw\ncA0u0D5tFjHZwgfDH+Ges4H3iuEyzTDbMcbYAAS9zV4AYBYBBLIEEMgSQCBLAIEsAQSyBBDIEkAg\nSwCBrN0aQ34ev2uMedr7x372Coscvq6zV1jkfjtu1npo5Iz9r8vlc/YKi5zO30+dMTdAIEsAgSwB\nBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDI\n2o61fq0BeDFugECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghk\nCSCQJYBAlgACWQIIZAkgkCWAQJYAAll/rTcaZbNw8noAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef197344a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 220.6615991   105.96114865   54.51295045]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARxJREFUeJzt3cFpQkEUQFENacK9fYRUkEbE1BFw\nKbhJBfbgPqQbK5i08MHwR7jnNPDeYrjMZpjtGGNsAIJeZi8AMIsAAlkCCGQJIJAlgECWAAJZAghk\nCSCQJYBA1usaQ35u1zXGPOx0/p69wiJfu/vsFRbZX343az00csb+19v7x+wVFjl+Hh46Y26AQJYA\nAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghk\nCSCQtR1r/VoD8GTcAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSy\nBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLL+ACFEG2WPRzDHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef197bc320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 225.46568332  226.30794702  223.11258278]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARdJREFUeJzt3cFpQkEUQFEVKwi4VawgkAqswDQS\ntAKLsIG0Omnhg+GPcM9p4L3FcJnNMNsxxtgABO1mLwAwiwACWQIIZAkgkCWAQJYAAlkCCGQJIJAl\ngEDWfo0hh4/DGmNe9nm5zl5hkd/nY/YKi5yO581aD42csf/1ffmavcIit/vPS2fMDRDIEkAgSwCB\nLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLK2\nY61fawDejBsgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkC\nCGQJIJAlgECWAAJZAghkCSCQJYBA1h8PxBRl57R0KQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef19858630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 163.55474453  163.35036496  167.76642336]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARRJREFUeJzt3TFqQkEUQFGV7C+pRAs7C8EQK/cr\nWGcN4xY+GP4E7jkbeK8YLtMMsx1jjA1A0G72AgCzCCCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGR9\nrDHk8fxdY8zbPo9fs1dY5H65zl5hkdPpuFnroZEz9rfO+8PsFRb5uX2/dcbcAIEsAQSyBBDIEkAg\nSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIGs71vq1\nBuCfcQMEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwB\nBLIEEMgSQCBLAIEsAQSyBBDIegE7rhplzoelvQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef198c95c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 242.74365704  243.50568679  243.93088364]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARNJREFUeJzt3cFpQkEUQFENbk0J8eMuxbhJ2gja\ngdj52MIHwx/hntPAe4vhMpth9mOMsQMI+pi9AMAsAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlmH\nLYYs5+8txrzs9+cye4VVHo/77BVW+Twed1s9NHLG/tfydZq9wirX299LZ8wNEMgSQCBLAIEsAQSy\nBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsvZjq19r\nAN6MGyCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkg\nkCWAQJYAAlkCCGQJIJAlgEDWE/QkFGV1ZtyaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1a945240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 158.52694851  159.99499098  162.6537768 ]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARdJREFUeJzt3TFKQ0EUQNFE3Fnq4A4EU4dkH7au\nIq2NYGXltiZb+PDDfOWes4H3iuEyzTD7McbYAQQ9bb0AwFYEEMgSQCBLAIEsAQSyBBDIEkAgSwCB\nLAEEsp5nDHk5HmaMWe3947b1Cov8/nxtvcIip7fX3ayHRs7YY31//o89L9fzqjPmBghkCSCQJYBA\nlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAln7\nMevXGoA/xg0QyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwB\nBLIEEMgSQCBLAIEsAQSyBBDIEkAg6w58AhplyHsTbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1a9c0518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 237.52037915  239.45402844  240.92890995]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARhJREFUeJzt3bFtQjEUQFGIMgAoTQJZI+PQUKNQ\ns00WySBM46zwJaJvpHvOAu8V1pUby9sxxtgABL3MXgBgFgEEsgQQyBJAIEsAgSwBBLIEEMgSQCBL\nAIGs1zWGfBw+1xjzsPfTz+wVFvm9fc1eYZG3/W6z1kMjZ+x/nY/32Sss8n29PHTG3ACBLAEEsgQQ\nyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBr\nO9b6tQbgybgBAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAl\ngECWAAJZAghkCSCQJYBAlgACWQIIZP0B7lwVZXa1oGwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1aa4d668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 242.96136457  241.75297986  239.02671599]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARdJREFUeJzt3bFpQzEUQFHbpHbwfmlMUhqnTu11\nMsLHTRbIRsoKH2y+AvecBd4rxEWN0H6MMXYAQYfZCwDMIoBAlgACWQIIZAkgkCWAQJYAAlkCCGQJ\nIJD1ssWQ++1jizEP+/r+nb3CKsvPMnuFVY6vp91WD42csed6ez/PXmGV6+floTPmBghkCSCQJYBA\nlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAln7\nsdWvNQD/jBsgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkC\nCGQJIJAlgECWAAJZAghkCSCQJYBA1h9hFxplEIvBvAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1aabdd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 229.68225806  231.31989247  232.04247312]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARdJREFUeJzt3cFNAzEQQNEsShEsES3kQhE50AMV\nIFB6oIo067SwUqI10n+vgZmD9eWL5WWMMQ4AQS+zFwCYRQCBLAEEsgQQyBJAIEsAgSwBBLIEEMgS\nQCDruMeQdX3bY8zDPj6/Zq+wye3vOnuFTU7r62Gvh0bO2HNdzu+zV9jk5/f7oTPmBghkCSCQJYBA\nlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlnL\n2OvXGoB/xg0QyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwB\nBLIEEMgSQCBLAIEsAQSyBBDIEkAg6w5uRBRl5nAJEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1ab4e4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 219.30563936  224.40914365  233.19076352]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARZJREFUeJzt3TFKQ0EUQNFEXEGatDaCkDp9Utua\nBdiLNu5/soUPP8xX7jkbmFc8LtMMsx9jjB1A0NPWAwBsRQCBLAEEsgQQyBJAIEsAgSwBBLIEEMgS\nQCDrecYhb+frjGNWu7zfth5hkd/Pj61HWOT15bib9dDIjj3W6fA/Hoh9/3yt2jE3QCBLAIEsAQSy\nBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyNqP\nWb/WAPwxboBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkg\nkCWAQJYAAlkCCGQJIJAlgECWAAJZd137FWXCHlX1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1abd7208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 205.15995029  211.48943724  225.28865613]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARlJREFUeJzt3bFpA0EQQFGdcBkGV6DAKHYjClSB\nsHP34VLckco4WLVwIHNr+O81MBMsn02WXcYY4wAQdJy9AMAsAghkCSCQJYBAlgACWQIIZAkgkCWA\nQJYAAlkvewxZ13WPMU/7+P6dvcImP9f32Stscj69HfZ6aOSM/a3L6332Cpt8ft2eOmNugECWAAJZ\nAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkg\nkLWMvX6tAfhn3ACBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQ\nyBJAIEsAgSwBBLIEEMgSQCBLAIEsAQSyHgVUG2U/QNf4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1ac73630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 236.1466285   237.09160305  237.71851145]\n",
      "[ 146.84451294  145.24127197  148.03347778]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUAAAAA2CAYAAAC2nEDCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAARVJREFUeJzt3bFtQjEUQFFAbEBqyo9SZQTKFKyC\nSJ+KzZ0VvkT0jXTPWeC9wrpyY3k/xhg7gKDD7AUAZhFAIEsAgSwBBLIEEMgSQCBLAIEsAQSyBBDI\nOm4xZPm8bjHmZd+3r9krrPL8fc5eYZWP02m31UMjZ+x/LefL7BVWefzcXzpjboBAlgACWQIIZAkg\nkCWAQJYAAlkCCGQJIJAlgECWAAJZAghkCSCQJYBAlgACWQIIZAkgkCWAQJYAAlkCCGQJIJC1H1v9\nWgPwZtwAgSwBBLIEEMgSQCBLAIEsAQSyBBDIEkAgSwCBLAEEsgQQyBJAIEsAgSwBBLIEEMgSQCBL\nAIEsAQSyBBDIEkAgSwCBLAEEsv4AA/MUZXrzRgkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1acf7710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for idx in range(30):\n",
    "#     input_color = np.vstack([color1[idx], color2[idx]])[np.newaxis, :, :]\n",
    "#     color_3_pre = model.predict(input_color)[0]\n",
    "\n",
    "#     true = [color1[idx], color2[idx], color3[idx]]\n",
    "#     predict = [color1[idx], color2[idx], color_3_pre]\n",
    "#     print(color3[idx])\n",
    "#     print(color_3_pre)\n",
    "#     bar_true = plot_colors([0.33, 0.33, 0.33], true, h=50, w=300)\n",
    "#     bar_predict = plot_colors([0.33, 0.33, 0.33], predict, h=50, w=300)\n",
    "    \n",
    "#     plt.figure(figsize = (5, 1))  \n",
    "\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.imshow(bar_true)\n",
    "#     plt.axis('off')\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.imshow(bar_predict)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1116, 6), (1116, 3))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change form\n",
    "X0.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(0)\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(5, input_dim=6, activation = \"relu\"))\n",
    "model1.add(Dense(3, activation = \"linear\"))\n",
    "model1.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X0/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 892 samples, validate on 224 samples\n",
      "Epoch 1/2500\n",
      "892/892 [==============================] - 0s 459us/step - loss: 27045.2374 - val_loss: 26260.7042\n",
      "Epoch 2/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 26994.1952 - val_loss: 26197.5321\n",
      "Epoch 3/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 26917.4652 - val_loss: 26100.9598\n",
      "Epoch 4/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 26807.7013 - val_loss: 25972.9406\n",
      "Epoch 5/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 26672.2569 - val_loss: 25827.0921\n",
      "Epoch 6/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 26520.3756 - val_loss: 25663.4788\n",
      "Epoch 7/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 26351.0132 - val_loss: 25482.3326\n",
      "Epoch 8/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 26164.5951 - val_loss: 25282.7282\n",
      "Epoch 9/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 25960.9156 - val_loss: 25065.2985\n",
      "Epoch 10/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 25739.2459 - val_loss: 24831.9210\n",
      "Epoch 11/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 25501.0933 - val_loss: 24582.0195\n",
      "Epoch 12/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 25247.3255 - val_loss: 24314.5938\n",
      "Epoch 13/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 24978.1809 - val_loss: 24031.9138\n",
      "Epoch 14/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 24693.7716 - val_loss: 23735.3644\n",
      "Epoch 15/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 24396.3321 - val_loss: 23424.5312\n",
      "Epoch 16/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 24085.3141 - val_loss: 23101.7662\n",
      "Epoch 17/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 23761.9103 - val_loss: 22769.0329\n",
      "Epoch 18/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 23428.9161 - val_loss: 22424.0042\n",
      "Epoch 19/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 23085.5728 - val_loss: 22068.6710\n",
      "Epoch 20/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 22732.2336 - val_loss: 21707.1574\n",
      "Epoch 21/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 22372.1529 - val_loss: 21336.4590\n",
      "Epoch 22/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 22004.3718 - val_loss: 20960.7623\n",
      "Epoch 23/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 21631.4646 - val_loss: 20579.1349\n",
      "Epoch 24/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 21252.7893 - val_loss: 20195.5106\n",
      "Epoch 25/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 20871.4335 - val_loss: 19806.6814\n",
      "Epoch 26/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 20485.8032 - val_loss: 19417.1967\n",
      "Epoch 27/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 20100.0894 - val_loss: 19024.6744\n",
      "Epoch 28/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 19712.9224 - val_loss: 18631.5894\n",
      "Epoch 29/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 19325.7738 - val_loss: 18240.0137\n",
      "Epoch 30/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 18938.5922 - val_loss: 17853.2718\n",
      "Epoch 31/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 18556.6464 - val_loss: 17465.6090\n",
      "Epoch 32/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 18174.7297 - val_loss: 17085.4230\n",
      "Epoch 33/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 17799.2582 - val_loss: 16707.2017\n",
      "Epoch 34/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 17426.8837 - val_loss: 16336.6653\n",
      "Epoch 35/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 17060.3392 - val_loss: 15971.7637\n",
      "Epoch 36/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 16699.4904 - val_loss: 15615.8334\n",
      "Epoch 37/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 16347.8099 - val_loss: 15263.7527\n",
      "Epoch 38/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 16001.1639 - val_loss: 14923.9796\n",
      "Epoch 39/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 15664.3473 - val_loss: 14591.4987\n",
      "Epoch 40/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 15335.5714 - val_loss: 14269.0159\n",
      "Epoch 41/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 15016.4039 - val_loss: 13955.8838\n",
      "Epoch 42/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 14706.9152 - val_loss: 13653.2907\n",
      "Epoch 43/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 14408.5273 - val_loss: 13359.4337\n",
      "Epoch 44/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 14116.4894 - val_loss: 13085.2460\n",
      "Epoch 45/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 13842.0882 - val_loss: 12811.3838\n",
      "Epoch 46/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 13571.7694 - val_loss: 12556.2125\n",
      "Epoch 47/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 13316.3952 - val_loss: 12309.4109\n",
      "Epoch 48/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 13071.1332 - val_loss: 12075.2167\n",
      "Epoch 49/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 12837.4658 - val_loss: 11853.4143\n",
      "Epoch 50/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 12613.9017 - val_loss: 11646.4255\n",
      "Epoch 51/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 12404.5973 - val_loss: 11444.6671\n",
      "Epoch 52/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 12205.5703 - val_loss: 11253.7733\n",
      "Epoch 53/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 12015.1133 - val_loss: 11079.3290\n",
      "Epoch 54/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 11838.8432 - val_loss: 10913.2383\n",
      "Epoch 55/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 11669.5692 - val_loss: 10765.0018\n",
      "Epoch 56/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 11518.5715 - val_loss: 10613.3817\n",
      "Epoch 57/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 11367.1353 - val_loss: 10485.9169\n",
      "Epoch 58/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 11234.7710 - val_loss: 10358.6616\n",
      "Epoch 59/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 11105.3517 - val_loss: 10247.2487\n",
      "Epoch 60/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 10989.9066 - val_loss: 10138.6650\n",
      "Epoch 61/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 10877.7232 - val_loss: 10045.2718\n",
      "Epoch 62/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 10777.9033 - val_loss: 9955.9745\n",
      "Epoch 63/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 10683.4573 - val_loss: 9876.6742\n",
      "Epoch 64/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 10599.1495 - val_loss: 9800.1251\n",
      "Epoch 65/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 10518.1580 - val_loss: 9734.5054\n",
      "Epoch 66/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 10445.7394 - val_loss: 9673.4789\n",
      "Epoch 67/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 10378.4395 - val_loss: 9617.7792\n",
      "Epoch 68/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 10316.8128 - val_loss: 9567.2263\n",
      "Epoch 69/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 10261.4798 - val_loss: 9519.8613\n",
      "Epoch 70/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 10207.8985 - val_loss: 9480.7223\n",
      "Epoch 71/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 10161.2050 - val_loss: 9441.9392\n",
      "Epoch 72/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 10117.2416 - val_loss: 9407.2531\n",
      "Epoch 73/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 10078.1792 - val_loss: 9374.3258\n",
      "Epoch 74/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 10039.3241 - val_loss: 9347.4971\n",
      "Epoch 75/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 10005.5288 - val_loss: 9321.5271\n",
      "Epoch 76/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 9975.0308 - val_loss: 9295.6425\n",
      "Epoch 77/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 9943.5208 - val_loss: 9275.9623\n",
      "Epoch 78/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 9917.5598 - val_loss: 9255.4127\n",
      "Epoch 79/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 9891.7715 - val_loss: 9236.4400\n",
      "Epoch 80/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 9868.6937 - val_loss: 9218.0017\n",
      "Epoch 81/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 9844.0338 - val_loss: 9203.3760\n",
      "Epoch 82/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 9822.9039 - val_loss: 9187.5149\n",
      "Epoch 83/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 9802.5706 - val_loss: 9172.0494\n",
      "Epoch 84/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 9782.4117 - val_loss: 9157.7862\n",
      "Epoch 85/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 9762.8067 - val_loss: 9144.6091\n",
      "Epoch 86/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 9744.5129 - val_loss: 9131.6364\n",
      "Epoch 87/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 9727.6973 - val_loss: 9117.8677\n",
      "Epoch 88/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 9709.5654 - val_loss: 9106.0356\n",
      "Epoch 89/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 9692.8565 - val_loss: 9092.7201\n",
      "Epoch 90/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 9674.9373 - val_loss: 9080.6011\n",
      "Epoch 91/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 9658.9637 - val_loss: 9067.9893\n",
      "Epoch 92/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 9642.5251 - val_loss: 9056.0857\n",
      "Epoch 93/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 9626.1238 - val_loss: 9043.8051\n",
      "Epoch 94/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 9611.7829 - val_loss: 9031.9514\n",
      "Epoch 95/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 9594.9359 - val_loss: 9019.4331\n",
      "Epoch 96/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 9578.7732 - val_loss: 9007.4884\n",
      "Epoch 97/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 9563.4641 - val_loss: 8995.1041\n",
      "Epoch 98/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 9547.1594 - val_loss: 8982.7356\n",
      "Epoch 99/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 9532.1538 - val_loss: 8970.9895\n",
      "Epoch 100/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 9516.2816 - val_loss: 8958.6339\n",
      "Epoch 101/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 9500.5930 - val_loss: 8946.0070\n",
      "Epoch 102/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 9484.9108 - val_loss: 8934.0425\n",
      "Epoch 103/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 9469.8826 - val_loss: 8921.9472\n",
      "Epoch 104/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 9454.1684 - val_loss: 8908.9731\n",
      "Epoch 105/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 9438.3471 - val_loss: 8896.1809\n",
      "Epoch 106/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 9422.5419 - val_loss: 8884.0361\n",
      "Epoch 107/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 9406.7090 - val_loss: 8871.1345\n",
      "Epoch 108/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 9391.3006 - val_loss: 8857.9524\n",
      "Epoch 109/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 9374.9153 - val_loss: 8845.6616\n",
      "Epoch 110/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 9360.1166 - val_loss: 8833.7462\n",
      "Epoch 111/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 9343.7609 - val_loss: 8819.5452\n",
      "Epoch 112/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 9328.0208 - val_loss: 8806.3555\n",
      "Epoch 113/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 9311.4612 - val_loss: 8794.1090\n",
      "Epoch 114/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 9295.8023 - val_loss: 8781.2416\n",
      "Epoch 115/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 9279.9977 - val_loss: 8767.3807\n",
      "Epoch 116/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 9264.0882 - val_loss: 8754.1748\n",
      "Epoch 117/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 9247.9465 - val_loss: 8740.5965\n",
      "Epoch 118/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 9231.4820 - val_loss: 8727.8917\n",
      "Epoch 119/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 9214.9846 - val_loss: 8714.0418\n",
      "Epoch 120/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 9198.5477 - val_loss: 8700.7156\n",
      "Epoch 121/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 9182.6215 - val_loss: 8687.3650\n",
      "Epoch 122/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 9165.8726 - val_loss: 8673.1889\n",
      "Epoch 123/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 9149.9193 - val_loss: 8660.2525\n",
      "Epoch 124/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 9133.1047 - val_loss: 8645.5828\n",
      "Epoch 125/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 9116.7282 - val_loss: 8630.8845\n",
      "Epoch 126/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 9099.2464 - val_loss: 8617.5361\n",
      "Epoch 127/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 9082.8641 - val_loss: 8603.9646\n",
      "Epoch 128/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 9066.7981 - val_loss: 8589.3994\n",
      "Epoch 129/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 9049.5304 - val_loss: 8575.7780\n",
      "Epoch 130/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 9032.0144 - val_loss: 8561.2011\n",
      "Epoch 131/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 9015.1781 - val_loss: 8547.5457\n",
      "Epoch 132/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 8997.7631 - val_loss: 8532.8585\n",
      "Epoch 133/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 8980.4835 - val_loss: 8517.9718\n",
      "Epoch 134/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 8963.4393 - val_loss: 8503.5991\n",
      "Epoch 135/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 8946.4605 - val_loss: 8489.2990\n",
      "Epoch 136/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 8928.6195 - val_loss: 8474.0273\n",
      "Epoch 137/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 8910.6675 - val_loss: 8459.7516\n",
      "Epoch 138/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 8893.6459 - val_loss: 8444.6991\n",
      "Epoch 139/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 8875.8428 - val_loss: 8429.9175\n",
      "Epoch 140/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 8857.7982 - val_loss: 8414.8814\n",
      "Epoch 141/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 8841.0561 - val_loss: 8399.5208\n",
      "Epoch 142/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 8822.3212 - val_loss: 8384.5670\n",
      "Epoch 143/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 8803.9809 - val_loss: 8369.3101\n",
      "Epoch 144/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 8785.8332 - val_loss: 8353.7979\n",
      "Epoch 145/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 8767.6287 - val_loss: 8339.2044\n",
      "Epoch 146/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 8749.8931 - val_loss: 8323.4851\n",
      "Epoch 147/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 8731.1286 - val_loss: 8306.8412\n",
      "Epoch 148/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 8714.8567 - val_loss: 8291.9694\n",
      "Epoch 149/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 8693.5111 - val_loss: 8276.1089\n",
      "Epoch 150/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 8675.2369 - val_loss: 8259.7393\n",
      "Epoch 151/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 8656.4963 - val_loss: 8243.3034\n",
      "Epoch 152/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 8637.5006 - val_loss: 8227.2922\n",
      "Epoch 153/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 8618.4396 - val_loss: 8211.5746\n",
      "Epoch 154/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 8599.8971 - val_loss: 8195.2370\n",
      "Epoch 155/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 8581.6485 - val_loss: 8179.7607\n",
      "Epoch 156/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 8561.8198 - val_loss: 8162.0906\n",
      "Epoch 157/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 8541.9859 - val_loss: 8145.9054\n",
      "Epoch 158/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 8522.5140 - val_loss: 8129.7391\n",
      "Epoch 159/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 8504.1564 - val_loss: 8113.1857\n",
      "Epoch 160/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 8484.0820 - val_loss: 8095.9062\n",
      "Epoch 161/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 8463.6205 - val_loss: 8078.9413\n",
      "Epoch 162/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 8443.9374 - val_loss: 8061.9546\n",
      "Epoch 163/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 8423.8044 - val_loss: 8044.6982\n",
      "Epoch 164/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 8403.8636 - val_loss: 8027.6744\n",
      "Epoch 165/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 8384.3819 - val_loss: 8009.9633\n",
      "Epoch 166/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 8364.6513 - val_loss: 7993.7153\n",
      "Epoch 167/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 8344.1710 - val_loss: 7975.3722\n",
      "Epoch 168/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 8322.5803 - val_loss: 7957.9396\n",
      "Epoch 169/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 8303.3746 - val_loss: 7940.2499\n",
      "Epoch 170/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 8281.9433 - val_loss: 7922.4473\n",
      "Epoch 171/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 8261.7119 - val_loss: 7905.0133\n",
      "Epoch 172/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 8240.5863 - val_loss: 7887.1793\n",
      "Epoch 173/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 8220.6367 - val_loss: 7869.3862\n",
      "Epoch 174/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 8198.3805 - val_loss: 7851.4827\n",
      "Epoch 175/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 8177.7294 - val_loss: 7833.3313\n",
      "Epoch 176/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 8156.8373 - val_loss: 7814.6362\n",
      "Epoch 177/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 8135.3000 - val_loss: 7796.2463\n",
      "Epoch 178/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 8114.5096 - val_loss: 7777.9842\n",
      "Epoch 179/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 8092.9645 - val_loss: 7760.0041\n",
      "Epoch 180/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 8071.2251 - val_loss: 7740.7145\n",
      "Epoch 181/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 8049.8285 - val_loss: 7721.9206\n",
      "Epoch 182/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 8028.2149 - val_loss: 7703.2063\n",
      "Epoch 183/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 8007.6861 - val_loss: 7684.1332\n",
      "Epoch 184/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 7984.2522 - val_loss: 7665.5414\n",
      "Epoch 185/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 7962.8247 - val_loss: 7646.9208\n",
      "Epoch 186/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 7941.4195 - val_loss: 7628.1516\n",
      "Epoch 187/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 7918.1656 - val_loss: 7608.1500\n",
      "Epoch 188/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 7895.9948 - val_loss: 7588.6413\n",
      "Epoch 189/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 7873.3647 - val_loss: 7569.1099\n",
      "Epoch 190/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 7851.5612 - val_loss: 7549.8216\n",
      "Epoch 191/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 7828.8901 - val_loss: 7530.2144\n",
      "Epoch 192/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 7806.2700 - val_loss: 7510.6442\n",
      "Epoch 193/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 7783.1349 - val_loss: 7490.3599\n",
      "Epoch 194/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 7761.0938 - val_loss: 7470.5123\n",
      "Epoch 195/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 7737.5725 - val_loss: 7450.1500\n",
      "Epoch 196/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 7714.0601 - val_loss: 7430.1341\n",
      "Epoch 197/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 7691.0941 - val_loss: 7409.9187\n",
      "Epoch 198/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 7667.7397 - val_loss: 7390.1175\n",
      "Epoch 199/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 7644.0740 - val_loss: 7369.9676\n",
      "Epoch 200/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 7620.6694 - val_loss: 7349.5654\n",
      "Epoch 201/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 7597.0821 - val_loss: 7328.8174\n",
      "Epoch 202/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 7573.9662 - val_loss: 7308.5001\n",
      "Epoch 203/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 7549.4374 - val_loss: 7288.0080\n",
      "Epoch 204/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 7526.6705 - val_loss: 7266.9839\n",
      "Epoch 205/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 7502.0222 - val_loss: 7245.8608\n",
      "Epoch 206/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 7481.4013 - val_loss: 7224.2637\n",
      "Epoch 207/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 7453.2621 - val_loss: 7203.5359\n",
      "Epoch 208/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 7429.8236 - val_loss: 7182.6513\n",
      "Epoch 209/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 7405.6877 - val_loss: 7160.7939\n",
      "Epoch 210/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 7380.9648 - val_loss: 7139.4844\n",
      "Epoch 211/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 7355.8078 - val_loss: 7117.7217\n",
      "Epoch 212/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 7330.8058 - val_loss: 7096.2441\n",
      "Epoch 213/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 7306.4037 - val_loss: 7074.0991\n",
      "Epoch 214/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 7280.6625 - val_loss: 7052.1396\n",
      "Epoch 215/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 7255.6094 - val_loss: 7030.1606\n",
      "Epoch 216/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 7230.8162 - val_loss: 7008.6812\n",
      "Epoch 217/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 7205.2592 - val_loss: 6986.2557\n",
      "Epoch 218/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 7179.5485 - val_loss: 6963.8085\n",
      "Epoch 219/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 7154.2587 - val_loss: 6941.1465\n",
      "Epoch 220/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 7128.4750 - val_loss: 6918.8854\n",
      "Epoch 221/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 7102.8938 - val_loss: 6895.9606\n",
      "Epoch 222/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 7076.6380 - val_loss: 6873.6970\n",
      "Epoch 223/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 7051.2866 - val_loss: 6850.1252\n",
      "Epoch 224/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 7024.6705 - val_loss: 6828.1121\n",
      "Epoch 225/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 6998.4809 - val_loss: 6804.6042\n",
      "Epoch 226/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 6971.7445 - val_loss: 6781.2940\n",
      "Epoch 227/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 6945.6869 - val_loss: 6757.9648\n",
      "Epoch 228/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 6919.1884 - val_loss: 6734.5536\n",
      "Epoch 229/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 6892.8525 - val_loss: 6711.5268\n",
      "Epoch 230/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 6865.8347 - val_loss: 6687.7340\n",
      "Epoch 231/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 6838.5893 - val_loss: 6663.9796\n",
      "Epoch 232/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 6811.8768 - val_loss: 6640.4303\n",
      "Epoch 233/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 6784.3401 - val_loss: 6616.4623\n",
      "Epoch 234/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 6757.8806 - val_loss: 6593.1384\n",
      "Epoch 235/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 6729.9969 - val_loss: 6568.7407\n",
      "Epoch 236/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 6702.9772 - val_loss: 6544.6625\n",
      "Epoch 237/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 6675.1938 - val_loss: 6520.3334\n",
      "Epoch 238/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 6647.5230 - val_loss: 6496.1921\n",
      "Epoch 239/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 6620.5250 - val_loss: 6471.9842\n",
      "Epoch 240/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 6593.7642 - val_loss: 6447.3524\n",
      "Epoch 241/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 6564.5930 - val_loss: 6422.4280\n",
      "Epoch 242/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 6536.0170 - val_loss: 6397.9966\n",
      "Epoch 243/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 6509.6031 - val_loss: 6373.1606\n",
      "Epoch 244/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 6480.1212 - val_loss: 6348.6776\n",
      "Epoch 245/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 6454.6588 - val_loss: 6324.3613\n",
      "Epoch 246/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 6426.7923 - val_loss: 6299.9341\n",
      "Epoch 247/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 6397.2784 - val_loss: 6274.5827\n",
      "Epoch 248/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 6368.0120 - val_loss: 6249.8944\n",
      "Epoch 249/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 6341.4852 - val_loss: 6225.0254\n",
      "Epoch 250/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 6312.1408 - val_loss: 6200.5479\n",
      "Epoch 251/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 6283.9052 - val_loss: 6175.2012\n",
      "Epoch 252/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 6256.7133 - val_loss: 6150.9374\n",
      "Epoch 253/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 6227.4990 - val_loss: 6125.6674\n",
      "Epoch 254/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 6199.3313 - val_loss: 6100.6606\n",
      "Epoch 255/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 6170.4066 - val_loss: 6075.6131\n",
      "Epoch 256/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 6141.6507 - val_loss: 6050.1156\n",
      "Epoch 257/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 6113.0523 - val_loss: 6025.1569\n",
      "Epoch 258/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 6085.1079 - val_loss: 5999.2949\n",
      "Epoch 259/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 6055.8924 - val_loss: 5974.4178\n",
      "Epoch 260/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 6027.0039 - val_loss: 5948.8060\n",
      "Epoch 261/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 5997.8500 - val_loss: 5923.2530\n",
      "Epoch 262/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 5968.5239 - val_loss: 5898.0997\n",
      "Epoch 263/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 5941.0490 - val_loss: 5872.0425\n",
      "Epoch 264/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 5910.8640 - val_loss: 5846.1357\n",
      "Epoch 265/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 5881.5652 - val_loss: 5820.0288\n",
      "Epoch 266/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 5852.1625 - val_loss: 5794.2056\n",
      "Epoch 267/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 5822.8537 - val_loss: 5768.3411\n",
      "Epoch 268/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 5793.9045 - val_loss: 5742.7783\n",
      "Epoch 269/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 5764.3920 - val_loss: 5717.0435\n",
      "Epoch 270/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 5734.7592 - val_loss: 5690.3974\n",
      "Epoch 271/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 5706.2493 - val_loss: 5664.2168\n",
      "Epoch 272/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 5675.5078 - val_loss: 5637.7552\n",
      "Epoch 273/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 5646.2560 - val_loss: 5611.8824\n",
      "Epoch 274/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 5616.2015 - val_loss: 5585.6809\n",
      "Epoch 275/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 5587.0470 - val_loss: 5558.9689\n",
      "Epoch 276/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 5556.6553 - val_loss: 5533.3384\n",
      "Epoch 277/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 5527.0655 - val_loss: 5506.5042\n",
      "Epoch 278/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 5497.1497 - val_loss: 5480.3217\n",
      "Epoch 279/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 5467.8583 - val_loss: 5453.8892\n",
      "Epoch 280/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 5437.8723 - val_loss: 5427.8786\n",
      "Epoch 281/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 5409.3869 - val_loss: 5401.1801\n",
      "Epoch 282/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 5380.6541 - val_loss: 5376.1839\n",
      "Epoch 283/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 5350.5857 - val_loss: 5350.0735\n",
      "Epoch 284/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 5319.8340 - val_loss: 5323.3574\n",
      "Epoch 285/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 5290.5703 - val_loss: 5296.9828\n",
      "Epoch 286/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 5261.3530 - val_loss: 5271.3560\n",
      "Epoch 287/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 5232.3222 - val_loss: 5245.3903\n",
      "Epoch 288/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 5202.7151 - val_loss: 5219.3711\n",
      "Epoch 289/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 5173.3192 - val_loss: 5193.6829\n",
      "Epoch 290/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 5145.6418 - val_loss: 5167.9342\n",
      "Epoch 291/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 5114.0433 - val_loss: 5141.8366\n",
      "Epoch 292/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 5085.6276 - val_loss: 5115.0792\n",
      "Epoch 293/2500\n",
      "892/892 [==============================] - 0s 48us/step - loss: 5056.5276 - val_loss: 5088.9786\n",
      "Epoch 294/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 5027.0366 - val_loss: 5063.6417\n",
      "Epoch 295/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 4998.0686 - val_loss: 5037.7802\n",
      "Epoch 296/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 4970.1151 - val_loss: 5012.3286\n",
      "Epoch 297/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 4940.4446 - val_loss: 4987.2691\n",
      "Epoch 298/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 4911.4503 - val_loss: 4961.2793\n",
      "Epoch 299/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 4883.4703 - val_loss: 4935.1959\n",
      "Epoch 300/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 4853.8388 - val_loss: 4909.4717\n",
      "Epoch 301/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 4824.9719 - val_loss: 4883.8512\n",
      "Epoch 302/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 4797.2047 - val_loss: 4858.2177\n",
      "Epoch 303/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 4769.5542 - val_loss: 4834.0165\n",
      "Epoch 304/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 4739.5605 - val_loss: 4807.8284\n",
      "Epoch 305/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 4711.2696 - val_loss: 4782.3308\n",
      "Epoch 306/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 4682.4450 - val_loss: 4757.5468\n",
      "Epoch 307/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 4654.6478 - val_loss: 4732.9524\n",
      "Epoch 308/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 4626.4803 - val_loss: 4708.7459\n",
      "Epoch 309/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 4598.6263 - val_loss: 4682.6491\n",
      "Epoch 310/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 4570.0770 - val_loss: 4657.5864\n",
      "Epoch 311/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 4542.7711 - val_loss: 4634.1737\n",
      "Epoch 312/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 4514.9672 - val_loss: 4608.1839\n",
      "Epoch 313/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 4487.1626 - val_loss: 4583.6953\n",
      "Epoch 314/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 4459.7701 - val_loss: 4559.3213\n",
      "Epoch 315/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 4431.3917 - val_loss: 4535.2853\n",
      "Epoch 316/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 4404.6984 - val_loss: 4509.7586\n",
      "Epoch 317/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 4377.2391 - val_loss: 4485.1809\n",
      "Epoch 318/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 4349.9252 - val_loss: 4462.8710\n",
      "Epoch 319/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 4323.4443 - val_loss: 4437.9849\n",
      "Epoch 320/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 4296.4558 - val_loss: 4414.0783\n",
      "Epoch 321/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 4269.3909 - val_loss: 4391.3163\n",
      "Epoch 322/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 4243.5551 - val_loss: 4367.7142\n",
      "Epoch 323/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 4217.8172 - val_loss: 4343.4744\n",
      "Epoch 324/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 4190.6735 - val_loss: 4320.9192\n",
      "Epoch 325/2500\n",
      "892/892 [==============================] - 0s 48us/step - loss: 4164.9655 - val_loss: 4298.2679\n",
      "Epoch 326/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 4138.5410 - val_loss: 4275.1855\n",
      "Epoch 327/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 4113.0545 - val_loss: 4251.6458\n",
      "Epoch 328/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 4087.1468 - val_loss: 4228.5864\n",
      "Epoch 329/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 4061.7971 - val_loss: 4205.9588\n",
      "Epoch 330/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 4036.2186 - val_loss: 4183.3592\n",
      "Epoch 331/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 4012.1307 - val_loss: 4160.8256\n",
      "Epoch 332/2500\n",
      "892/892 [==============================] - 0s 48us/step - loss: 3986.3177 - val_loss: 4138.2456\n",
      "Epoch 333/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 3961.4039 - val_loss: 4116.5530\n",
      "Epoch 334/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 3936.6717 - val_loss: 4094.2457\n",
      "Epoch 335/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 3912.6526 - val_loss: 4073.0889\n",
      "Epoch 336/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 3888.5249 - val_loss: 4050.3530\n",
      "Epoch 337/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 3864.2626 - val_loss: 4029.6265\n",
      "Epoch 338/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 3840.1793 - val_loss: 4008.8312\n",
      "Epoch 339/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 3817.9967 - val_loss: 3989.4821\n",
      "Epoch 340/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 3794.5397 - val_loss: 3968.2528\n",
      "Epoch 341/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 3770.6612 - val_loss: 3946.8158\n",
      "Epoch 342/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 3747.4781 - val_loss: 3926.4457\n",
      "Epoch 343/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 3725.2069 - val_loss: 3906.4351\n",
      "Epoch 344/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 3702.5856 - val_loss: 3886.9239\n",
      "Epoch 345/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 3680.4711 - val_loss: 3866.0866\n",
      "Epoch 346/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 3658.5288 - val_loss: 3846.3052\n",
      "Epoch 347/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 3637.3244 - val_loss: 3827.7958\n",
      "Epoch 348/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 3615.5431 - val_loss: 3809.2982\n",
      "Epoch 349/2500\n",
      "892/892 [==============================] - 0s 48us/step - loss: 3594.3705 - val_loss: 3789.0829\n",
      "Epoch 350/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 3573.3172 - val_loss: 3771.3966\n",
      "Epoch 351/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 3552.3355 - val_loss: 3753.1204\n",
      "Epoch 352/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 3531.7283 - val_loss: 3734.8522\n",
      "Epoch 353/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 3512.4867 - val_loss: 3715.6655\n",
      "Epoch 354/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 3491.5639 - val_loss: 3697.9301\n",
      "Epoch 355/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 3472.0653 - val_loss: 3680.1355\n",
      "Epoch 356/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 3451.9580 - val_loss: 3663.2662\n",
      "Epoch 357/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 3432.5910 - val_loss: 3646.3751\n",
      "Epoch 358/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 3414.2679 - val_loss: 3629.3543\n",
      "Epoch 359/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 3394.3354 - val_loss: 3612.0905\n",
      "Epoch 360/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 3375.9571 - val_loss: 3595.1380\n",
      "Epoch 361/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 3357.4216 - val_loss: 3579.7164\n",
      "Epoch 362/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 3339.0151 - val_loss: 3563.0533\n",
      "Epoch 363/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 3320.8302 - val_loss: 3546.5772\n",
      "Epoch 364/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 3303.1068 - val_loss: 3530.6906\n",
      "Epoch 365/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 3285.6058 - val_loss: 3515.0662\n",
      "Epoch 366/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 3269.0739 - val_loss: 3498.7220\n",
      "Epoch 367/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 3251.7119 - val_loss: 3483.1061\n",
      "Epoch 368/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 3234.1975 - val_loss: 3469.4870\n",
      "Epoch 369/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 3217.9866 - val_loss: 3454.0985\n",
      "Epoch 370/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 3201.5606 - val_loss: 3440.0701\n",
      "Epoch 371/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 3185.8813 - val_loss: 3426.0581\n",
      "Epoch 372/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 3169.9337 - val_loss: 3411.1207\n",
      "Epoch 373/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 3153.8241 - val_loss: 3398.1892\n",
      "Epoch 374/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 3139.0544 - val_loss: 3383.0702\n",
      "Epoch 375/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 3124.2116 - val_loss: 3371.2177\n",
      "Epoch 376/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 3109.7595 - val_loss: 3357.0896\n",
      "Epoch 377/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 3093.9505 - val_loss: 3344.0183\n",
      "Epoch 378/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 3082.3592 - val_loss: 3328.8178\n",
      "Epoch 379/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 3066.2860 - val_loss: 3319.4602\n",
      "Epoch 380/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 3052.1159 - val_loss: 3306.6867\n",
      "Epoch 381/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 3039.3399 - val_loss: 3293.7657\n",
      "Epoch 382/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 3025.9074 - val_loss: 3283.0989\n",
      "Epoch 383/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 3013.0928 - val_loss: 3270.4452\n",
      "Epoch 384/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 3000.3819 - val_loss: 3260.4252\n",
      "Epoch 385/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2986.8615 - val_loss: 3248.2631\n",
      "Epoch 386/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2974.8051 - val_loss: 3236.9764\n",
      "Epoch 387/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2963.0141 - val_loss: 3226.8214\n",
      "Epoch 388/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2952.3203 - val_loss: 3214.6923\n",
      "Epoch 389/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2939.6290 - val_loss: 3206.5034\n",
      "Epoch 390/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2928.5194 - val_loss: 3196.9837\n",
      "Epoch 391/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2917.7162 - val_loss: 3184.3504\n",
      "Epoch 392/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2907.0270 - val_loss: 3175.6527\n",
      "Epoch 393/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2895.0670 - val_loss: 3166.7816\n",
      "Epoch 394/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2884.4554 - val_loss: 3155.8202\n",
      "Epoch 395/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2875.1014 - val_loss: 3147.7475\n",
      "Epoch 396/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2864.6526 - val_loss: 3136.9593\n",
      "Epoch 397/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2854.3021 - val_loss: 3128.9940\n",
      "Epoch 398/2500\n",
      "892/892 [==============================] - 0s 146us/step - loss: 2844.6163 - val_loss: 3120.4426\n",
      "Epoch 399/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2834.8972 - val_loss: 3111.8856\n",
      "Epoch 400/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2825.5635 - val_loss: 3103.3342\n",
      "Epoch 401/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2816.7982 - val_loss: 3096.2385\n",
      "Epoch 402/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2808.6975 - val_loss: 3086.6430\n",
      "Epoch 403/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2799.3214 - val_loss: 3080.7180\n",
      "Epoch 404/2500\n",
      "892/892 [==============================] - 0s 125us/step - loss: 2791.0120 - val_loss: 3072.4915\n",
      "Epoch 405/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2782.7290 - val_loss: 3064.9526\n",
      "Epoch 406/2500\n",
      "892/892 [==============================] - 0s 152us/step - loss: 2774.7446 - val_loss: 3058.3673\n",
      "Epoch 407/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2767.1765 - val_loss: 3050.4220\n",
      "Epoch 408/2500\n",
      "892/892 [==============================] - 0s 147us/step - loss: 2759.3215 - val_loss: 3043.6412\n",
      "Epoch 409/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2751.7393 - val_loss: 3038.1229\n",
      "Epoch 410/2500\n",
      "892/892 [==============================] - 0s 147us/step - loss: 2744.4233 - val_loss: 3031.6656\n",
      "Epoch 411/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2738.6861 - val_loss: 3027.1369\n",
      "Epoch 412/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2730.4713 - val_loss: 3020.2621\n",
      "Epoch 413/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2725.0595 - val_loss: 3010.9138\n",
      "Epoch 414/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2718.3050 - val_loss: 3006.7049\n",
      "Epoch 415/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2712.2154 - val_loss: 2999.8277\n",
      "Epoch 416/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2705.3044 - val_loss: 2996.7644\n",
      "Epoch 417/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2698.8839 - val_loss: 2990.0049\n",
      "Epoch 418/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2692.8817 - val_loss: 2985.2668\n",
      "Epoch 419/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2687.8002 - val_loss: 2980.2804\n",
      "Epoch 420/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2681.7978 - val_loss: 2973.3067\n",
      "Epoch 421/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2676.3934 - val_loss: 2971.2095\n",
      "Epoch 422/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2671.5575 - val_loss: 2966.2160\n",
      "Epoch 423/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2665.7559 - val_loss: 2960.5972\n",
      "Epoch 424/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2661.9078 - val_loss: 2955.1942\n",
      "Epoch 425/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2656.3550 - val_loss: 2952.9404\n",
      "Epoch 426/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2652.5684 - val_loss: 2950.0741\n",
      "Epoch 427/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2646.9050 - val_loss: 2943.8125\n",
      "Epoch 428/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2642.8714 - val_loss: 2940.8592\n",
      "Epoch 429/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 2639.3056 - val_loss: 2935.7607\n",
      "Epoch 430/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2634.7503 - val_loss: 2933.0991\n",
      "Epoch 431/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2630.1649 - val_loss: 2929.1253\n",
      "Epoch 432/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2626.5776 - val_loss: 2924.9968\n",
      "Epoch 433/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2623.5888 - val_loss: 2919.9513\n",
      "Epoch 434/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2618.5212 - val_loss: 2919.9407\n",
      "Epoch 435/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2617.8607 - val_loss: 2920.0140\n",
      "Epoch 436/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2612.0786 - val_loss: 2912.0017\n",
      "Epoch 437/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2608.2887 - val_loss: 2910.2229\n",
      "Epoch 438/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2605.7104 - val_loss: 2908.3623\n",
      "Epoch 439/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2601.9914 - val_loss: 2904.5085\n",
      "Epoch 440/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2599.0002 - val_loss: 2900.6761\n",
      "Epoch 441/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2596.0783 - val_loss: 2899.8615\n",
      "Epoch 442/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2592.6363 - val_loss: 2896.9211\n",
      "Epoch 443/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2589.9605 - val_loss: 2894.7768\n",
      "Epoch 444/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2587.1719 - val_loss: 2892.1698\n",
      "Epoch 445/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2586.3353 - val_loss: 2888.8981\n",
      "Epoch 446/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2581.8160 - val_loss: 2887.5060\n",
      "Epoch 447/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2579.2966 - val_loss: 2885.7428\n",
      "Epoch 448/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2577.5539 - val_loss: 2882.4825\n",
      "Epoch 449/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2574.1063 - val_loss: 2882.6501\n",
      "Epoch 450/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2571.7244 - val_loss: 2880.9371\n",
      "Epoch 451/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2569.6172 - val_loss: 2879.6818\n",
      "Epoch 452/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2567.2188 - val_loss: 2877.0772\n",
      "Epoch 453/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2565.0297 - val_loss: 2875.6690\n",
      "Epoch 454/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2563.5423 - val_loss: 2871.7462\n",
      "Epoch 455/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2560.8254 - val_loss: 2872.7742\n",
      "Epoch 456/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2559.1554 - val_loss: 2870.8347\n",
      "Epoch 457/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2557.0103 - val_loss: 2867.9061\n",
      "Epoch 458/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2554.5318 - val_loss: 2867.7277\n",
      "Epoch 459/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2552.8932 - val_loss: 2865.4497\n",
      "Epoch 460/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2551.6079 - val_loss: 2863.3617\n",
      "Epoch 461/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2549.4137 - val_loss: 2862.4181\n",
      "Epoch 462/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2547.3672 - val_loss: 2861.2972\n",
      "Epoch 463/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2545.5986 - val_loss: 2860.8400\n",
      "Epoch 464/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2543.8411 - val_loss: 2858.9528\n",
      "Epoch 465/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2542.2327 - val_loss: 2858.5491\n",
      "Epoch 466/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2540.6914 - val_loss: 2859.6059\n",
      "Epoch 467/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2538.5319 - val_loss: 2854.6897\n",
      "Epoch 468/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2537.1663 - val_loss: 2856.2484\n",
      "Epoch 469/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2535.7384 - val_loss: 2852.0801\n",
      "Epoch 470/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2534.7217 - val_loss: 2854.6912\n",
      "Epoch 471/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2531.6115 - val_loss: 2849.8160\n",
      "Epoch 472/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2529.6665 - val_loss: 2849.3793\n",
      "Epoch 473/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2528.4485 - val_loss: 2849.5452\n",
      "Epoch 474/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2526.5898 - val_loss: 2847.2079\n",
      "Epoch 475/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2524.9218 - val_loss: 2845.0289\n",
      "Epoch 476/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2524.7160 - val_loss: 2849.0075\n",
      "Epoch 477/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2521.9274 - val_loss: 2846.3056\n",
      "Epoch 478/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2519.1130 - val_loss: 2841.9417\n",
      "Epoch 479/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2517.8668 - val_loss: 2839.1354\n",
      "Epoch 480/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2515.9948 - val_loss: 2837.1785\n",
      "Epoch 481/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2514.3120 - val_loss: 2837.9298\n",
      "Epoch 482/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2512.9431 - val_loss: 2834.8461\n",
      "Epoch 483/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2511.3995 - val_loss: 2833.3328\n",
      "Epoch 484/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2509.8002 - val_loss: 2836.1156\n",
      "Epoch 485/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2508.0274 - val_loss: 2833.7185\n",
      "Epoch 486/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2506.8325 - val_loss: 2829.2446\n",
      "Epoch 487/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2506.0892 - val_loss: 2829.6121\n",
      "Epoch 488/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2504.7418 - val_loss: 2827.1898\n",
      "Epoch 489/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2503.7525 - val_loss: 2824.8677\n",
      "Epoch 490/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2502.5406 - val_loss: 2823.8141\n",
      "Epoch 491/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2501.4173 - val_loss: 2825.3514\n",
      "Epoch 492/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2500.1340 - val_loss: 2823.9139\n",
      "Epoch 493/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2499.1667 - val_loss: 2824.6259\n",
      "Epoch 494/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2499.5371 - val_loss: 2820.0089\n",
      "Epoch 495/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2497.0133 - val_loss: 2820.1487\n",
      "Epoch 496/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2495.9400 - val_loss: 2820.0287\n",
      "Epoch 497/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2495.7473 - val_loss: 2816.9596\n",
      "Epoch 498/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2495.7720 - val_loss: 2819.0831\n",
      "Epoch 499/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2493.6448 - val_loss: 2818.2523\n",
      "Epoch 500/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2492.6159 - val_loss: 2814.3174\n",
      "Epoch 501/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2491.6551 - val_loss: 2815.9296\n",
      "Epoch 502/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2490.8393 - val_loss: 2813.7650\n",
      "Epoch 503/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2489.9199 - val_loss: 2814.7618\n",
      "Epoch 504/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2489.3469 - val_loss: 2811.6498\n",
      "Epoch 505/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2489.1308 - val_loss: 2812.8580\n",
      "Epoch 506/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2487.4882 - val_loss: 2809.6740\n",
      "Epoch 507/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2487.1644 - val_loss: 2809.2669\n",
      "Epoch 508/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2486.0625 - val_loss: 2810.3227\n",
      "Epoch 509/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2485.4760 - val_loss: 2809.5701\n",
      "Epoch 510/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2485.1734 - val_loss: 2810.0244\n",
      "Epoch 511/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2484.7021 - val_loss: 2807.1417\n",
      "Epoch 512/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2484.6390 - val_loss: 2807.0256\n",
      "Epoch 513/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 2483.8150 - val_loss: 2805.7372\n",
      "Epoch 514/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2482.8627 - val_loss: 2805.4091\n",
      "Epoch 515/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2483.2491 - val_loss: 2804.2550\n",
      "Epoch 516/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2482.0545 - val_loss: 2805.5898\n",
      "Epoch 517/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2481.6884 - val_loss: 2805.9786\n",
      "Epoch 518/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2481.0746 - val_loss: 2804.0130\n",
      "Epoch 519/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2481.5719 - val_loss: 2801.9087\n",
      "Epoch 520/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2480.7199 - val_loss: 2803.2918\n",
      "Epoch 521/2500\n",
      "892/892 [==============================] - 0s 160us/step - loss: 2480.3204 - val_loss: 2800.6954\n",
      "Epoch 522/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2480.0597 - val_loss: 2801.0718\n",
      "Epoch 523/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2479.5317 - val_loss: 2802.2521\n",
      "Epoch 524/2500\n",
      "892/892 [==============================] - 0s 166us/step - loss: 2479.2927 - val_loss: 2800.5375\n",
      "Epoch 525/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2478.9281 - val_loss: 2799.8451\n",
      "Epoch 526/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2478.8624 - val_loss: 2800.6584\n",
      "Epoch 527/2500\n",
      "892/892 [==============================] - 0s 166us/step - loss: 2479.3552 - val_loss: 2801.3804\n",
      "Epoch 528/2500\n",
      "892/892 [==============================] - 0s 158us/step - loss: 2478.0175 - val_loss: 2798.5329\n",
      "Epoch 529/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2479.0283 - val_loss: 2796.4402\n",
      "Epoch 530/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2478.0877 - val_loss: 2799.0491\n",
      "Epoch 531/2500\n",
      "892/892 [==============================] - 0s 137us/step - loss: 2477.7523 - val_loss: 2800.1774\n",
      "Epoch 532/2500\n",
      "892/892 [==============================] - 0s 146us/step - loss: 2478.7179 - val_loss: 2795.6486\n",
      "Epoch 533/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2477.1540 - val_loss: 2798.8736\n",
      "Epoch 534/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2476.5824 - val_loss: 2798.1286\n",
      "Epoch 535/2500\n",
      "892/892 [==============================] - 0s 147us/step - loss: 2476.7949 - val_loss: 2796.5688\n",
      "Epoch 536/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2476.3255 - val_loss: 2796.8375\n",
      "Epoch 537/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2477.1720 - val_loss: 2799.9171\n",
      "Epoch 538/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2478.8647 - val_loss: 2794.1237\n",
      "Epoch 539/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2475.7812 - val_loss: 2797.0686\n",
      "Epoch 540/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2476.0425 - val_loss: 2796.9375\n",
      "Epoch 541/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2475.4514 - val_loss: 2797.4208\n",
      "Epoch 542/2500\n",
      "892/892 [==============================] - 0s 154us/step - loss: 2475.4777 - val_loss: 2796.2514\n",
      "Epoch 543/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2475.2400 - val_loss: 2796.1024\n",
      "Epoch 544/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2475.1705 - val_loss: 2794.5428\n",
      "Epoch 545/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2476.0936 - val_loss: 2797.7708\n",
      "Epoch 546/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2474.5221 - val_loss: 2794.8579\n",
      "Epoch 547/2500\n",
      "892/892 [==============================] - 0s 132us/step - loss: 2474.6684 - val_loss: 2796.3601\n",
      "Epoch 548/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2474.0263 - val_loss: 2795.1157\n",
      "Epoch 549/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2475.9446 - val_loss: 2792.8330\n",
      "Epoch 550/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2473.7100 - val_loss: 2795.4336\n",
      "Epoch 551/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2474.0263 - val_loss: 2792.8735\n",
      "Epoch 552/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2474.0011 - val_loss: 2793.9369\n",
      "Epoch 553/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2473.1989 - val_loss: 2794.6656\n",
      "Epoch 554/2500\n",
      "892/892 [==============================] - 0s 179us/step - loss: 2474.2074 - val_loss: 2793.8336\n",
      "Epoch 555/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2473.6975 - val_loss: 2796.0104\n",
      "Epoch 556/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2472.8313 - val_loss: 2793.4885\n",
      "Epoch 557/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2472.7073 - val_loss: 2793.4367\n",
      "Epoch 558/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2472.8397 - val_loss: 2793.0689\n",
      "Epoch 559/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2472.5023 - val_loss: 2794.0384\n",
      "Epoch 560/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2473.2336 - val_loss: 2791.2012\n",
      "Epoch 561/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2472.7456 - val_loss: 2791.3921\n",
      "Epoch 562/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2472.4915 - val_loss: 2794.8183\n",
      "Epoch 563/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2472.8507 - val_loss: 2791.6448\n",
      "Epoch 564/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2471.7582 - val_loss: 2792.4211\n",
      "Epoch 565/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2472.2733 - val_loss: 2790.5712\n",
      "Epoch 566/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2472.7989 - val_loss: 2794.4405\n",
      "Epoch 567/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2471.2841 - val_loss: 2791.4543\n",
      "Epoch 568/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 2471.9057 - val_loss: 2789.7850\n",
      "Epoch 569/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2471.4555 - val_loss: 2791.4721\n",
      "Epoch 570/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2472.5102 - val_loss: 2794.8100\n",
      "Epoch 571/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2470.8763 - val_loss: 2792.5054\n",
      "Epoch 572/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2471.2226 - val_loss: 2789.8997\n",
      "Epoch 573/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2471.6117 - val_loss: 2792.4337\n",
      "Epoch 574/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2471.0248 - val_loss: 2791.1507\n",
      "Epoch 575/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2470.5381 - val_loss: 2791.3424\n",
      "Epoch 576/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2470.7945 - val_loss: 2790.7889\n",
      "Epoch 577/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2471.0693 - val_loss: 2791.5436\n",
      "Epoch 578/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2470.5161 - val_loss: 2790.1041\n",
      "Epoch 579/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2470.7374 - val_loss: 2789.5543\n",
      "Epoch 580/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2469.9281 - val_loss: 2790.8610\n",
      "Epoch 581/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2470.9267 - val_loss: 2794.0090\n",
      "Epoch 582/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2470.3265 - val_loss: 2790.2351\n",
      "Epoch 583/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2470.4323 - val_loss: 2789.5011\n",
      "Epoch 584/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2470.1258 - val_loss: 2792.0438\n",
      "Epoch 585/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2471.1612 - val_loss: 2789.1136\n",
      "Epoch 586/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2469.1634 - val_loss: 2791.6572\n",
      "Epoch 587/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2469.7611 - val_loss: 2790.8948\n",
      "Epoch 588/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 2469.3586 - val_loss: 2792.4100\n",
      "Epoch 589/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2469.8172 - val_loss: 2792.8001\n",
      "Epoch 590/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2469.9484 - val_loss: 2789.7194\n",
      "Epoch 591/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2469.0804 - val_loss: 2790.7595\n",
      "Epoch 592/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2469.2212 - val_loss: 2792.2783\n",
      "Epoch 593/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2469.3289 - val_loss: 2789.0476\n",
      "Epoch 594/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2470.7166 - val_loss: 2790.4685\n",
      "Epoch 595/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2469.3830 - val_loss: 2789.9220\n",
      "Epoch 596/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2469.0882 - val_loss: 2789.6167\n",
      "Epoch 597/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2468.8096 - val_loss: 2792.1101\n",
      "Epoch 598/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2468.9557 - val_loss: 2789.2343\n",
      "Epoch 599/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2468.4827 - val_loss: 2790.4541\n",
      "Epoch 600/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 2468.8423 - val_loss: 2791.7030\n",
      "Epoch 601/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2468.4017 - val_loss: 2791.7382\n",
      "Epoch 602/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2469.7307 - val_loss: 2793.4102\n",
      "Epoch 603/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2467.7444 - val_loss: 2788.1520\n",
      "Epoch 604/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2468.0331 - val_loss: 2788.9505\n",
      "Epoch 605/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2468.6525 - val_loss: 2790.6352\n",
      "Epoch 606/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2468.7793 - val_loss: 2786.9178\n",
      "Epoch 607/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2468.3642 - val_loss: 2791.0627\n",
      "Epoch 608/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2467.9442 - val_loss: 2789.5252\n",
      "Epoch 609/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2467.3537 - val_loss: 2789.5939\n",
      "Epoch 610/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2467.9166 - val_loss: 2791.8253\n",
      "Epoch 611/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2467.7987 - val_loss: 2788.8888\n",
      "Epoch 612/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2467.2450 - val_loss: 2790.4817\n",
      "Epoch 613/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2467.8150 - val_loss: 2789.3412\n",
      "Epoch 614/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2467.1740 - val_loss: 2791.1983\n",
      "Epoch 615/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2467.2180 - val_loss: 2789.2992\n",
      "Epoch 616/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2467.0203 - val_loss: 2790.2470\n",
      "Epoch 617/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2468.0240 - val_loss: 2788.9005\n",
      "Epoch 618/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2466.9252 - val_loss: 2791.0218\n",
      "Epoch 619/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2466.5687 - val_loss: 2790.0532\n",
      "Epoch 620/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2467.9384 - val_loss: 2787.2068\n",
      "Epoch 621/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2467.3785 - val_loss: 2791.7945\n",
      "Epoch 622/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2466.2695 - val_loss: 2789.7016\n",
      "Epoch 623/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2466.8260 - val_loss: 2789.9901\n",
      "Epoch 624/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2467.7557 - val_loss: 2788.5234\n",
      "Epoch 625/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2466.7308 - val_loss: 2789.2447\n",
      "Epoch 626/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2466.7657 - val_loss: 2791.4327\n",
      "Epoch 627/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2466.0509 - val_loss: 2789.7320\n",
      "Epoch 628/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2466.7932 - val_loss: 2789.1393\n",
      "Epoch 629/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2466.6567 - val_loss: 2790.0720\n",
      "Epoch 630/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2466.2000 - val_loss: 2787.5010\n",
      "Epoch 631/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2466.2835 - val_loss: 2788.5297\n",
      "Epoch 632/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2466.4705 - val_loss: 2791.1277\n",
      "Epoch 633/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2466.6134 - val_loss: 2790.7517\n",
      "Epoch 634/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2465.5224 - val_loss: 2789.1423\n",
      "Epoch 635/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2465.8833 - val_loss: 2788.1571\n",
      "Epoch 636/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2465.7710 - val_loss: 2787.9841\n",
      "Epoch 637/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2465.8870 - val_loss: 2791.0971\n",
      "Epoch 638/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2465.5326 - val_loss: 2790.0295\n",
      "Epoch 639/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2465.4369 - val_loss: 2789.0906\n",
      "Epoch 640/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2465.7091 - val_loss: 2791.0108\n",
      "Epoch 641/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2465.1854 - val_loss: 2789.1152\n",
      "Epoch 642/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2464.9028 - val_loss: 2789.4130\n",
      "Epoch 643/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2464.8715 - val_loss: 2789.2823\n",
      "Epoch 644/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2464.9420 - val_loss: 2789.9124\n",
      "Epoch 645/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2467.0740 - val_loss: 2786.2027\n",
      "Epoch 646/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2464.2301 - val_loss: 2789.8718\n",
      "Epoch 647/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2465.1845 - val_loss: 2791.5437\n",
      "Epoch 648/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2465.7731 - val_loss: 2787.8282\n",
      "Epoch 649/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2464.4563 - val_loss: 2790.0626\n",
      "Epoch 650/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2465.9826 - val_loss: 2787.5204\n",
      "Epoch 651/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2464.8828 - val_loss: 2791.5600\n",
      "Epoch 652/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2464.7210 - val_loss: 2788.5913\n",
      "Epoch 653/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2464.4021 - val_loss: 2789.3563\n",
      "Epoch 654/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2464.1525 - val_loss: 2790.6962\n",
      "Epoch 655/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2464.8244 - val_loss: 2788.7259\n",
      "Epoch 656/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2464.4486 - val_loss: 2789.5398\n",
      "Epoch 657/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2465.7972 - val_loss: 2787.5194\n",
      "Epoch 658/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2465.5258 - val_loss: 2792.7355\n",
      "Epoch 659/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2464.6947 - val_loss: 2788.3432\n",
      "Epoch 660/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2464.0290 - val_loss: 2788.5944\n",
      "Epoch 661/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2463.9404 - val_loss: 2790.9212\n",
      "Epoch 662/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2463.7563 - val_loss: 2787.8510\n",
      "Epoch 663/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2463.9064 - val_loss: 2790.9470\n",
      "Epoch 664/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2463.4224 - val_loss: 2789.1238\n",
      "Epoch 665/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2463.4926 - val_loss: 2788.9717\n",
      "Epoch 666/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2463.5241 - val_loss: 2790.2665\n",
      "Epoch 667/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2463.7614 - val_loss: 2787.4135\n",
      "Epoch 668/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2463.5012 - val_loss: 2788.2967\n",
      "Epoch 669/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2463.0803 - val_loss: 2789.5842\n",
      "Epoch 670/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2463.0009 - val_loss: 2788.6735\n",
      "Epoch 671/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2464.0794 - val_loss: 2791.3241\n",
      "Epoch 672/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2463.7720 - val_loss: 2789.6247\n",
      "Epoch 673/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2463.4273 - val_loss: 2788.3667\n",
      "Epoch 674/2500\n",
      "892/892 [==============================] - 0s 155us/step - loss: 2463.3033 - val_loss: 2786.5365\n",
      "Epoch 675/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2462.6825 - val_loss: 2788.9573\n",
      "Epoch 676/2500\n",
      "892/892 [==============================] - 0s 162us/step - loss: 2462.9801 - val_loss: 2790.2211\n",
      "Epoch 677/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2462.3940 - val_loss: 2788.4940\n",
      "Epoch 678/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2462.7840 - val_loss: 2788.4358\n",
      "Epoch 679/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2463.6765 - val_loss: 2788.2142\n",
      "Epoch 680/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2463.4564 - val_loss: 2792.0393\n",
      "Epoch 681/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2462.5342 - val_loss: 2790.2420\n",
      "Epoch 682/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2462.1196 - val_loss: 2788.9442\n",
      "Epoch 683/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2462.3110 - val_loss: 2786.7935\n",
      "Epoch 684/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2462.5561 - val_loss: 2789.4544\n",
      "Epoch 685/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2462.4618 - val_loss: 2789.9403\n",
      "Epoch 686/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2462.0488 - val_loss: 2788.7439\n",
      "Epoch 687/2500\n",
      "892/892 [==============================] - 0s 137us/step - loss: 2461.7732 - val_loss: 2788.9132\n",
      "Epoch 688/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2461.6275 - val_loss: 2788.4148\n",
      "Epoch 689/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2461.8759 - val_loss: 2787.8463\n",
      "Epoch 690/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2462.2470 - val_loss: 2788.4338\n",
      "Epoch 691/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2462.8960 - val_loss: 2790.1896\n",
      "Epoch 692/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2462.9805 - val_loss: 2785.8833\n",
      "Epoch 693/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2463.1911 - val_loss: 2791.9298\n",
      "Epoch 694/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2462.4957 - val_loss: 2786.6739\n",
      "Epoch 695/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2462.2133 - val_loss: 2790.8805\n",
      "Epoch 696/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2461.0157 - val_loss: 2788.7529\n",
      "Epoch 697/2500\n",
      "892/892 [==============================] - 0s 175us/step - loss: 2461.8631 - val_loss: 2786.6960\n",
      "Epoch 698/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2461.2133 - val_loss: 2786.9413\n",
      "Epoch 699/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2461.0621 - val_loss: 2789.1778\n",
      "Epoch 700/2500\n",
      "892/892 [==============================] - 0s 168us/step - loss: 2461.3266 - val_loss: 2788.8765\n",
      "Epoch 701/2500\n",
      "892/892 [==============================] - 0s 148us/step - loss: 2462.7844 - val_loss: 2789.1966\n",
      "Epoch 702/2500\n",
      "892/892 [==============================] - 0s 159us/step - loss: 2461.0515 - val_loss: 2787.8899\n",
      "Epoch 703/2500\n",
      "892/892 [==============================] - 0s 150us/step - loss: 2461.4194 - val_loss: 2790.0947\n",
      "Epoch 704/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2460.7386 - val_loss: 2788.9028\n",
      "Epoch 705/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2461.9171 - val_loss: 2787.1284\n",
      "Epoch 706/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2461.0028 - val_loss: 2789.5566\n",
      "Epoch 707/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2461.0948 - val_loss: 2790.4380\n",
      "Epoch 708/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2460.8931 - val_loss: 2786.9945\n",
      "Epoch 709/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2460.5561 - val_loss: 2787.5396\n",
      "Epoch 710/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2461.6452 - val_loss: 2790.4471\n",
      "Epoch 711/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2460.2681 - val_loss: 2787.4894\n",
      "Epoch 712/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2460.5688 - val_loss: 2789.1557\n",
      "Epoch 713/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2460.1692 - val_loss: 2787.4400\n",
      "Epoch 714/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2460.4996 - val_loss: 2788.6414\n",
      "Epoch 715/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2460.5439 - val_loss: 2788.6523\n",
      "Epoch 716/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2460.5712 - val_loss: 2789.4306\n",
      "Epoch 717/2500\n",
      "892/892 [==============================] - 0s 165us/step - loss: 2461.3898 - val_loss: 2789.2348\n",
      "Epoch 718/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2460.7433 - val_loss: 2788.2299\n",
      "Epoch 719/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2460.0117 - val_loss: 2786.4175\n",
      "Epoch 720/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2460.1464 - val_loss: 2788.9558\n",
      "Epoch 721/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2460.7418 - val_loss: 2790.1976\n",
      "Epoch 722/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2460.1614 - val_loss: 2786.0137\n",
      "Epoch 723/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2460.0893 - val_loss: 2787.1390\n",
      "Epoch 724/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2460.0257 - val_loss: 2787.3590\n",
      "Epoch 725/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2460.2889 - val_loss: 2790.4413\n",
      "Epoch 726/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2460.0018 - val_loss: 2787.1309\n",
      "Epoch 727/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2460.1394 - val_loss: 2788.8366\n",
      "Epoch 728/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2460.2673 - val_loss: 2787.4471\n",
      "Epoch 729/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2461.9164 - val_loss: 2791.6478\n",
      "Epoch 730/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2459.0825 - val_loss: 2788.0561\n",
      "Epoch 731/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2459.2211 - val_loss: 2788.8096\n",
      "Epoch 732/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2459.3211 - val_loss: 2787.4622\n",
      "Epoch 733/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2459.3186 - val_loss: 2788.3663\n",
      "Epoch 734/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2459.7851 - val_loss: 2787.6005\n",
      "Epoch 735/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2458.9827 - val_loss: 2787.5638\n",
      "Epoch 736/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2459.2658 - val_loss: 2788.3721\n",
      "Epoch 737/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2458.8613 - val_loss: 2790.0333\n",
      "Epoch 738/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2460.3737 - val_loss: 2787.3966\n",
      "Epoch 739/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2458.9394 - val_loss: 2788.9230\n",
      "Epoch 740/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2458.6804 - val_loss: 2790.2200\n",
      "Epoch 741/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2458.7027 - val_loss: 2789.3508\n",
      "Epoch 742/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2460.0229 - val_loss: 2792.2298\n",
      "Epoch 743/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2459.2929 - val_loss: 2787.8496\n",
      "Epoch 744/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2459.3569 - val_loss: 2789.1412\n",
      "Epoch 745/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2459.1682 - val_loss: 2790.1514\n",
      "Epoch 746/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2458.8273 - val_loss: 2788.6599\n",
      "Epoch 747/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2458.3373 - val_loss: 2789.0530\n",
      "Epoch 748/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2458.6165 - val_loss: 2788.8254\n",
      "Epoch 749/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 2458.5268 - val_loss: 2789.8914\n",
      "Epoch 750/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2458.1442 - val_loss: 2788.6837\n",
      "Epoch 751/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2458.7241 - val_loss: 2786.6404\n",
      "Epoch 752/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2457.8898 - val_loss: 2788.7579\n",
      "Epoch 753/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2460.5223 - val_loss: 2792.1911\n",
      "Epoch 754/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2458.7051 - val_loss: 2788.3731\n",
      "Epoch 755/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2458.9980 - val_loss: 2790.5423\n",
      "Epoch 756/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2458.5825 - val_loss: 2786.0442\n",
      "Epoch 757/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2457.8347 - val_loss: 2787.1568\n",
      "Epoch 758/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2457.9177 - val_loss: 2789.0126\n",
      "Epoch 759/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2457.5305 - val_loss: 2789.0769\n",
      "Epoch 760/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2457.5027 - val_loss: 2788.6404\n",
      "Epoch 761/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2459.0256 - val_loss: 2787.6037\n",
      "Epoch 762/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2457.6199 - val_loss: 2788.2714\n",
      "Epoch 763/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2457.5980 - val_loss: 2790.2107\n",
      "Epoch 764/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2457.3738 - val_loss: 2790.1282\n",
      "Epoch 765/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2457.3480 - val_loss: 2788.7913\n",
      "Epoch 766/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2457.1363 - val_loss: 2788.0430\n",
      "Epoch 767/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2457.9094 - val_loss: 2788.0308\n",
      "Epoch 768/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2457.0237 - val_loss: 2788.3494\n",
      "Epoch 769/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2457.4795 - val_loss: 2789.0770\n",
      "Epoch 770/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2457.0365 - val_loss: 2789.0985\n",
      "Epoch 771/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2457.8244 - val_loss: 2787.5487\n",
      "Epoch 772/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2457.3597 - val_loss: 2790.6329\n",
      "Epoch 773/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2456.9931 - val_loss: 2788.0543\n",
      "Epoch 774/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2457.3610 - val_loss: 2787.6225\n",
      "Epoch 775/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2456.9797 - val_loss: 2790.3071\n",
      "Epoch 776/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2457.2451 - val_loss: 2787.8216\n",
      "Epoch 777/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2456.7729 - val_loss: 2791.1451\n",
      "Epoch 778/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2456.8572 - val_loss: 2789.6106\n",
      "Epoch 779/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2456.4063 - val_loss: 2788.0263\n",
      "Epoch 780/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2456.2631 - val_loss: 2788.5787\n",
      "Epoch 781/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2458.2372 - val_loss: 2791.7301\n",
      "Epoch 782/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2457.0394 - val_loss: 2786.3788\n",
      "Epoch 783/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2456.9553 - val_loss: 2787.1659\n",
      "Epoch 784/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2456.4660 - val_loss: 2790.2519\n",
      "Epoch 785/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2456.0583 - val_loss: 2789.1021\n",
      "Epoch 786/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2456.6411 - val_loss: 2787.7545\n",
      "Epoch 787/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2456.8214 - val_loss: 2789.6794\n",
      "Epoch 788/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2456.0439 - val_loss: 2787.3341\n",
      "Epoch 789/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2456.0859 - val_loss: 2788.1523\n",
      "Epoch 790/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2456.0553 - val_loss: 2787.5707\n",
      "Epoch 791/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2455.9029 - val_loss: 2788.6093\n",
      "Epoch 792/2500\n",
      "892/892 [==============================] - 0s 144us/step - loss: 2456.4749 - val_loss: 2789.4970\n",
      "Epoch 793/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2456.1308 - val_loss: 2787.4129\n",
      "Epoch 794/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2455.7604 - val_loss: 2789.4112\n",
      "Epoch 795/2500\n",
      "892/892 [==============================] - 0s 136us/step - loss: 2456.7703 - val_loss: 2787.5480\n",
      "Epoch 796/2500\n",
      "892/892 [==============================] - 0s 138us/step - loss: 2455.9088 - val_loss: 2789.6837\n",
      "Epoch 797/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2455.7447 - val_loss: 2789.4877\n",
      "Epoch 798/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2455.8350 - val_loss: 2788.2802\n",
      "Epoch 799/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2456.2901 - val_loss: 2790.8012\n",
      "Epoch 800/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2456.8266 - val_loss: 2786.2079\n",
      "Epoch 801/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2456.3924 - val_loss: 2791.3625\n",
      "Epoch 802/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2455.2034 - val_loss: 2787.8610\n",
      "Epoch 803/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2455.3963 - val_loss: 2788.9273\n",
      "Epoch 804/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2455.7904 - val_loss: 2788.4656\n",
      "Epoch 805/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2455.6481 - val_loss: 2789.0642\n",
      "Epoch 806/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2456.4651 - val_loss: 2787.4714\n",
      "Epoch 807/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2455.0751 - val_loss: 2788.1794\n",
      "Epoch 808/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2456.9623 - val_loss: 2791.5376\n",
      "Epoch 809/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2454.7211 - val_loss: 2787.5038\n",
      "Epoch 810/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2455.1793 - val_loss: 2787.5118\n",
      "Epoch 811/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2455.8886 - val_loss: 2786.4627\n",
      "Epoch 812/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2455.9867 - val_loss: 2790.9285\n",
      "Epoch 813/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2455.0167 - val_loss: 2788.6983\n",
      "Epoch 814/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2455.1431 - val_loss: 2788.1499\n",
      "Epoch 815/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2455.5950 - val_loss: 2787.1864\n",
      "Epoch 816/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2454.6115 - val_loss: 2788.3596\n",
      "Epoch 817/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2455.1851 - val_loss: 2788.0881\n",
      "Epoch 818/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2455.5337 - val_loss: 2790.7841\n",
      "Epoch 819/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2456.2190 - val_loss: 2787.1430\n",
      "Epoch 820/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2454.2872 - val_loss: 2789.6639\n",
      "Epoch 821/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2454.5666 - val_loss: 2790.4542\n",
      "Epoch 822/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2454.3782 - val_loss: 2789.7894\n",
      "Epoch 823/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2454.7587 - val_loss: 2788.8478\n",
      "Epoch 824/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2454.5523 - val_loss: 2788.4507\n",
      "Epoch 825/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2454.5562 - val_loss: 2789.6555\n",
      "Epoch 826/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2454.6618 - val_loss: 2788.4861\n",
      "Epoch 827/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2454.3430 - val_loss: 2789.1129\n",
      "Epoch 828/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2454.4900 - val_loss: 2788.2485\n",
      "Epoch 829/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2454.4966 - val_loss: 2788.8914\n",
      "Epoch 830/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2454.4134 - val_loss: 2789.3018\n",
      "Epoch 831/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2454.5047 - val_loss: 2787.8857\n",
      "Epoch 832/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2453.8682 - val_loss: 2788.4328\n",
      "Epoch 833/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2453.9590 - val_loss: 2789.7357\n",
      "Epoch 834/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2455.2891 - val_loss: 2787.0569\n",
      "Epoch 835/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2454.3791 - val_loss: 2788.2982\n",
      "Epoch 836/2500\n",
      "892/892 [==============================] - 0s 147us/step - loss: 2454.4410 - val_loss: 2791.8968\n",
      "Epoch 837/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2454.6895 - val_loss: 2791.3247\n",
      "Epoch 838/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2455.2184 - val_loss: 2787.0839\n",
      "Epoch 839/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2453.8178 - val_loss: 2790.0074\n",
      "Epoch 840/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2453.6116 - val_loss: 2788.3600\n",
      "Epoch 841/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2453.5158 - val_loss: 2787.9297\n",
      "Epoch 842/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2453.9736 - val_loss: 2791.0530\n",
      "Epoch 843/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2453.7256 - val_loss: 2787.7010\n",
      "Epoch 844/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2453.5927 - val_loss: 2787.2685\n",
      "Epoch 845/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2453.2747 - val_loss: 2789.1553\n",
      "Epoch 846/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2453.1741 - val_loss: 2789.7260\n",
      "Epoch 847/2500\n",
      "892/892 [==============================] - 0s 137us/step - loss: 2454.1241 - val_loss: 2790.2566\n",
      "Epoch 848/2500\n",
      "892/892 [==============================] - 0s 139us/step - loss: 2454.0045 - val_loss: 2790.9607\n",
      "Epoch 849/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2455.0544 - val_loss: 2789.9879\n",
      "Epoch 850/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2453.2851 - val_loss: 2789.0085\n",
      "Epoch 851/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2453.2895 - val_loss: 2787.3527\n",
      "Epoch 852/2500\n",
      "892/892 [==============================] - 0s 125us/step - loss: 2453.2990 - val_loss: 2790.6502\n",
      "Epoch 853/2500\n",
      "892/892 [==============================] - 0s 170us/step - loss: 2454.0048 - val_loss: 2787.2303\n",
      "Epoch 854/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2454.1325 - val_loss: 2791.4204\n",
      "Epoch 855/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2454.3831 - val_loss: 2786.4056\n",
      "Epoch 856/2500\n",
      "892/892 [==============================] - 0s 136us/step - loss: 2453.0760 - val_loss: 2790.9803\n",
      "Epoch 857/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2454.3271 - val_loss: 2793.2371\n",
      "Epoch 858/2500\n",
      "892/892 [==============================] - 0s 159us/step - loss: 2454.3727 - val_loss: 2787.7807\n",
      "Epoch 859/2500\n",
      "892/892 [==============================] - 0s 139us/step - loss: 2452.8048 - val_loss: 2787.7858\n",
      "Epoch 860/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2453.3745 - val_loss: 2789.1694\n",
      "Epoch 861/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2453.6217 - val_loss: 2791.0482\n",
      "Epoch 862/2500\n",
      "892/892 [==============================] - 0s 139us/step - loss: 2452.4481 - val_loss: 2788.5645\n",
      "Epoch 863/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2452.4953 - val_loss: 2788.2365\n",
      "Epoch 864/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2453.2766 - val_loss: 2787.2565\n",
      "Epoch 865/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2453.5006 - val_loss: 2786.9871\n",
      "Epoch 866/2500\n",
      "892/892 [==============================] - 0s 140us/step - loss: 2452.9774 - val_loss: 2790.1061\n",
      "Epoch 867/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2452.5186 - val_loss: 2790.1979\n",
      "Epoch 868/2500\n",
      "892/892 [==============================] - 0s 147us/step - loss: 2452.5591 - val_loss: 2790.4338\n",
      "Epoch 869/2500\n",
      "892/892 [==============================] - 0s 161us/step - loss: 2452.3712 - val_loss: 2789.2527\n",
      "Epoch 870/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2453.5258 - val_loss: 2787.4060\n",
      "Epoch 871/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2452.3363 - val_loss: 2789.7088\n",
      "Epoch 872/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2452.4073 - val_loss: 2791.2815\n",
      "Epoch 873/2500\n",
      "892/892 [==============================] - 0s 154us/step - loss: 2452.3674 - val_loss: 2788.6101\n",
      "Epoch 874/2500\n",
      "892/892 [==============================] - 0s 126us/step - loss: 2453.3572 - val_loss: 2790.9023\n",
      "Epoch 875/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2452.7591 - val_loss: 2787.4892\n",
      "Epoch 876/2500\n",
      "892/892 [==============================] - 0s 158us/step - loss: 2452.3708 - val_loss: 2789.1629\n",
      "Epoch 877/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2451.9907 - val_loss: 2789.9156\n",
      "Epoch 878/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2452.0653 - val_loss: 2790.5005\n",
      "Epoch 879/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2452.9324 - val_loss: 2787.1878\n",
      "Epoch 880/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2451.6396 - val_loss: 2789.8371\n",
      "Epoch 881/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2452.0340 - val_loss: 2789.4363\n",
      "Epoch 882/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2451.6990 - val_loss: 2789.8823\n",
      "Epoch 883/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2451.5785 - val_loss: 2788.3494\n",
      "Epoch 884/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2452.3763 - val_loss: 2788.8085\n",
      "Epoch 885/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2451.8619 - val_loss: 2789.9471\n",
      "Epoch 886/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2452.1074 - val_loss: 2788.6346\n",
      "Epoch 887/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2452.3813 - val_loss: 2787.9266\n",
      "Epoch 888/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2452.3007 - val_loss: 2790.2959\n",
      "Epoch 889/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2451.6599 - val_loss: 2788.2490\n",
      "Epoch 890/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2451.3392 - val_loss: 2789.8273\n",
      "Epoch 891/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2451.4453 - val_loss: 2789.1650\n",
      "Epoch 892/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2451.9129 - val_loss: 2788.0210\n",
      "Epoch 893/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2451.4195 - val_loss: 2790.0892\n",
      "Epoch 894/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2451.4839 - val_loss: 2788.8369\n",
      "Epoch 895/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2451.7722 - val_loss: 2790.6420\n",
      "Epoch 896/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2451.2409 - val_loss: 2789.9964\n",
      "Epoch 897/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2451.6758 - val_loss: 2789.2207\n",
      "Epoch 898/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2451.2960 - val_loss: 2787.7327\n",
      "Epoch 899/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2451.2456 - val_loss: 2789.9028\n",
      "Epoch 900/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2451.1835 - val_loss: 2788.6046\n",
      "Epoch 901/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2450.7605 - val_loss: 2789.5055\n",
      "Epoch 902/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2452.5331 - val_loss: 2787.7450\n",
      "Epoch 903/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2450.8625 - val_loss: 2789.5050\n",
      "Epoch 904/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2452.0165 - val_loss: 2789.9100\n",
      "Epoch 905/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2450.8632 - val_loss: 2790.3641\n",
      "Epoch 906/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2451.7221 - val_loss: 2787.4495\n",
      "Epoch 907/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2450.5665 - val_loss: 2790.5280\n",
      "Epoch 908/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2450.9342 - val_loss: 2790.0259\n",
      "Epoch 909/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2450.9757 - val_loss: 2789.6884\n",
      "Epoch 910/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2450.8086 - val_loss: 2790.4268\n",
      "Epoch 911/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2450.9639 - val_loss: 2789.6631\n",
      "Epoch 912/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2450.4948 - val_loss: 2790.1381\n",
      "Epoch 913/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2450.3850 - val_loss: 2790.1085\n",
      "Epoch 914/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2451.6114 - val_loss: 2788.4692\n",
      "Epoch 915/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2450.6521 - val_loss: 2788.1258\n",
      "Epoch 916/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2450.1811 - val_loss: 2790.7259\n",
      "Epoch 917/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2451.5983 - val_loss: 2787.4637\n",
      "Epoch 918/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2450.0808 - val_loss: 2790.4656\n",
      "Epoch 919/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2450.1044 - val_loss: 2789.4949\n",
      "Epoch 920/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2450.1345 - val_loss: 2790.6155\n",
      "Epoch 921/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2450.8175 - val_loss: 2789.8619\n",
      "Epoch 922/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2450.5357 - val_loss: 2790.4293\n",
      "Epoch 923/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2450.9719 - val_loss: 2788.0589\n",
      "Epoch 924/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2451.1622 - val_loss: 2791.9803\n",
      "Epoch 925/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2451.4316 - val_loss: 2787.7985\n",
      "Epoch 926/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2450.7458 - val_loss: 2789.8166\n",
      "Epoch 927/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2450.1264 - val_loss: 2788.3953\n",
      "Epoch 928/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2450.0298 - val_loss: 2788.3970\n",
      "Epoch 929/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2449.9851 - val_loss: 2791.1361\n",
      "Epoch 930/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2449.7912 - val_loss: 2791.2187\n",
      "Epoch 931/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2450.2481 - val_loss: 2788.3887\n",
      "Epoch 932/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2450.0684 - val_loss: 2789.2186\n",
      "Epoch 933/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2449.9311 - val_loss: 2788.4961\n",
      "Epoch 934/2500\n",
      "892/892 [==============================] - 0s 125us/step - loss: 2449.6929 - val_loss: 2791.1064\n",
      "Epoch 935/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2449.9001 - val_loss: 2790.6467\n",
      "Epoch 936/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2449.5803 - val_loss: 2789.7402\n",
      "Epoch 937/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2450.3516 - val_loss: 2787.8592\n",
      "Epoch 938/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2449.2352 - val_loss: 2788.9910\n",
      "Epoch 939/2500\n",
      "892/892 [==============================] - 0s 139us/step - loss: 2450.8143 - val_loss: 2793.3684\n",
      "Epoch 940/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2449.6031 - val_loss: 2788.9548\n",
      "Epoch 941/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2449.6947 - val_loss: 2790.9148\n",
      "Epoch 942/2500\n",
      "892/892 [==============================] - 0s 143us/step - loss: 2449.5587 - val_loss: 2788.7730\n",
      "Epoch 943/2500\n",
      "892/892 [==============================] - 0s 136us/step - loss: 2449.3476 - val_loss: 2789.1375\n",
      "Epoch 944/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2449.4881 - val_loss: 2789.6780\n",
      "Epoch 945/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2450.1587 - val_loss: 2791.7579\n",
      "Epoch 946/2500\n",
      "892/892 [==============================] - 0s 126us/step - loss: 2449.2469 - val_loss: 2790.1582\n",
      "Epoch 947/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2449.1725 - val_loss: 2789.3593\n",
      "Epoch 948/2500\n",
      "892/892 [==============================] - 0s 189us/step - loss: 2449.2769 - val_loss: 2788.0056\n",
      "Epoch 949/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2449.6634 - val_loss: 2789.3497\n",
      "Epoch 950/2500\n",
      "892/892 [==============================] - 0s 179us/step - loss: 2449.3669 - val_loss: 2790.5821\n",
      "Epoch 951/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2449.7864 - val_loss: 2789.5883\n",
      "Epoch 952/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2449.0781 - val_loss: 2789.1152\n",
      "Epoch 953/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2449.0022 - val_loss: 2789.8415\n",
      "Epoch 954/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2449.7072 - val_loss: 2787.7944\n",
      "Epoch 955/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2449.6623 - val_loss: 2789.1666\n",
      "Epoch 956/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2451.1623 - val_loss: 2790.8684\n",
      "Epoch 957/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2448.7315 - val_loss: 2789.0924\n",
      "Epoch 958/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2448.8043 - val_loss: 2788.0465\n",
      "Epoch 959/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2449.5015 - val_loss: 2791.2702\n",
      "Epoch 960/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2449.7497 - val_loss: 2791.2763\n",
      "Epoch 961/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2450.6205 - val_loss: 2791.2295\n",
      "Epoch 962/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2449.4694 - val_loss: 2787.9538\n",
      "Epoch 963/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2449.5940 - val_loss: 2786.1713\n",
      "Epoch 964/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2448.4944 - val_loss: 2789.3025\n",
      "Epoch 965/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2449.0239 - val_loss: 2791.1787\n",
      "Epoch 966/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2448.6380 - val_loss: 2789.0911\n",
      "Epoch 967/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2448.5050 - val_loss: 2789.4244\n",
      "Epoch 968/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2448.7116 - val_loss: 2790.7259\n",
      "Epoch 969/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2448.2009 - val_loss: 2788.7870\n",
      "Epoch 970/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2450.0453 - val_loss: 2786.9242\n",
      "Epoch 971/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2449.4272 - val_loss: 2789.7611\n",
      "Epoch 972/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2449.3815 - val_loss: 2789.5063\n",
      "Epoch 973/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2448.1616 - val_loss: 2789.8931\n",
      "Epoch 974/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2448.0917 - val_loss: 2790.5939\n",
      "Epoch 975/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2448.7699 - val_loss: 2790.1462\n",
      "Epoch 976/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2448.2126 - val_loss: 2789.4286\n",
      "Epoch 977/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2448.2683 - val_loss: 2790.2777\n",
      "Epoch 978/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2448.1165 - val_loss: 2789.3429\n",
      "Epoch 979/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2448.2009 - val_loss: 2791.1004\n",
      "Epoch 980/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2448.1973 - val_loss: 2790.1285\n",
      "Epoch 981/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2448.0427 - val_loss: 2789.3147\n",
      "Epoch 982/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2448.5164 - val_loss: 2788.7553\n",
      "Epoch 983/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2450.4871 - val_loss: 2794.2042\n",
      "Epoch 984/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2447.6768 - val_loss: 2789.9127\n",
      "Epoch 985/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2448.5895 - val_loss: 2790.6106\n",
      "Epoch 986/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2448.0789 - val_loss: 2790.3994\n",
      "Epoch 987/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2447.6716 - val_loss: 2789.1770\n",
      "Epoch 988/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2447.6458 - val_loss: 2788.8919\n",
      "Epoch 989/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2447.9309 - val_loss: 2787.8959\n",
      "Epoch 990/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2447.6928 - val_loss: 2789.7940\n",
      "Epoch 991/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2447.7495 - val_loss: 2790.4568\n",
      "Epoch 992/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2447.7213 - val_loss: 2790.0978\n",
      "Epoch 993/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2447.7147 - val_loss: 2790.6991\n",
      "Epoch 994/2500\n",
      "892/892 [==============================] - 0s 171us/step - loss: 2447.6442 - val_loss: 2789.7214\n",
      "Epoch 995/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2450.0563 - val_loss: 2786.3821\n",
      "Epoch 996/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2447.7800 - val_loss: 2792.2550\n",
      "Epoch 997/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2447.8911 - val_loss: 2789.8020\n",
      "Epoch 998/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2447.4964 - val_loss: 2789.8633\n",
      "Epoch 999/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2447.3393 - val_loss: 2790.6173\n",
      "Epoch 1000/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2447.5741 - val_loss: 2790.3498\n",
      "Epoch 1001/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2447.7785 - val_loss: 2789.3205\n",
      "Epoch 1002/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2447.6441 - val_loss: 2790.5577\n",
      "Epoch 1003/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2447.1719 - val_loss: 2790.5371\n",
      "Epoch 1004/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2447.9764 - val_loss: 2791.9450\n",
      "Epoch 1005/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2448.3680 - val_loss: 2788.1675\n",
      "Epoch 1006/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2448.8595 - val_loss: 2791.5269\n",
      "Epoch 1007/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2448.5717 - val_loss: 2791.9138\n",
      "Epoch 1008/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2446.7978 - val_loss: 2789.4532\n",
      "Epoch 1009/2500\n",
      "892/892 [==============================] - 0s 155us/step - loss: 2447.2127 - val_loss: 2788.3964\n",
      "Epoch 1010/2500\n",
      "892/892 [==============================] - 0s 178us/step - loss: 2446.8016 - val_loss: 2790.4666\n",
      "Epoch 1011/2500\n",
      "892/892 [==============================] - 0s 170us/step - loss: 2447.8258 - val_loss: 2788.6836\n",
      "Epoch 1012/2500\n",
      "892/892 [==============================] - 0s 143us/step - loss: 2447.2206 - val_loss: 2791.7543\n",
      "Epoch 1013/2500\n",
      "892/892 [==============================] - 0s 176us/step - loss: 2446.9287 - val_loss: 2790.9782\n",
      "Epoch 1014/2500\n",
      "892/892 [==============================] - 0s 151us/step - loss: 2446.8254 - val_loss: 2789.9769\n",
      "Epoch 1015/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2446.8749 - val_loss: 2789.2682\n",
      "Epoch 1016/2500\n",
      "892/892 [==============================] - 0s 167us/step - loss: 2447.1473 - val_loss: 2791.0451\n",
      "Epoch 1017/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2446.7577 - val_loss: 2790.7891\n",
      "Epoch 1018/2500\n",
      "892/892 [==============================] - 0s 146us/step - loss: 2447.3221 - val_loss: 2789.1383\n",
      "Epoch 1019/2500\n",
      "892/892 [==============================] - 0s 148us/step - loss: 2447.2752 - val_loss: 2788.0742\n",
      "Epoch 1020/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2446.4331 - val_loss: 2790.6783\n",
      "Epoch 1021/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2446.5936 - val_loss: 2792.4040\n",
      "Epoch 1022/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2447.5317 - val_loss: 2793.4673\n",
      "Epoch 1023/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2447.1815 - val_loss: 2789.5365\n",
      "Epoch 1024/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2447.1132 - val_loss: 2788.2552\n",
      "Epoch 1025/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2446.4354 - val_loss: 2789.5062\n",
      "Epoch 1026/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2447.3948 - val_loss: 2792.7973\n",
      "Epoch 1027/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2446.3237 - val_loss: 2789.2279\n",
      "Epoch 1028/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2447.3133 - val_loss: 2790.1095\n",
      "Epoch 1029/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2446.9974 - val_loss: 2789.3500\n",
      "Epoch 1030/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2446.6984 - val_loss: 2790.9436\n",
      "Epoch 1031/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2446.7588 - val_loss: 2789.8803\n",
      "Epoch 1032/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2446.4869 - val_loss: 2791.2616\n",
      "Epoch 1033/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2446.4178 - val_loss: 2791.9714\n",
      "Epoch 1034/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2446.4093 - val_loss: 2790.0391\n",
      "Epoch 1035/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2445.9567 - val_loss: 2790.4587\n",
      "Epoch 1036/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2446.6897 - val_loss: 2789.5514\n",
      "Epoch 1037/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2446.6150 - val_loss: 2789.1161\n",
      "Epoch 1038/2500\n",
      "892/892 [==============================] - 0s 150us/step - loss: 2446.0087 - val_loss: 2792.5704\n",
      "Epoch 1039/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2446.1765 - val_loss: 2792.3063\n",
      "Epoch 1040/2500\n",
      "892/892 [==============================] - 0s 180us/step - loss: 2446.3566 - val_loss: 2792.2198\n",
      "Epoch 1041/2500\n",
      "892/892 [==============================] - 0s 163us/step - loss: 2445.7250 - val_loss: 2790.9189\n",
      "Epoch 1042/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2446.6107 - val_loss: 2790.9517\n",
      "Epoch 1043/2500\n",
      "892/892 [==============================] - 0s 151us/step - loss: 2446.1937 - val_loss: 2790.4133\n",
      "Epoch 1044/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2446.6278 - val_loss: 2791.2696\n",
      "Epoch 1045/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2446.2147 - val_loss: 2792.1189\n",
      "Epoch 1046/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2445.9240 - val_loss: 2790.1410\n",
      "Epoch 1047/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2445.6778 - val_loss: 2790.7647\n",
      "Epoch 1048/2500\n",
      "892/892 [==============================] - 0s 132us/step - loss: 2445.6428 - val_loss: 2790.2026\n",
      "Epoch 1049/2500\n",
      "892/892 [==============================] - 0s 138us/step - loss: 2446.1289 - val_loss: 2789.4312\n",
      "Epoch 1050/2500\n",
      "892/892 [==============================] - 0s 165us/step - loss: 2445.8469 - val_loss: 2790.2051\n",
      "Epoch 1051/2500\n",
      "892/892 [==============================] - 0s 153us/step - loss: 2446.4047 - val_loss: 2791.9554\n",
      "Epoch 1052/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2445.8934 - val_loss: 2790.1321\n",
      "Epoch 1053/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2446.0222 - val_loss: 2791.0488\n",
      "Epoch 1054/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2446.8604 - val_loss: 2793.9766\n",
      "Epoch 1055/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2445.4286 - val_loss: 2790.3594\n",
      "Epoch 1056/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2445.4081 - val_loss: 2790.2266\n",
      "Epoch 1057/2500\n",
      "892/892 [==============================] - 0s 149us/step - loss: 2445.5029 - val_loss: 2790.5137\n",
      "Epoch 1058/2500\n",
      "892/892 [==============================] - 0s 150us/step - loss: 2446.0874 - val_loss: 2789.1466\n",
      "Epoch 1059/2500\n",
      "892/892 [==============================] - 0s 169us/step - loss: 2445.8287 - val_loss: 2790.6895\n",
      "Epoch 1060/2500\n",
      "892/892 [==============================] - 0s 132us/step - loss: 2445.6003 - val_loss: 2790.3457\n",
      "Epoch 1061/2500\n",
      "892/892 [==============================] - 0s 149us/step - loss: 2445.7379 - val_loss: 2790.0816\n",
      "Epoch 1062/2500\n",
      "892/892 [==============================] - 0s 142us/step - loss: 2446.1897 - val_loss: 2792.5269\n",
      "Epoch 1063/2500\n",
      "892/892 [==============================] - 0s 174us/step - loss: 2445.3089 - val_loss: 2790.1300\n",
      "Epoch 1064/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2445.7981 - val_loss: 2791.0175\n",
      "Epoch 1065/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2445.4112 - val_loss: 2790.4622\n",
      "Epoch 1066/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2445.3156 - val_loss: 2791.2652\n",
      "Epoch 1067/2500\n",
      "892/892 [==============================] - 0s 137us/step - loss: 2445.3976 - val_loss: 2791.4116\n",
      "Epoch 1068/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2445.8236 - val_loss: 2790.2030\n",
      "Epoch 1069/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2446.4268 - val_loss: 2788.1139\n",
      "Epoch 1070/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2446.0393 - val_loss: 2793.1029\n",
      "Epoch 1071/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2445.0599 - val_loss: 2791.3927\n",
      "Epoch 1072/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2444.9842 - val_loss: 2789.9528\n",
      "Epoch 1073/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2445.3254 - val_loss: 2791.7558\n",
      "Epoch 1074/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2445.0767 - val_loss: 2790.9603\n",
      "Epoch 1075/2500\n",
      "892/892 [==============================] - 0s 154us/step - loss: 2445.3063 - val_loss: 2788.9273\n",
      "Epoch 1076/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2445.2222 - val_loss: 2789.7369\n",
      "Epoch 1077/2500\n",
      "892/892 [==============================] - 0s 135us/step - loss: 2445.3538 - val_loss: 2793.0222\n",
      "Epoch 1078/2500\n",
      "892/892 [==============================] - 0s 194us/step - loss: 2444.9809 - val_loss: 2790.1844\n",
      "Epoch 1079/2500\n",
      "892/892 [==============================] - 0s 150us/step - loss: 2445.6232 - val_loss: 2790.2504\n",
      "Epoch 1080/2500\n",
      "892/892 [==============================] - 0s 146us/step - loss: 2444.9763 - val_loss: 2790.5932\n",
      "Epoch 1081/2500\n",
      "892/892 [==============================] - 0s 210us/step - loss: 2445.3966 - val_loss: 2792.0905\n",
      "Epoch 1082/2500\n",
      "892/892 [==============================] - 0s 135us/step - loss: 2445.6648 - val_loss: 2792.2922\n",
      "Epoch 1083/2500\n",
      "892/892 [==============================] - 0s 136us/step - loss: 2445.7511 - val_loss: 2788.9873\n",
      "Epoch 1084/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2446.6687 - val_loss: 2792.6216\n",
      "Epoch 1085/2500\n",
      "892/892 [==============================] - 0s 132us/step - loss: 2447.7512 - val_loss: 2788.4757\n",
      "Epoch 1086/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2444.3879 - val_loss: 2791.2500\n",
      "Epoch 1087/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2445.8502 - val_loss: 2792.4441\n",
      "Epoch 1088/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2444.6828 - val_loss: 2792.5908\n",
      "Epoch 1089/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2444.6961 - val_loss: 2791.7296\n",
      "Epoch 1090/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2445.1523 - val_loss: 2790.1836\n",
      "Epoch 1091/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2444.9812 - val_loss: 2790.8455\n",
      "Epoch 1092/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2444.4795 - val_loss: 2791.3026\n",
      "Epoch 1093/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2445.4278 - val_loss: 2790.7692\n",
      "Epoch 1094/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2444.7372 - val_loss: 2790.4993\n",
      "Epoch 1095/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2444.4146 - val_loss: 2792.3270\n",
      "Epoch 1096/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2444.3242 - val_loss: 2791.4277\n",
      "Epoch 1097/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2444.3689 - val_loss: 2791.4901\n",
      "Epoch 1098/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2444.4358 - val_loss: 2790.0475\n",
      "Epoch 1099/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2446.0706 - val_loss: 2794.4056\n",
      "Epoch 1100/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2444.3569 - val_loss: 2790.3346\n",
      "Epoch 1101/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2444.3262 - val_loss: 2789.7693\n",
      "Epoch 1102/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2444.6354 - val_loss: 2791.2894\n",
      "Epoch 1103/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2446.4924 - val_loss: 2788.4286\n",
      "Epoch 1104/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2444.1987 - val_loss: 2791.5601\n",
      "Epoch 1105/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2444.1454 - val_loss: 2792.0437\n",
      "Epoch 1106/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2444.1495 - val_loss: 2790.9954\n",
      "Epoch 1107/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2444.3487 - val_loss: 2790.0868\n",
      "Epoch 1108/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2444.7836 - val_loss: 2792.5064\n",
      "Epoch 1109/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2444.0011 - val_loss: 2791.2109\n",
      "Epoch 1110/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2444.1098 - val_loss: 2792.0907\n",
      "Epoch 1111/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2444.1261 - val_loss: 2790.5965\n",
      "Epoch 1112/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2443.9320 - val_loss: 2790.9494\n",
      "Epoch 1113/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2443.9110 - val_loss: 2790.2519\n",
      "Epoch 1114/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 2444.7676 - val_loss: 2791.2417\n",
      "Epoch 1115/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2444.7902 - val_loss: 2792.8457\n",
      "Epoch 1116/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2444.2204 - val_loss: 2793.3082\n",
      "Epoch 1117/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2443.7783 - val_loss: 2792.2225\n",
      "Epoch 1118/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2444.4769 - val_loss: 2790.6962\n",
      "Epoch 1119/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2444.3395 - val_loss: 2790.9889\n",
      "Epoch 1120/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2444.4279 - val_loss: 2789.8371\n",
      "Epoch 1121/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 2443.8891 - val_loss: 2792.2659\n",
      "Epoch 1122/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2444.2401 - val_loss: 2793.9405\n",
      "Epoch 1123/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 2443.5903 - val_loss: 2790.9159\n",
      "Epoch 1124/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2444.1395 - val_loss: 2791.8850\n",
      "Epoch 1125/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2443.6005 - val_loss: 2790.7578\n",
      "Epoch 1126/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2444.1669 - val_loss: 2791.4110\n",
      "Epoch 1127/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2443.9485 - val_loss: 2791.1183\n",
      "Epoch 1128/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2443.2883 - val_loss: 2791.2784\n",
      "Epoch 1129/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2443.6315 - val_loss: 2792.1967\n",
      "Epoch 1130/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2443.3087 - val_loss: 2790.7404\n",
      "Epoch 1131/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2443.5941 - val_loss: 2792.1103\n",
      "Epoch 1132/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2443.5924 - val_loss: 2790.5647\n",
      "Epoch 1133/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 2444.1376 - val_loss: 2790.7510\n",
      "Epoch 1134/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2444.0011 - val_loss: 2794.1162\n",
      "Epoch 1135/2500\n",
      "892/892 [==============================] - 0s 144us/step - loss: 2443.9298 - val_loss: 2790.7847\n",
      "Epoch 1136/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2443.3982 - val_loss: 2791.9749\n",
      "Epoch 1137/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2443.1294 - val_loss: 2791.7799\n",
      "Epoch 1138/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2442.9936 - val_loss: 2790.7140\n",
      "Epoch 1139/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2445.4569 - val_loss: 2794.3684\n",
      "Epoch 1140/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2442.7927 - val_loss: 2791.9684\n",
      "Epoch 1141/2500\n",
      "892/892 [==============================] - 0s 137us/step - loss: 2444.2139 - val_loss: 2789.2427\n",
      "Epoch 1142/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2443.2406 - val_loss: 2790.3557\n",
      "Epoch 1143/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2443.2888 - val_loss: 2791.7820\n",
      "Epoch 1144/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2442.9987 - val_loss: 2791.1740\n",
      "Epoch 1145/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2443.4182 - val_loss: 2791.3568\n",
      "Epoch 1146/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2442.9778 - val_loss: 2792.3482\n",
      "Epoch 1147/2500\n",
      "892/892 [==============================] - 0s 49us/step - loss: 2444.0687 - val_loss: 2791.7979\n",
      "Epoch 1148/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2444.2424 - val_loss: 2792.4021\n",
      "Epoch 1149/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2443.4101 - val_loss: 2792.1102\n",
      "Epoch 1150/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2443.1434 - val_loss: 2791.9875\n",
      "Epoch 1151/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2442.7786 - val_loss: 2791.0409\n",
      "Epoch 1152/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2443.1153 - val_loss: 2791.3515\n",
      "Epoch 1153/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2443.3020 - val_loss: 2790.4173\n",
      "Epoch 1154/2500\n",
      "892/892 [==============================] - 0s 146us/step - loss: 2442.9390 - val_loss: 2792.2597\n",
      "Epoch 1155/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2442.8184 - val_loss: 2793.3890\n",
      "Epoch 1156/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2442.6259 - val_loss: 2791.5952\n",
      "Epoch 1157/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2442.9138 - val_loss: 2791.9410\n",
      "Epoch 1158/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2443.2414 - val_loss: 2790.2126\n",
      "Epoch 1159/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2443.4985 - val_loss: 2790.5231\n",
      "Epoch 1160/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2442.8645 - val_loss: 2793.9097\n",
      "Epoch 1161/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2443.0074 - val_loss: 2793.7514\n",
      "Epoch 1162/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2442.8942 - val_loss: 2790.7224\n",
      "Epoch 1163/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2443.6148 - val_loss: 2794.2197\n",
      "Epoch 1164/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2442.4529 - val_loss: 2790.6417\n",
      "Epoch 1165/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2442.8594 - val_loss: 2791.3019\n",
      "Epoch 1166/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2442.4056 - val_loss: 2790.8800\n",
      "Epoch 1167/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2442.4869 - val_loss: 2790.4464\n",
      "Epoch 1168/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2443.2177 - val_loss: 2791.2438\n",
      "Epoch 1169/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2442.3634 - val_loss: 2790.9649\n",
      "Epoch 1170/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2442.7875 - val_loss: 2791.3048\n",
      "Epoch 1171/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2442.8866 - val_loss: 2793.4905\n",
      "Epoch 1172/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2442.7489 - val_loss: 2793.3641\n",
      "Epoch 1173/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2442.7354 - val_loss: 2792.1397\n",
      "Epoch 1174/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2443.7053 - val_loss: 2789.1297\n",
      "Epoch 1175/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2442.1592 - val_loss: 2792.7739\n",
      "Epoch 1176/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2442.2306 - val_loss: 2792.4213\n",
      "Epoch 1177/2500\n",
      "892/892 [==============================] - 0s 162us/step - loss: 2443.0042 - val_loss: 2791.5536\n",
      "Epoch 1178/2500\n",
      "892/892 [==============================] - 0s 136us/step - loss: 2442.8109 - val_loss: 2793.7355\n",
      "Epoch 1179/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2442.5450 - val_loss: 2791.0823\n",
      "Epoch 1180/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2442.9082 - val_loss: 2790.8550\n",
      "Epoch 1181/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2442.4847 - val_loss: 2792.1099\n",
      "Epoch 1182/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2442.6759 - val_loss: 2793.0069\n",
      "Epoch 1183/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2442.6560 - val_loss: 2794.3880\n",
      "Epoch 1184/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2443.0146 - val_loss: 2789.9007\n",
      "Epoch 1185/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2442.4765 - val_loss: 2789.7410\n",
      "Epoch 1186/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2442.9629 - val_loss: 2793.3399\n",
      "Epoch 1187/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2443.0723 - val_loss: 2790.0964\n",
      "Epoch 1188/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2441.8428 - val_loss: 2791.5801\n",
      "Epoch 1189/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2442.2358 - val_loss: 2792.6123\n",
      "Epoch 1190/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2442.2116 - val_loss: 2792.7094\n",
      "Epoch 1191/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2442.2135 - val_loss: 2794.0448\n",
      "Epoch 1192/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2442.4038 - val_loss: 2793.1596\n",
      "Epoch 1193/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2442.8673 - val_loss: 2793.1902\n",
      "Epoch 1194/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2441.8041 - val_loss: 2792.3609\n",
      "Epoch 1195/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2441.6908 - val_loss: 2790.8565\n",
      "Epoch 1196/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2442.2904 - val_loss: 2790.4828\n",
      "Epoch 1197/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2441.7454 - val_loss: 2792.9352\n",
      "Epoch 1198/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2441.7192 - val_loss: 2792.6551\n",
      "Epoch 1199/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2441.9380 - val_loss: 2793.3765\n",
      "Epoch 1200/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2441.7759 - val_loss: 2792.9312\n",
      "Epoch 1201/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2442.8962 - val_loss: 2789.9095\n",
      "Epoch 1202/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2441.8127 - val_loss: 2793.1346\n",
      "Epoch 1203/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2441.4587 - val_loss: 2791.9021\n",
      "Epoch 1204/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2442.1315 - val_loss: 2792.4879\n",
      "Epoch 1205/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2441.5826 - val_loss: 2791.6628\n",
      "Epoch 1206/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2442.0842 - val_loss: 2792.0248\n",
      "Epoch 1207/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2442.1094 - val_loss: 2793.3388\n",
      "Epoch 1208/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2441.6916 - val_loss: 2793.2145\n",
      "Epoch 1209/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2441.8083 - val_loss: 2790.8645\n",
      "Epoch 1210/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2441.3788 - val_loss: 2792.4993\n",
      "Epoch 1211/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2441.4062 - val_loss: 2790.9889\n",
      "Epoch 1212/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2442.0463 - val_loss: 2793.1480\n",
      "Epoch 1213/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2441.7303 - val_loss: 2792.3135\n",
      "Epoch 1214/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2441.6069 - val_loss: 2792.5545\n",
      "Epoch 1215/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2441.4753 - val_loss: 2792.1555\n",
      "Epoch 1216/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2441.4154 - val_loss: 2792.3327\n",
      "Epoch 1217/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2441.4053 - val_loss: 2792.8457\n",
      "Epoch 1218/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 2441.4544 - val_loss: 2790.9917\n",
      "Epoch 1219/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2441.4177 - val_loss: 2793.1912\n",
      "Epoch 1220/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2441.4062 - val_loss: 2794.0898\n",
      "Epoch 1221/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2441.3210 - val_loss: 2791.5767\n",
      "Epoch 1222/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2441.3772 - val_loss: 2791.8094\n",
      "Epoch 1223/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2441.8960 - val_loss: 2793.1058\n",
      "Epoch 1224/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2441.9125 - val_loss: 2791.1272\n",
      "Epoch 1225/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2441.5250 - val_loss: 2793.7318\n",
      "Epoch 1226/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2442.4243 - val_loss: 2792.2132\n",
      "Epoch 1227/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2440.9190 - val_loss: 2792.9259\n",
      "Epoch 1228/2500\n",
      "892/892 [==============================] - 0s 165us/step - loss: 2442.2424 - val_loss: 2791.8439\n",
      "Epoch 1229/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2441.7869 - val_loss: 2793.2286\n",
      "Epoch 1230/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2441.2462 - val_loss: 2794.1828\n",
      "Epoch 1231/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2442.3445 - val_loss: 2790.8486\n",
      "Epoch 1232/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2440.8546 - val_loss: 2793.5597\n",
      "Epoch 1233/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2440.8511 - val_loss: 2792.1210\n",
      "Epoch 1234/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2441.2423 - val_loss: 2793.7824\n",
      "Epoch 1235/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2441.1964 - val_loss: 2791.8993\n",
      "Epoch 1236/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2440.9233 - val_loss: 2792.4506\n",
      "Epoch 1237/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2441.5684 - val_loss: 2790.8049\n",
      "Epoch 1238/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2440.6970 - val_loss: 2793.9843\n",
      "Epoch 1239/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2441.2495 - val_loss: 2792.4709\n",
      "Epoch 1240/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2441.0219 - val_loss: 2793.3253\n",
      "Epoch 1241/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2441.2633 - val_loss: 2790.8526\n",
      "Epoch 1242/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2441.9282 - val_loss: 2793.0366\n",
      "Epoch 1243/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2440.8984 - val_loss: 2791.8105\n",
      "Epoch 1244/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2441.7594 - val_loss: 2791.3399\n",
      "Epoch 1245/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2441.0033 - val_loss: 2793.1712\n",
      "Epoch 1246/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2440.9216 - val_loss: 2791.7474\n",
      "Epoch 1247/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2441.6324 - val_loss: 2793.9566\n",
      "Epoch 1248/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2441.2955 - val_loss: 2793.9963\n",
      "Epoch 1249/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2440.6891 - val_loss: 2793.1847\n",
      "Epoch 1250/2500\n",
      "892/892 [==============================] - 0s 144us/step - loss: 2441.5602 - val_loss: 2791.6326\n",
      "Epoch 1251/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2441.3205 - val_loss: 2791.8709\n",
      "Epoch 1252/2500\n",
      "892/892 [==============================] - 0s 150us/step - loss: 2441.0438 - val_loss: 2795.2954\n",
      "Epoch 1253/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2440.5196 - val_loss: 2792.5472\n",
      "Epoch 1254/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2440.5785 - val_loss: 2792.3152\n",
      "Epoch 1255/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2441.1084 - val_loss: 2793.8246\n",
      "Epoch 1256/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2440.3491 - val_loss: 2793.5042\n",
      "Epoch 1257/2500\n",
      "892/892 [==============================] - 0s 196us/step - loss: 2440.1300 - val_loss: 2793.2663\n",
      "Epoch 1258/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2440.5653 - val_loss: 2791.9538\n",
      "Epoch 1259/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2441.0791 - val_loss: 2791.4360\n",
      "Epoch 1260/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2440.9822 - val_loss: 2794.1504\n",
      "Epoch 1261/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2440.3714 - val_loss: 2793.0011\n",
      "Epoch 1262/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2440.3416 - val_loss: 2793.3190\n",
      "Epoch 1263/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2440.6809 - val_loss: 2793.9672\n",
      "Epoch 1264/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2440.2911 - val_loss: 2791.8931\n",
      "Epoch 1265/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2440.4659 - val_loss: 2790.3708\n",
      "Epoch 1266/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2440.3038 - val_loss: 2793.1785\n",
      "Epoch 1267/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2439.9202 - val_loss: 2792.5683\n",
      "Epoch 1268/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2440.3929 - val_loss: 2792.3209\n",
      "Epoch 1269/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2440.3424 - val_loss: 2792.1877\n",
      "Epoch 1270/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2441.9490 - val_loss: 2790.6402\n",
      "Epoch 1271/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 2441.1339 - val_loss: 2794.3061\n",
      "Epoch 1272/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2440.2531 - val_loss: 2791.8469\n",
      "Epoch 1273/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2439.8322 - val_loss: 2794.1796\n",
      "Epoch 1274/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2440.1625 - val_loss: 2793.4583\n",
      "Epoch 1275/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2439.9282 - val_loss: 2791.8931\n",
      "Epoch 1276/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2441.5325 - val_loss: 2790.6893\n",
      "Epoch 1277/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2439.8074 - val_loss: 2793.8534\n",
      "Epoch 1278/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2439.7838 - val_loss: 2794.6541\n",
      "Epoch 1279/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2439.7175 - val_loss: 2793.8084\n",
      "Epoch 1280/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2439.9121 - val_loss: 2791.9002\n",
      "Epoch 1281/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2439.8586 - val_loss: 2791.6262\n",
      "Epoch 1282/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2440.9623 - val_loss: 2791.7317\n",
      "Epoch 1283/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2439.9663 - val_loss: 2794.1033\n",
      "Epoch 1284/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2440.0428 - val_loss: 2791.8561\n",
      "Epoch 1285/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2439.9985 - val_loss: 2792.4593\n",
      "Epoch 1286/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2440.6245 - val_loss: 2794.5781\n",
      "Epoch 1287/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2439.7693 - val_loss: 2793.3193\n",
      "Epoch 1288/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2439.5820 - val_loss: 2793.5905\n",
      "Epoch 1289/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2440.4261 - val_loss: 2790.5448\n",
      "Epoch 1290/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2439.6269 - val_loss: 2791.3422\n",
      "Epoch 1291/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2440.0804 - val_loss: 2792.1698\n",
      "Epoch 1292/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2439.7492 - val_loss: 2793.8661\n",
      "Epoch 1293/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2440.7146 - val_loss: 2794.1549\n",
      "Epoch 1294/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2439.5228 - val_loss: 2794.0178\n",
      "Epoch 1295/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2439.7150 - val_loss: 2794.1806\n",
      "Epoch 1296/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2440.0814 - val_loss: 2792.0223\n",
      "Epoch 1297/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2439.5322 - val_loss: 2792.9765\n",
      "Epoch 1298/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2439.8259 - val_loss: 2793.6888\n",
      "Epoch 1299/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2439.6941 - val_loss: 2795.5555\n",
      "Epoch 1300/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2439.5544 - val_loss: 2791.9656\n",
      "Epoch 1301/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2439.3225 - val_loss: 2791.8122\n",
      "Epoch 1302/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2439.9703 - val_loss: 2792.9709\n",
      "Epoch 1303/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2439.4867 - val_loss: 2795.0105\n",
      "Epoch 1304/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 2439.8027 - val_loss: 2792.2665\n",
      "Epoch 1305/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2439.3065 - val_loss: 2793.4418\n",
      "Epoch 1306/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2439.1599 - val_loss: 2792.6534\n",
      "Epoch 1307/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2439.5571 - val_loss: 2791.7556\n",
      "Epoch 1308/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2439.0145 - val_loss: 2793.7007\n",
      "Epoch 1309/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2440.1713 - val_loss: 2794.0502\n",
      "Epoch 1310/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2439.7250 - val_loss: 2793.2922\n",
      "Epoch 1311/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2439.8841 - val_loss: 2794.6687\n",
      "Epoch 1312/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2439.0823 - val_loss: 2792.0788\n",
      "Epoch 1313/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2439.3112 - val_loss: 2791.4810\n",
      "Epoch 1314/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2439.4747 - val_loss: 2792.2269\n",
      "Epoch 1315/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2440.3005 - val_loss: 2794.4859\n",
      "Epoch 1316/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2439.7845 - val_loss: 2792.2357\n",
      "Epoch 1317/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2439.0156 - val_loss: 2794.4838\n",
      "Epoch 1318/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2438.8476 - val_loss: 2794.4514\n",
      "Epoch 1319/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2439.3024 - val_loss: 2792.7517\n",
      "Epoch 1320/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2439.1652 - val_loss: 2794.0912\n",
      "Epoch 1321/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2438.7375 - val_loss: 2793.4424\n",
      "Epoch 1322/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2438.9865 - val_loss: 2794.0230\n",
      "Epoch 1323/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2439.1381 - val_loss: 2792.4021\n",
      "Epoch 1324/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2439.2581 - val_loss: 2791.4769\n",
      "Epoch 1325/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2440.2473 - val_loss: 2795.3783\n",
      "Epoch 1326/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2438.7010 - val_loss: 2793.3458\n",
      "Epoch 1327/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2439.6493 - val_loss: 2792.4237\n",
      "Epoch 1328/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2438.9924 - val_loss: 2792.0020\n",
      "Epoch 1329/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2438.6569 - val_loss: 2793.8388\n",
      "Epoch 1330/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2439.2719 - val_loss: 2793.6572\n",
      "Epoch 1331/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2439.1746 - val_loss: 2791.6576\n",
      "Epoch 1332/2500\n",
      "892/892 [==============================] - 0s 132us/step - loss: 2439.1286 - val_loss: 2793.4520\n",
      "Epoch 1333/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2438.9621 - val_loss: 2794.6948\n",
      "Epoch 1334/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2438.6088 - val_loss: 2793.0242\n",
      "Epoch 1335/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2438.7027 - val_loss: 2793.8824\n",
      "Epoch 1336/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2438.8005 - val_loss: 2792.9992\n",
      "Epoch 1337/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2438.9118 - val_loss: 2792.3122\n",
      "Epoch 1338/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2438.5907 - val_loss: 2794.2295\n",
      "Epoch 1339/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2439.1976 - val_loss: 2793.6765\n",
      "Epoch 1340/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2440.1404 - val_loss: 2791.0704\n",
      "Epoch 1341/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2438.7816 - val_loss: 2795.4328\n",
      "Epoch 1342/2500\n",
      "892/892 [==============================] - 0s 149us/step - loss: 2439.1536 - val_loss: 2794.1834\n",
      "Epoch 1343/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2439.3126 - val_loss: 2794.8492\n",
      "Epoch 1344/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2438.8323 - val_loss: 2792.5003\n",
      "Epoch 1345/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2438.7463 - val_loss: 2793.9043\n",
      "Epoch 1346/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2438.6657 - val_loss: 2793.6692\n",
      "Epoch 1347/2500\n",
      "892/892 [==============================] - 0s 147us/step - loss: 2438.4311 - val_loss: 2792.3761\n",
      "Epoch 1348/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2438.6849 - val_loss: 2794.8495\n",
      "Epoch 1349/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2438.4791 - val_loss: 2793.6487\n",
      "Epoch 1350/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2438.7844 - val_loss: 2792.1337\n",
      "Epoch 1351/2500\n",
      "892/892 [==============================] - 0s 126us/step - loss: 2439.0315 - val_loss: 2793.5329\n",
      "Epoch 1352/2500\n",
      "892/892 [==============================] - 0s 164us/step - loss: 2438.8499 - val_loss: 2795.2216\n",
      "Epoch 1353/2500\n",
      "892/892 [==============================] - 0s 149us/step - loss: 2438.3732 - val_loss: 2794.4729\n",
      "Epoch 1354/2500\n",
      "892/892 [==============================] - 0s 139us/step - loss: 2439.1376 - val_loss: 2792.1803\n",
      "Epoch 1355/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2438.3326 - val_loss: 2794.3957\n",
      "Epoch 1356/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2438.9759 - val_loss: 2792.1097\n",
      "Epoch 1357/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2438.3696 - val_loss: 2795.7340\n",
      "Epoch 1358/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2438.8395 - val_loss: 2794.8490\n",
      "Epoch 1359/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2438.7731 - val_loss: 2793.3083\n",
      "Epoch 1360/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2439.0765 - val_loss: 2794.9021\n",
      "Epoch 1361/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2438.0597 - val_loss: 2793.3743\n",
      "Epoch 1362/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2439.6952 - val_loss: 2792.9887\n",
      "Epoch 1363/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2438.3467 - val_loss: 2793.5694\n",
      "Epoch 1364/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2439.3043 - val_loss: 2792.9489\n",
      "Epoch 1365/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2438.3084 - val_loss: 2794.2283\n",
      "Epoch 1366/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2438.3797 - val_loss: 2792.1934\n",
      "Epoch 1367/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2438.3075 - val_loss: 2793.4782\n",
      "Epoch 1368/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2440.5153 - val_loss: 2792.0420\n",
      "Epoch 1369/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2437.6802 - val_loss: 2793.0473\n",
      "Epoch 1370/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2438.1406 - val_loss: 2794.8105\n",
      "Epoch 1371/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2438.3625 - val_loss: 2795.4282\n",
      "Epoch 1372/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2439.0318 - val_loss: 2792.3713\n",
      "Epoch 1373/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2437.9454 - val_loss: 2794.0850\n",
      "Epoch 1374/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2437.9462 - val_loss: 2794.9952\n",
      "Epoch 1375/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2438.6047 - val_loss: 2793.6389\n",
      "Epoch 1376/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2438.1449 - val_loss: 2794.2251\n",
      "Epoch 1377/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2437.6250 - val_loss: 2792.6917\n",
      "Epoch 1378/2500\n",
      "892/892 [==============================] - 0s 142us/step - loss: 2437.7728 - val_loss: 2793.3784\n",
      "Epoch 1379/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2437.9420 - val_loss: 2793.4644\n",
      "Epoch 1380/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2437.9135 - val_loss: 2793.4494\n",
      "Epoch 1381/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2437.8292 - val_loss: 2792.9577\n",
      "Epoch 1382/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2437.7348 - val_loss: 2793.5228\n",
      "Epoch 1383/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2438.2553 - val_loss: 2795.5777\n",
      "Epoch 1384/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2438.2430 - val_loss: 2794.3832\n",
      "Epoch 1385/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2438.4813 - val_loss: 2792.1794\n",
      "Epoch 1386/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2437.8049 - val_loss: 2793.6477\n",
      "Epoch 1387/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2438.2352 - val_loss: 2792.0316\n",
      "Epoch 1388/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2438.4785 - val_loss: 2795.5706\n",
      "Epoch 1389/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2438.9310 - val_loss: 2793.7727\n",
      "Epoch 1390/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2438.0032 - val_loss: 2793.7596\n",
      "Epoch 1391/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2438.7685 - val_loss: 2794.4268\n",
      "Epoch 1392/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2437.5478 - val_loss: 2793.4589\n",
      "Epoch 1393/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2437.6250 - val_loss: 2793.8828\n",
      "Epoch 1394/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2437.5426 - val_loss: 2792.8913\n",
      "Epoch 1395/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2437.7060 - val_loss: 2792.6406\n",
      "Epoch 1396/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2437.4235 - val_loss: 2793.9299\n",
      "Epoch 1397/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2437.5179 - val_loss: 2794.0231\n",
      "Epoch 1398/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2437.3625 - val_loss: 2795.0268\n",
      "Epoch 1399/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2437.4427 - val_loss: 2793.1017\n",
      "Epoch 1400/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2437.3584 - val_loss: 2794.2812\n",
      "Epoch 1401/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2437.4486 - val_loss: 2794.0714\n",
      "Epoch 1402/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2437.6779 - val_loss: 2792.7153\n",
      "Epoch 1403/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2437.5419 - val_loss: 2796.0893\n",
      "Epoch 1404/2500\n",
      "892/892 [==============================] - 0s 126us/step - loss: 2438.4103 - val_loss: 2793.3065\n",
      "Epoch 1405/2500\n",
      "892/892 [==============================] - 0s 155us/step - loss: 2437.1683 - val_loss: 2793.8789\n",
      "Epoch 1406/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2437.4212 - val_loss: 2795.6174\n",
      "Epoch 1407/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2439.9006 - val_loss: 2792.0654\n",
      "Epoch 1408/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2437.6142 - val_loss: 2795.3824\n",
      "Epoch 1409/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2437.9180 - val_loss: 2795.1102\n",
      "Epoch 1410/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2437.7392 - val_loss: 2792.5910\n",
      "Epoch 1411/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2437.5271 - val_loss: 2793.4621\n",
      "Epoch 1412/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2437.5144 - val_loss: 2795.1060\n",
      "Epoch 1413/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2437.9204 - val_loss: 2792.4767\n",
      "Epoch 1414/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2436.9765 - val_loss: 2793.7366\n",
      "Epoch 1415/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2437.1582 - val_loss: 2794.4493\n",
      "Epoch 1416/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2437.8858 - val_loss: 2795.1128\n",
      "Epoch 1417/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2437.2111 - val_loss: 2794.1704\n",
      "Epoch 1418/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2437.2798 - val_loss: 2792.6512\n",
      "Epoch 1419/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2437.8280 - val_loss: 2794.1170\n",
      "Epoch 1420/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2437.0391 - val_loss: 2792.7440\n",
      "Epoch 1421/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2437.1506 - val_loss: 2792.9088\n",
      "Epoch 1422/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2437.4034 - val_loss: 2793.3041\n",
      "Epoch 1423/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2437.1203 - val_loss: 2794.4770\n",
      "Epoch 1424/2500\n",
      "892/892 [==============================] - 0s 187us/step - loss: 2437.5314 - val_loss: 2794.7575\n",
      "Epoch 1425/2500\n",
      "892/892 [==============================] - 0s 145us/step - loss: 2437.2141 - val_loss: 2796.6432\n",
      "Epoch 1426/2500\n",
      "892/892 [==============================] - 0s 135us/step - loss: 2437.1854 - val_loss: 2793.0387\n",
      "Epoch 1427/2500\n",
      "892/892 [==============================] - 0s 170us/step - loss: 2436.9510 - val_loss: 2793.3216\n",
      "Epoch 1428/2500\n",
      "892/892 [==============================] - 0s 167us/step - loss: 2437.5778 - val_loss: 2792.7202\n",
      "Epoch 1429/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2436.9323 - val_loss: 2794.8402\n",
      "Epoch 1430/2500\n",
      "892/892 [==============================] - 0s 143us/step - loss: 2436.7763 - val_loss: 2795.6706\n",
      "Epoch 1431/2500\n",
      "892/892 [==============================] - 0s 229us/step - loss: 2437.7021 - val_loss: 2794.4591\n",
      "Epoch 1432/2500\n",
      "892/892 [==============================] - 0s 212us/step - loss: 2437.3312 - val_loss: 2793.0203\n",
      "Epoch 1433/2500\n",
      "892/892 [==============================] - 0s 216us/step - loss: 2437.0380 - val_loss: 2793.1853\n",
      "Epoch 1434/2500\n",
      "892/892 [==============================] - 0s 171us/step - loss: 2437.2088 - val_loss: 2795.6290\n",
      "Epoch 1435/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2437.7308 - val_loss: 2793.4015\n",
      "Epoch 1436/2500\n",
      "892/892 [==============================] - 0s 183us/step - loss: 2437.7612 - val_loss: 2795.3813\n",
      "Epoch 1437/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2436.9772 - val_loss: 2795.3144\n",
      "Epoch 1438/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2437.0398 - val_loss: 2794.9460\n",
      "Epoch 1439/2500\n",
      "892/892 [==============================] - 0s 135us/step - loss: 2436.5719 - val_loss: 2792.9424\n",
      "Epoch 1440/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2436.3925 - val_loss: 2793.7376\n",
      "Epoch 1441/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2437.2872 - val_loss: 2795.5155\n",
      "Epoch 1442/2500\n",
      "892/892 [==============================] - 0s 146us/step - loss: 2437.7025 - val_loss: 2792.4780\n",
      "Epoch 1443/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2437.0958 - val_loss: 2792.7690\n",
      "Epoch 1444/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2436.5712 - val_loss: 2795.6381\n",
      "Epoch 1445/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2437.0554 - val_loss: 2795.4302\n",
      "Epoch 1446/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2436.7534 - val_loss: 2794.9730\n",
      "Epoch 1447/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2436.6650 - val_loss: 2794.4088\n",
      "Epoch 1448/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2437.0604 - val_loss: 2792.9721\n",
      "Epoch 1449/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2436.5599 - val_loss: 2793.5214\n",
      "Epoch 1450/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2436.7075 - val_loss: 2796.0440\n",
      "Epoch 1451/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2436.5233 - val_loss: 2794.3366\n",
      "Epoch 1452/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2436.7033 - val_loss: 2795.4367\n",
      "Epoch 1453/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2436.8688 - val_loss: 2793.5030\n",
      "Epoch 1454/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2436.5356 - val_loss: 2794.6638\n",
      "Epoch 1455/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2436.9663 - val_loss: 2793.5007\n",
      "Epoch 1456/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2437.0407 - val_loss: 2794.1264\n",
      "Epoch 1457/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2437.1356 - val_loss: 2796.4259\n",
      "Epoch 1458/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2436.7271 - val_loss: 2793.0770\n",
      "Epoch 1459/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2436.4696 - val_loss: 2794.3503\n",
      "Epoch 1460/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2436.1302 - val_loss: 2793.8230\n",
      "Epoch 1461/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2436.1152 - val_loss: 2793.6456\n",
      "Epoch 1462/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2437.4178 - val_loss: 2793.1818\n",
      "Epoch 1463/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2436.2999 - val_loss: 2794.1500\n",
      "Epoch 1464/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2436.7034 - val_loss: 2794.8432\n",
      "Epoch 1465/2500\n",
      "892/892 [==============================] - 0s 186us/step - loss: 2436.4785 - val_loss: 2792.8931\n",
      "Epoch 1466/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2437.2525 - val_loss: 2794.7654\n",
      "Epoch 1467/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2436.6308 - val_loss: 2792.9964\n",
      "Epoch 1468/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2436.6044 - val_loss: 2795.6603\n",
      "Epoch 1469/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2436.5464 - val_loss: 2794.0245\n",
      "Epoch 1470/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2436.8416 - val_loss: 2792.5796\n",
      "Epoch 1471/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2435.9822 - val_loss: 2794.8019\n",
      "Epoch 1472/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2436.0165 - val_loss: 2793.4681\n",
      "Epoch 1473/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2436.1025 - val_loss: 2793.2845\n",
      "Epoch 1474/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2437.6448 - val_loss: 2791.9816\n",
      "Epoch 1475/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2436.2562 - val_loss: 2794.4701\n",
      "Epoch 1476/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2436.5381 - val_loss: 2796.5485\n",
      "Epoch 1477/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2436.0007 - val_loss: 2795.1584\n",
      "Epoch 1478/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2436.5056 - val_loss: 2797.6058\n",
      "Epoch 1479/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2435.8243 - val_loss: 2794.8894\n",
      "Epoch 1480/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2435.8599 - val_loss: 2793.1893\n",
      "Epoch 1481/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2437.0379 - val_loss: 2793.7000\n",
      "Epoch 1482/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2437.5576 - val_loss: 2796.9529\n",
      "Epoch 1483/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2436.1991 - val_loss: 2792.9922\n",
      "Epoch 1484/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2436.8301 - val_loss: 2795.4078\n",
      "Epoch 1485/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2435.6895 - val_loss: 2793.7062\n",
      "Epoch 1486/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2436.1982 - val_loss: 2791.8666\n",
      "Epoch 1487/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 2435.6626 - val_loss: 2793.6892\n",
      "Epoch 1488/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2436.0276 - val_loss: 2793.0174\n",
      "Epoch 1489/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 2435.8533 - val_loss: 2796.3468\n",
      "Epoch 1490/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2436.0116 - val_loss: 2794.6945\n",
      "Epoch 1491/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2435.8175 - val_loss: 2794.2615\n",
      "Epoch 1492/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2436.3938 - val_loss: 2794.7208\n",
      "Epoch 1493/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2436.4541 - val_loss: 2793.4790\n",
      "Epoch 1494/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2436.1294 - val_loss: 2794.1920\n",
      "Epoch 1495/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2435.5678 - val_loss: 2796.0188\n",
      "Epoch 1496/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2436.0551 - val_loss: 2794.4718\n",
      "Epoch 1497/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2435.7974 - val_loss: 2794.2132\n",
      "Epoch 1498/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2435.6835 - val_loss: 2796.0824\n",
      "Epoch 1499/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 2435.7154 - val_loss: 2795.0032\n",
      "Epoch 1500/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2435.5897 - val_loss: 2794.8101\n",
      "Epoch 1501/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2436.5484 - val_loss: 2794.1806\n",
      "Epoch 1502/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2435.9326 - val_loss: 2794.1361\n",
      "Epoch 1503/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2435.7215 - val_loss: 2795.6174\n",
      "Epoch 1504/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2435.9548 - val_loss: 2793.7597\n",
      "Epoch 1505/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2435.8738 - val_loss: 2793.7064\n",
      "Epoch 1506/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2436.1517 - val_loss: 2797.5998\n",
      "Epoch 1507/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2435.5677 - val_loss: 2796.5519\n",
      "Epoch 1508/2500\n",
      "892/892 [==============================] - 0s 164us/step - loss: 2436.5455 - val_loss: 2793.8663\n",
      "Epoch 1509/2500\n",
      "892/892 [==============================] - 0s 207us/step - loss: 2435.3965 - val_loss: 2795.0052\n",
      "Epoch 1510/2500\n",
      "892/892 [==============================] - 0s 143us/step - loss: 2436.0870 - val_loss: 2795.7150\n",
      "Epoch 1511/2500\n",
      "892/892 [==============================] - 0s 178us/step - loss: 2435.3904 - val_loss: 2794.7910\n",
      "Epoch 1512/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2435.6642 - val_loss: 2795.0179\n",
      "Epoch 1513/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2435.3167 - val_loss: 2793.9245\n",
      "Epoch 1514/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2435.3986 - val_loss: 2795.2438\n",
      "Epoch 1515/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2435.4618 - val_loss: 2794.4528\n",
      "Epoch 1516/2500\n",
      "892/892 [==============================] - 0s 142us/step - loss: 2435.8717 - val_loss: 2794.2352\n",
      "Epoch 1517/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2435.5392 - val_loss: 2794.1308\n",
      "Epoch 1518/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2435.2475 - val_loss: 2794.7391\n",
      "Epoch 1519/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2435.5128 - val_loss: 2795.7559\n",
      "Epoch 1520/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2435.8952 - val_loss: 2797.1999\n",
      "Epoch 1521/2500\n",
      "892/892 [==============================] - 0s 171us/step - loss: 2435.4012 - val_loss: 2793.4581\n",
      "Epoch 1522/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2436.0944 - val_loss: 2796.7354\n",
      "Epoch 1523/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2435.1785 - val_loss: 2794.9687\n",
      "Epoch 1524/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2435.9426 - val_loss: 2794.6217\n",
      "Epoch 1525/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2435.1191 - val_loss: 2794.5211\n",
      "Epoch 1526/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2435.2040 - val_loss: 2793.6792\n",
      "Epoch 1527/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2435.1255 - val_loss: 2794.2881\n",
      "Epoch 1528/2500\n",
      "892/892 [==============================] - 0s 136us/step - loss: 2434.9503 - val_loss: 2795.1874\n",
      "Epoch 1529/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2435.9149 - val_loss: 2795.3943\n",
      "Epoch 1530/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2435.1168 - val_loss: 2795.1614\n",
      "Epoch 1531/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2435.1610 - val_loss: 2795.3077\n",
      "Epoch 1532/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2435.3172 - val_loss: 2793.9310\n",
      "Epoch 1533/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2435.0999 - val_loss: 2795.7799\n",
      "Epoch 1534/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2435.8213 - val_loss: 2794.8449\n",
      "Epoch 1535/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 2435.3514 - val_loss: 2795.6549\n",
      "Epoch 1536/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2435.8231 - val_loss: 2795.0427\n",
      "Epoch 1537/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2436.3290 - val_loss: 2792.2822\n",
      "Epoch 1538/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2436.2189 - val_loss: 2794.6729\n",
      "Epoch 1539/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2434.9823 - val_loss: 2795.7282\n",
      "Epoch 1540/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2435.1640 - val_loss: 2794.5251\n",
      "Epoch 1541/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2434.6936 - val_loss: 2794.8955\n",
      "Epoch 1542/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2434.8910 - val_loss: 2795.0116\n",
      "Epoch 1543/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2434.7605 - val_loss: 2794.2242\n",
      "Epoch 1544/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2435.1499 - val_loss: 2795.1671\n",
      "Epoch 1545/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2435.4439 - val_loss: 2794.9265\n",
      "Epoch 1546/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2435.6137 - val_loss: 2794.9484\n",
      "Epoch 1547/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2435.5383 - val_loss: 2796.2998\n",
      "Epoch 1548/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2436.5849 - val_loss: 2791.6267\n",
      "Epoch 1549/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2435.7401 - val_loss: 2796.6706\n",
      "Epoch 1550/2500\n",
      "892/892 [==============================] - 0s 159us/step - loss: 2435.1310 - val_loss: 2795.3086\n",
      "Epoch 1551/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2434.9662 - val_loss: 2796.5389\n",
      "Epoch 1552/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2435.0200 - val_loss: 2794.9425\n",
      "Epoch 1553/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2434.5380 - val_loss: 2794.2825\n",
      "Epoch 1554/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2435.9993 - val_loss: 2794.0033\n",
      "Epoch 1555/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2434.5362 - val_loss: 2796.0556\n",
      "Epoch 1556/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2435.4423 - val_loss: 2795.5372\n",
      "Epoch 1557/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2434.8935 - val_loss: 2794.9886\n",
      "Epoch 1558/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2436.3792 - val_loss: 2797.9513\n",
      "Epoch 1559/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2434.9468 - val_loss: 2795.4041\n",
      "Epoch 1560/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2434.8915 - val_loss: 2794.4918\n",
      "Epoch 1561/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2434.8418 - val_loss: 2793.5192\n",
      "Epoch 1562/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2435.2131 - val_loss: 2797.1592\n",
      "Epoch 1563/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2434.9420 - val_loss: 2793.6748\n",
      "Epoch 1564/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2434.6338 - val_loss: 2795.2638\n",
      "Epoch 1565/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2434.3778 - val_loss: 2794.6645\n",
      "Epoch 1566/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2434.6592 - val_loss: 2794.5692\n",
      "Epoch 1567/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2434.6052 - val_loss: 2794.7698\n",
      "Epoch 1568/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2434.7927 - val_loss: 2796.5561\n",
      "Epoch 1569/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2434.9042 - val_loss: 2795.5014\n",
      "Epoch 1570/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2434.4152 - val_loss: 2794.6970\n",
      "Epoch 1571/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2434.1747 - val_loss: 2795.1351\n",
      "Epoch 1572/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2434.8670 - val_loss: 2794.9667\n",
      "Epoch 1573/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2434.8037 - val_loss: 2796.9908\n",
      "Epoch 1574/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 2434.4598 - val_loss: 2795.3215\n",
      "Epoch 1575/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2434.7448 - val_loss: 2794.3329\n",
      "Epoch 1576/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2434.3746 - val_loss: 2795.3236\n",
      "Epoch 1577/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2434.2905 - val_loss: 2795.1830\n",
      "Epoch 1578/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2434.7021 - val_loss: 2796.0921\n",
      "Epoch 1579/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2436.2584 - val_loss: 2793.8466\n",
      "Epoch 1580/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2434.4908 - val_loss: 2796.3546\n",
      "Epoch 1581/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2435.1032 - val_loss: 2794.6172\n",
      "Epoch 1582/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2435.0502 - val_loss: 2797.6552\n",
      "Epoch 1583/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2434.3537 - val_loss: 2795.3500\n",
      "Epoch 1584/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 2434.1582 - val_loss: 2795.3933\n",
      "Epoch 1585/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 2434.4475 - val_loss: 2795.6619\n",
      "Epoch 1586/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2434.4141 - val_loss: 2793.7586\n",
      "Epoch 1587/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2433.9587 - val_loss: 2795.7386\n",
      "Epoch 1588/2500\n",
      "892/892 [==============================] - 0s 52us/step - loss: 2434.0142 - val_loss: 2795.2813\n",
      "Epoch 1589/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2435.0877 - val_loss: 2793.7305\n",
      "Epoch 1590/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2434.3675 - val_loss: 2797.1066\n",
      "Epoch 1591/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2434.5844 - val_loss: 2796.7643\n",
      "Epoch 1592/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2433.8833 - val_loss: 2795.9097\n",
      "Epoch 1593/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2435.8486 - val_loss: 2792.6642\n",
      "Epoch 1594/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2433.9650 - val_loss: 2794.7451\n",
      "Epoch 1595/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2434.3397 - val_loss: 2796.4806\n",
      "Epoch 1596/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2434.5960 - val_loss: 2795.9984\n",
      "Epoch 1597/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2434.1859 - val_loss: 2795.2317\n",
      "Epoch 1598/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2434.7480 - val_loss: 2797.4579\n",
      "Epoch 1599/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2433.9564 - val_loss: 2794.1234\n",
      "Epoch 1600/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2433.8783 - val_loss: 2795.6655\n",
      "Epoch 1601/2500\n",
      "892/892 [==============================] - 0s 139us/step - loss: 2434.3890 - val_loss: 2795.5468\n",
      "Epoch 1602/2500\n",
      "892/892 [==============================] - 0s 142us/step - loss: 2434.1421 - val_loss: 2795.5669\n",
      "Epoch 1603/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2433.9590 - val_loss: 2793.6914\n",
      "Epoch 1604/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2433.9826 - val_loss: 2793.2885\n",
      "Epoch 1605/2500\n",
      "892/892 [==============================] - 0s 140us/step - loss: 2434.2222 - val_loss: 2796.0354\n",
      "Epoch 1606/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2434.7239 - val_loss: 2797.0480\n",
      "Epoch 1607/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2433.5866 - val_loss: 2793.7538\n",
      "Epoch 1608/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2434.5697 - val_loss: 2794.5905\n",
      "Epoch 1609/2500\n",
      "892/892 [==============================] - 0s 164us/step - loss: 2433.6023 - val_loss: 2793.8442\n",
      "Epoch 1610/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2433.9624 - val_loss: 2794.4759\n",
      "Epoch 1611/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2434.1595 - val_loss: 2795.5786\n",
      "Epoch 1612/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2434.2054 - val_loss: 2796.8980\n",
      "Epoch 1613/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2435.2468 - val_loss: 2794.3604\n",
      "Epoch 1614/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2434.7881 - val_loss: 2797.3329\n",
      "Epoch 1615/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2434.2722 - val_loss: 2795.4959\n",
      "Epoch 1616/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2433.8790 - val_loss: 2794.3603\n",
      "Epoch 1617/2500\n",
      "892/892 [==============================] - 0s 132us/step - loss: 2433.6057 - val_loss: 2795.4753\n",
      "Epoch 1618/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2433.9794 - val_loss: 2794.3387\n",
      "Epoch 1619/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2433.7764 - val_loss: 2793.2958\n",
      "Epoch 1620/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2434.0305 - val_loss: 2796.3235\n",
      "Epoch 1621/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2434.2157 - val_loss: 2795.3022\n",
      "Epoch 1622/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2433.5202 - val_loss: 2794.7264\n",
      "Epoch 1623/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2433.6506 - val_loss: 2795.1728\n",
      "Epoch 1624/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2433.6115 - val_loss: 2793.4371\n",
      "Epoch 1625/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2433.3903 - val_loss: 2795.3265\n",
      "Epoch 1626/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2434.7042 - val_loss: 2795.6260\n",
      "Epoch 1627/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2433.8887 - val_loss: 2794.3243\n",
      "Epoch 1628/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2433.8360 - val_loss: 2795.6065\n",
      "Epoch 1629/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2434.4845 - val_loss: 2793.5556\n",
      "Epoch 1630/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2433.5139 - val_loss: 2795.7363\n",
      "Epoch 1631/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2433.7933 - val_loss: 2796.4719\n",
      "Epoch 1632/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2434.1417 - val_loss: 2797.1479\n",
      "Epoch 1633/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2433.1675 - val_loss: 2794.4172\n",
      "Epoch 1634/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2434.3095 - val_loss: 2794.4176\n",
      "Epoch 1635/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2433.4721 - val_loss: 2793.9002\n",
      "Epoch 1636/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2433.5473 - val_loss: 2794.1412\n",
      "Epoch 1637/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2433.4720 - val_loss: 2796.7215\n",
      "Epoch 1638/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2433.7877 - val_loss: 2795.8098\n",
      "Epoch 1639/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2433.6250 - val_loss: 2795.6506\n",
      "Epoch 1640/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2433.6006 - val_loss: 2793.8911\n",
      "Epoch 1641/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2433.1061 - val_loss: 2794.5615\n",
      "Epoch 1642/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2433.9795 - val_loss: 2795.1650\n",
      "Epoch 1643/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2433.7638 - val_loss: 2797.9875\n",
      "Epoch 1644/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2434.9844 - val_loss: 2793.7130\n",
      "Epoch 1645/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2435.8389 - val_loss: 2798.1309\n",
      "Epoch 1646/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2433.2364 - val_loss: 2795.4133\n",
      "Epoch 1647/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2433.0915 - val_loss: 2795.3542\n",
      "Epoch 1648/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2433.7048 - val_loss: 2794.5682\n",
      "Epoch 1649/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2433.5330 - val_loss: 2796.4496\n",
      "Epoch 1650/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2433.7304 - val_loss: 2794.2816\n",
      "Epoch 1651/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2433.5820 - val_loss: 2797.3238\n",
      "Epoch 1652/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2433.3012 - val_loss: 2794.5832\n",
      "Epoch 1653/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2433.0305 - val_loss: 2794.9907\n",
      "Epoch 1654/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2433.6212 - val_loss: 2796.8262\n",
      "Epoch 1655/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2433.0007 - val_loss: 2795.0273\n",
      "Epoch 1656/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2433.2388 - val_loss: 2796.1462\n",
      "Epoch 1657/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 2433.1562 - val_loss: 2794.5359\n",
      "Epoch 1658/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2434.2155 - val_loss: 2795.5293\n",
      "Epoch 1659/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2433.3741 - val_loss: 2793.8246\n",
      "Epoch 1660/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2433.5411 - val_loss: 2794.4742\n",
      "Epoch 1661/2500\n",
      "892/892 [==============================] - 0s 51us/step - loss: 2433.7308 - val_loss: 2796.8865\n",
      "Epoch 1662/2500\n",
      "892/892 [==============================] - 0s 54us/step - loss: 2433.3543 - val_loss: 2796.5490\n",
      "Epoch 1663/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2432.8718 - val_loss: 2795.7517\n",
      "Epoch 1664/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2432.9180 - val_loss: 2794.4636\n",
      "Epoch 1665/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2433.6277 - val_loss: 2791.9993\n",
      "Epoch 1666/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2433.3598 - val_loss: 2792.8827\n",
      "Epoch 1667/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2433.3538 - val_loss: 2796.5752\n",
      "Epoch 1668/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2433.8413 - val_loss: 2797.0138\n",
      "Epoch 1669/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2435.6793 - val_loss: 2794.8188\n",
      "Epoch 1670/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2432.7866 - val_loss: 2795.5372\n",
      "Epoch 1671/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2433.1451 - val_loss: 2797.0672\n",
      "Epoch 1672/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2433.2948 - val_loss: 2795.8516\n",
      "Epoch 1673/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2433.8246 - val_loss: 2796.5098\n",
      "Epoch 1674/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2434.1119 - val_loss: 2794.6775\n",
      "Epoch 1675/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2433.3360 - val_loss: 2794.4141\n",
      "Epoch 1676/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2432.7386 - val_loss: 2794.8182\n",
      "Epoch 1677/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2433.0444 - val_loss: 2794.1913\n",
      "Epoch 1678/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2433.4620 - val_loss: 2795.3642\n",
      "Epoch 1679/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2432.6890 - val_loss: 2795.4570\n",
      "Epoch 1680/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2432.7385 - val_loss: 2796.7963\n",
      "Epoch 1681/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2433.1812 - val_loss: 2797.6719\n",
      "Epoch 1682/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2432.7307 - val_loss: 2796.5193\n",
      "Epoch 1683/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2432.8456 - val_loss: 2794.5671\n",
      "Epoch 1684/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2432.5345 - val_loss: 2794.7161\n",
      "Epoch 1685/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2432.9080 - val_loss: 2797.0782\n",
      "Epoch 1686/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2433.3423 - val_loss: 2794.1193\n",
      "Epoch 1687/2500\n",
      "892/892 [==============================] - 0s 174us/step - loss: 2432.9374 - val_loss: 2794.9213\n",
      "Epoch 1688/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2432.6706 - val_loss: 2795.0741\n",
      "Epoch 1689/2500\n",
      "892/892 [==============================] - 0s 146us/step - loss: 2432.8677 - val_loss: 2796.2668\n",
      "Epoch 1690/2500\n",
      "892/892 [==============================] - 0s 177us/step - loss: 2432.8549 - val_loss: 2795.7766\n",
      "Epoch 1691/2500\n",
      "892/892 [==============================] - 0s 197us/step - loss: 2433.0058 - val_loss: 2794.8600\n",
      "Epoch 1692/2500\n",
      "892/892 [==============================] - 0s 165us/step - loss: 2432.3917 - val_loss: 2796.3561\n",
      "Epoch 1693/2500\n",
      "892/892 [==============================] - 0s 236us/step - loss: 2432.7273 - val_loss: 2795.0268\n",
      "Epoch 1694/2500\n",
      "892/892 [==============================] - 0s 183us/step - loss: 2432.3990 - val_loss: 2796.0908\n",
      "Epoch 1695/2500\n",
      "892/892 [==============================] - 0s 155us/step - loss: 2432.6874 - val_loss: 2795.9905\n",
      "Epoch 1696/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2432.3093 - val_loss: 2794.9232\n",
      "Epoch 1697/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2432.5709 - val_loss: 2794.7299\n",
      "Epoch 1698/2500\n",
      "892/892 [==============================] - 0s 163us/step - loss: 2432.3370 - val_loss: 2795.0169\n",
      "Epoch 1699/2500\n",
      "892/892 [==============================] - 0s 148us/step - loss: 2432.8542 - val_loss: 2794.4533\n",
      "Epoch 1700/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2432.4546 - val_loss: 2796.4589\n",
      "Epoch 1701/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2432.4538 - val_loss: 2795.3403\n",
      "Epoch 1702/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2432.7844 - val_loss: 2797.4796\n",
      "Epoch 1703/2500\n",
      "892/892 [==============================] - 0s 137us/step - loss: 2432.4671 - val_loss: 2795.6922\n",
      "Epoch 1704/2500\n",
      "892/892 [==============================] - 0s 169us/step - loss: 2433.0362 - val_loss: 2794.1106\n",
      "Epoch 1705/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2433.2262 - val_loss: 2797.2909\n",
      "Epoch 1706/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2432.0455 - val_loss: 2796.0582\n",
      "Epoch 1707/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2432.9939 - val_loss: 2794.1375\n",
      "Epoch 1708/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2433.1144 - val_loss: 2794.9721\n",
      "Epoch 1709/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2432.7980 - val_loss: 2797.3181\n",
      "Epoch 1710/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2432.6519 - val_loss: 2795.3356\n",
      "Epoch 1711/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2432.8631 - val_loss: 2797.5416\n",
      "Epoch 1712/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2432.4865 - val_loss: 2794.0970\n",
      "Epoch 1713/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2434.8914 - val_loss: 2797.7023\n",
      "Epoch 1714/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2432.0199 - val_loss: 2795.4565\n",
      "Epoch 1715/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2432.7067 - val_loss: 2794.3201\n",
      "Epoch 1716/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 2433.4878 - val_loss: 2796.3906\n",
      "Epoch 1717/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2432.2172 - val_loss: 2795.0927\n",
      "Epoch 1718/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2434.5675 - val_loss: 2793.1324\n",
      "Epoch 1719/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2432.4027 - val_loss: 2795.7103\n",
      "Epoch 1720/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2433.8837 - val_loss: 2798.2515\n",
      "Epoch 1721/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2433.4283 - val_loss: 2793.9774\n",
      "Epoch 1722/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2433.0798 - val_loss: 2796.3049\n",
      "Epoch 1723/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2432.0115 - val_loss: 2795.3342\n",
      "Epoch 1724/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2432.1151 - val_loss: 2796.2796\n",
      "Epoch 1725/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2432.1991 - val_loss: 2796.2564\n",
      "Epoch 1726/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2432.0600 - val_loss: 2794.9967\n",
      "Epoch 1727/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2432.1912 - val_loss: 2795.8603\n",
      "Epoch 1728/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2433.6339 - val_loss: 2794.5087\n",
      "Epoch 1729/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2431.8009 - val_loss: 2796.2318\n",
      "Epoch 1730/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2431.6616 - val_loss: 2796.3670\n",
      "Epoch 1731/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2432.0817 - val_loss: 2796.7372\n",
      "Epoch 1732/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2431.9460 - val_loss: 2796.7565\n",
      "Epoch 1733/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2431.9045 - val_loss: 2796.9977\n",
      "Epoch 1734/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2431.8687 - val_loss: 2796.9506\n",
      "Epoch 1735/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2433.0903 - val_loss: 2793.7194\n",
      "Epoch 1736/2500\n",
      "892/892 [==============================] - 0s 125us/step - loss: 2431.5762 - val_loss: 2796.2750\n",
      "Epoch 1737/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2432.6008 - val_loss: 2797.4039\n",
      "Epoch 1738/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2431.5689 - val_loss: 2795.6494\n",
      "Epoch 1739/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2434.5315 - val_loss: 2793.1297\n",
      "Epoch 1740/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2432.5287 - val_loss: 2798.1321\n",
      "Epoch 1741/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2431.7435 - val_loss: 2796.1207\n",
      "Epoch 1742/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2431.8426 - val_loss: 2795.4846\n",
      "Epoch 1743/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2432.1588 - val_loss: 2797.3647\n",
      "Epoch 1744/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2431.5353 - val_loss: 2795.2737\n",
      "Epoch 1745/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2432.2778 - val_loss: 2796.1064\n",
      "Epoch 1746/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2432.3632 - val_loss: 2796.2447\n",
      "Epoch 1747/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2432.6573 - val_loss: 2793.9963\n",
      "Epoch 1748/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2432.8298 - val_loss: 2797.4932\n",
      "Epoch 1749/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2431.8854 - val_loss: 2796.3509\n",
      "Epoch 1750/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2432.5293 - val_loss: 2794.3505\n",
      "Epoch 1751/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2431.7526 - val_loss: 2796.7072\n",
      "Epoch 1752/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2431.8444 - val_loss: 2795.7342\n",
      "Epoch 1753/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2431.7771 - val_loss: 2795.8835\n",
      "Epoch 1754/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2432.3122 - val_loss: 2795.0531\n",
      "Epoch 1755/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2431.7694 - val_loss: 2796.6634\n",
      "Epoch 1756/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2432.1454 - val_loss: 2797.4113\n",
      "Epoch 1757/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2432.5399 - val_loss: 2795.4371\n",
      "Epoch 1758/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2431.5823 - val_loss: 2795.4292\n",
      "Epoch 1759/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2431.5006 - val_loss: 2796.5641\n",
      "Epoch 1760/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2432.2367 - val_loss: 2794.8145\n",
      "Epoch 1761/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2431.2448 - val_loss: 2796.0429\n",
      "Epoch 1762/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2431.6654 - val_loss: 2797.7276\n",
      "Epoch 1763/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2431.7511 - val_loss: 2796.9008\n",
      "Epoch 1764/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2432.4428 - val_loss: 2794.2325\n",
      "Epoch 1765/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2431.6543 - val_loss: 2796.4780\n",
      "Epoch 1766/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2431.5846 - val_loss: 2797.5186\n",
      "Epoch 1767/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2432.0493 - val_loss: 2795.1007\n",
      "Epoch 1768/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2431.7276 - val_loss: 2795.2477\n",
      "Epoch 1769/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2431.9849 - val_loss: 2795.4494\n",
      "Epoch 1770/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2431.5725 - val_loss: 2796.4786\n",
      "Epoch 1771/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2431.5397 - val_loss: 2797.4242\n",
      "Epoch 1772/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2431.9541 - val_loss: 2795.3570\n",
      "Epoch 1773/2500\n",
      "892/892 [==============================] - 0s 144us/step - loss: 2431.8135 - val_loss: 2797.9280\n",
      "Epoch 1774/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2431.2817 - val_loss: 2796.6911\n",
      "Epoch 1775/2500\n",
      "892/892 [==============================] - 0s 143us/step - loss: 2431.7763 - val_loss: 2796.0486\n",
      "Epoch 1776/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2431.1906 - val_loss: 2794.9907\n",
      "Epoch 1777/2500\n",
      "892/892 [==============================] - 0s 125us/step - loss: 2431.3372 - val_loss: 2796.1876\n",
      "Epoch 1778/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2432.4518 - val_loss: 2798.1365\n",
      "Epoch 1779/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2431.7451 - val_loss: 2793.9085\n",
      "Epoch 1780/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2431.3590 - val_loss: 2795.7723\n",
      "Epoch 1781/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2431.5741 - val_loss: 2795.0817\n",
      "Epoch 1782/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2431.2423 - val_loss: 2796.7210\n",
      "Epoch 1783/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2431.8076 - val_loss: 2796.6325\n",
      "Epoch 1784/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2431.7127 - val_loss: 2794.5000\n",
      "Epoch 1785/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2432.2747 - val_loss: 2796.3145\n",
      "Epoch 1786/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2431.3181 - val_loss: 2794.4098\n",
      "Epoch 1787/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2431.9506 - val_loss: 2797.7897\n",
      "Epoch 1788/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2432.7452 - val_loss: 2793.6407\n",
      "Epoch 1789/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2432.4052 - val_loss: 2798.7721\n",
      "Epoch 1790/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2431.8494 - val_loss: 2794.7977\n",
      "Epoch 1791/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2431.4350 - val_loss: 2796.9382\n",
      "Epoch 1792/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2431.1950 - val_loss: 2797.1323\n",
      "Epoch 1793/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2430.9492 - val_loss: 2795.3710\n",
      "Epoch 1794/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2430.9205 - val_loss: 2795.1748\n",
      "Epoch 1795/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2431.0041 - val_loss: 2795.8833\n",
      "Epoch 1796/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2432.3257 - val_loss: 2795.6160\n",
      "Epoch 1797/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2431.0346 - val_loss: 2796.5457\n",
      "Epoch 1798/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2431.7973 - val_loss: 2799.0663\n",
      "Epoch 1799/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2431.1474 - val_loss: 2796.7167\n",
      "Epoch 1800/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2431.1458 - val_loss: 2795.5602\n",
      "Epoch 1801/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2430.9962 - val_loss: 2794.4717\n",
      "Epoch 1802/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2431.3944 - val_loss: 2795.6147\n",
      "Epoch 1803/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 2431.1196 - val_loss: 2795.3078\n",
      "Epoch 1804/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2431.0275 - val_loss: 2796.5392\n",
      "Epoch 1805/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 2431.5799 - val_loss: 2796.7786\n",
      "Epoch 1806/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2431.3209 - val_loss: 2795.6298\n",
      "Epoch 1807/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2430.9284 - val_loss: 2796.1960\n",
      "Epoch 1808/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2430.8520 - val_loss: 2796.2245\n",
      "Epoch 1809/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2431.8142 - val_loss: 2794.4993\n",
      "Epoch 1810/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2430.8846 - val_loss: 2796.5200\n",
      "Epoch 1811/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2431.2120 - val_loss: 2796.2676\n",
      "Epoch 1812/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2431.1553 - val_loss: 2797.3826\n",
      "Epoch 1813/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2430.8468 - val_loss: 2796.1539\n",
      "Epoch 1814/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2430.9463 - val_loss: 2797.0942\n",
      "Epoch 1815/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2430.7672 - val_loss: 2795.0681\n",
      "Epoch 1816/2500\n",
      "892/892 [==============================] - 0s 53us/step - loss: 2432.3840 - val_loss: 2797.7962\n",
      "Epoch 1817/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 2431.1634 - val_loss: 2796.3123\n",
      "Epoch 1818/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2430.6358 - val_loss: 2795.6208\n",
      "Epoch 1819/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2430.6114 - val_loss: 2794.5508\n",
      "Epoch 1820/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2430.9357 - val_loss: 2796.3446\n",
      "Epoch 1821/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2431.0949 - val_loss: 2797.2312\n",
      "Epoch 1822/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2430.9425 - val_loss: 2796.9640\n",
      "Epoch 1823/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2430.7236 - val_loss: 2794.1837\n",
      "Epoch 1824/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2430.6996 - val_loss: 2793.4882\n",
      "Epoch 1825/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2431.4378 - val_loss: 2796.4454\n",
      "Epoch 1826/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2430.6706 - val_loss: 2795.8988\n",
      "Epoch 1827/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2431.4901 - val_loss: 2793.7395\n",
      "Epoch 1828/2500\n",
      "892/892 [==============================] - 0s 56us/step - loss: 2430.2540 - val_loss: 2795.0705\n",
      "Epoch 1829/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2430.6816 - val_loss: 2795.4761\n",
      "Epoch 1830/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 2431.0634 - val_loss: 2795.5007\n",
      "Epoch 1831/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2430.4986 - val_loss: 2796.0035\n",
      "Epoch 1832/2500\n",
      "892/892 [==============================] - 0s 55us/step - loss: 2431.1777 - val_loss: 2797.9256\n",
      "Epoch 1833/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2431.8221 - val_loss: 2793.7267\n",
      "Epoch 1834/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2430.4512 - val_loss: 2796.3409\n",
      "Epoch 1835/2500\n",
      "892/892 [==============================] - 0s 57us/step - loss: 2431.1961 - val_loss: 2795.6815\n",
      "Epoch 1836/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2430.6468 - val_loss: 2798.0147\n",
      "Epoch 1837/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2430.9518 - val_loss: 2796.2973\n",
      "Epoch 1838/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2430.5029 - val_loss: 2796.0919\n",
      "Epoch 1839/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2430.8549 - val_loss: 2795.7882\n",
      "Epoch 1840/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2430.4869 - val_loss: 2796.0052\n",
      "Epoch 1841/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2431.3123 - val_loss: 2796.9342\n",
      "Epoch 1842/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2430.8775 - val_loss: 2799.3661\n",
      "Epoch 1843/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2430.3985 - val_loss: 2797.2385\n",
      "Epoch 1844/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2430.5029 - val_loss: 2795.6867\n",
      "Epoch 1845/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2430.6334 - val_loss: 2797.2921\n",
      "Epoch 1846/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2430.6309 - val_loss: 2796.0656\n",
      "Epoch 1847/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2431.2704 - val_loss: 2796.8896\n",
      "Epoch 1848/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2430.6735 - val_loss: 2796.5821\n",
      "Epoch 1849/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2430.4721 - val_loss: 2795.3300\n",
      "Epoch 1850/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2431.1010 - val_loss: 2795.5285\n",
      "Epoch 1851/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2430.5958 - val_loss: 2794.6448\n",
      "Epoch 1852/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2431.9404 - val_loss: 2797.9048\n",
      "Epoch 1853/2500\n",
      "892/892 [==============================] - 0s 184us/step - loss: 2429.9498 - val_loss: 2796.3056\n",
      "Epoch 1854/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2430.2848 - val_loss: 2794.7848\n",
      "Epoch 1855/2500\n",
      "892/892 [==============================] - 0s 154us/step - loss: 2430.6158 - val_loss: 2795.2981\n",
      "Epoch 1856/2500\n",
      "892/892 [==============================] - 0s 154us/step - loss: 2430.0261 - val_loss: 2796.1094\n",
      "Epoch 1857/2500\n",
      "892/892 [==============================] - 0s 142us/step - loss: 2430.9634 - val_loss: 2797.3843\n",
      "Epoch 1858/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2431.8314 - val_loss: 2794.2976\n",
      "Epoch 1859/2500\n",
      "892/892 [==============================] - 0s 159us/step - loss: 2430.2162 - val_loss: 2796.4015\n",
      "Epoch 1860/2500\n",
      "892/892 [==============================] - 0s 149us/step - loss: 2430.6970 - val_loss: 2794.9674\n",
      "Epoch 1861/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2431.0756 - val_loss: 2795.9155\n",
      "Epoch 1862/2500\n",
      "892/892 [==============================] - 0s 163us/step - loss: 2430.5172 - val_loss: 2795.7094\n",
      "Epoch 1863/2500\n",
      "892/892 [==============================] - 0s 168us/step - loss: 2431.3774 - val_loss: 2799.1151\n",
      "Epoch 1864/2500\n",
      "892/892 [==============================] - 0s 178us/step - loss: 2429.9825 - val_loss: 2796.6967\n",
      "Epoch 1865/2500\n",
      "892/892 [==============================] - 0s 150us/step - loss: 2430.7717 - val_loss: 2794.7635\n",
      "Epoch 1866/2500\n",
      "892/892 [==============================] - 0s 159us/step - loss: 2430.5347 - val_loss: 2794.5769\n",
      "Epoch 1867/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2430.1043 - val_loss: 2797.0822\n",
      "Epoch 1868/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2430.4550 - val_loss: 2797.5539\n",
      "Epoch 1869/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2431.0951 - val_loss: 2793.7684\n",
      "Epoch 1870/2500\n",
      "892/892 [==============================] - 0s 149us/step - loss: 2431.3224 - val_loss: 2797.9828\n",
      "Epoch 1871/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2429.6925 - val_loss: 2795.8295\n",
      "Epoch 1872/2500\n",
      "892/892 [==============================] - 0s 164us/step - loss: 2430.1175 - val_loss: 2796.0554\n",
      "Epoch 1873/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2430.9340 - val_loss: 2796.8376\n",
      "Epoch 1874/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2429.7340 - val_loss: 2794.6301\n",
      "Epoch 1875/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2430.0967 - val_loss: 2794.4260\n",
      "Epoch 1876/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2431.0077 - val_loss: 2794.1774\n",
      "Epoch 1877/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2430.5012 - val_loss: 2795.3587\n",
      "Epoch 1878/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2431.1717 - val_loss: 2796.0878\n",
      "Epoch 1879/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2429.8083 - val_loss: 2796.3888\n",
      "Epoch 1880/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2430.0725 - val_loss: 2797.8673\n",
      "Epoch 1881/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2431.6108 - val_loss: 2795.1453\n",
      "Epoch 1882/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2431.6564 - val_loss: 2797.0112\n",
      "Epoch 1883/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2430.1393 - val_loss: 2795.1995\n",
      "Epoch 1884/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2430.1514 - val_loss: 2795.9621\n",
      "Epoch 1885/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2430.4083 - val_loss: 2795.0753\n",
      "Epoch 1886/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2432.4494 - val_loss: 2799.6048\n",
      "Epoch 1887/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2429.6215 - val_loss: 2797.4504\n",
      "Epoch 1888/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2429.4906 - val_loss: 2795.5585\n",
      "Epoch 1889/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2430.0985 - val_loss: 2796.7803\n",
      "Epoch 1890/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2429.8149 - val_loss: 2795.5751\n",
      "Epoch 1891/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2431.2295 - val_loss: 2798.5584\n",
      "Epoch 1892/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2430.0971 - val_loss: 2793.7822\n",
      "Epoch 1893/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2429.7463 - val_loss: 2794.3531\n",
      "Epoch 1894/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2429.4407 - val_loss: 2795.9723\n",
      "Epoch 1895/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2430.0536 - val_loss: 2795.6104\n",
      "Epoch 1896/2500\n",
      "892/892 [==============================] - 0s 138us/step - loss: 2429.6604 - val_loss: 2796.7074\n",
      "Epoch 1897/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2429.8409 - val_loss: 2795.2913\n",
      "Epoch 1898/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2430.0512 - val_loss: 2794.6091\n",
      "Epoch 1899/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2429.7359 - val_loss: 2796.5158\n",
      "Epoch 1900/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2429.7559 - val_loss: 2797.1681\n",
      "Epoch 1901/2500\n",
      "892/892 [==============================] - 0s 149us/step - loss: 2429.8880 - val_loss: 2797.5956\n",
      "Epoch 1902/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2429.8720 - val_loss: 2796.9750\n",
      "Epoch 1903/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2429.6356 - val_loss: 2794.0188\n",
      "Epoch 1904/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2430.5597 - val_loss: 2796.0628\n",
      "Epoch 1905/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2430.2930 - val_loss: 2797.7098\n",
      "Epoch 1906/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2429.8121 - val_loss: 2797.2408\n",
      "Epoch 1907/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2429.5010 - val_loss: 2794.4680\n",
      "Epoch 1908/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2429.8956 - val_loss: 2793.6759\n",
      "Epoch 1909/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2429.9098 - val_loss: 2794.1887\n",
      "Epoch 1910/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2429.9061 - val_loss: 2798.5319\n",
      "Epoch 1911/2500\n",
      "892/892 [==============================] - 0s 147us/step - loss: 2429.7963 - val_loss: 2797.6824\n",
      "Epoch 1912/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2429.2000 - val_loss: 2796.6465\n",
      "Epoch 1913/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2430.1940 - val_loss: 2794.6100\n",
      "Epoch 1914/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2429.4557 - val_loss: 2796.2284\n",
      "Epoch 1915/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2430.0759 - val_loss: 2796.2157\n",
      "Epoch 1916/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2429.6650 - val_loss: 2795.4062\n",
      "Epoch 1917/2500\n",
      "892/892 [==============================] - 0s 138us/step - loss: 2429.4958 - val_loss: 2795.3210\n",
      "Epoch 1918/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2430.0358 - val_loss: 2796.8536\n",
      "Epoch 1919/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2430.3247 - val_loss: 2794.6737\n",
      "Epoch 1920/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2429.6988 - val_loss: 2797.7944\n",
      "Epoch 1921/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2429.9399 - val_loss: 2795.9699\n",
      "Epoch 1922/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2429.5084 - val_loss: 2796.4658\n",
      "Epoch 1923/2500\n",
      "892/892 [==============================] - 0s 136us/step - loss: 2429.3029 - val_loss: 2796.2718\n",
      "Epoch 1924/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2429.3490 - val_loss: 2795.8195\n",
      "Epoch 1925/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2429.3664 - val_loss: 2795.9395\n",
      "Epoch 1926/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2429.2028 - val_loss: 2796.4581\n",
      "Epoch 1927/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2431.4535 - val_loss: 2794.9756\n",
      "Epoch 1928/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2429.6764 - val_loss: 2797.3768\n",
      "Epoch 1929/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2430.0669 - val_loss: 2796.2785\n",
      "Epoch 1930/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2429.5909 - val_loss: 2796.2568\n",
      "Epoch 1931/2500\n",
      "892/892 [==============================] - 0s 152us/step - loss: 2429.4331 - val_loss: 2794.7581\n",
      "Epoch 1932/2500\n",
      "892/892 [==============================] - 0s 160us/step - loss: 2429.1883 - val_loss: 2796.1467\n",
      "Epoch 1933/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2429.4388 - val_loss: 2796.5774\n",
      "Epoch 1934/2500\n",
      "892/892 [==============================] - 0s 176us/step - loss: 2429.5563 - val_loss: 2796.1398\n",
      "Epoch 1935/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2429.1951 - val_loss: 2795.4747\n",
      "Epoch 1936/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2429.5220 - val_loss: 2796.3665\n",
      "Epoch 1937/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2429.6002 - val_loss: 2796.4061\n",
      "Epoch 1938/2500\n",
      "892/892 [==============================] - 0s 175us/step - loss: 2429.7989 - val_loss: 2796.3114\n",
      "Epoch 1939/2500\n",
      "892/892 [==============================] - 0s 159us/step - loss: 2429.7474 - val_loss: 2793.6415\n",
      "Epoch 1940/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2429.9539 - val_loss: 2798.0532\n",
      "Epoch 1941/2500\n",
      "892/892 [==============================] - 0s 140us/step - loss: 2430.1667 - val_loss: 2796.8300\n",
      "Epoch 1942/2500\n",
      "892/892 [==============================] - 0s 126us/step - loss: 2429.2364 - val_loss: 2795.1479\n",
      "Epoch 1943/2500\n",
      "892/892 [==============================] - 0s 201us/step - loss: 2429.0939 - val_loss: 2795.5401\n",
      "Epoch 1944/2500\n",
      "892/892 [==============================] - 0s 175us/step - loss: 2429.4466 - val_loss: 2796.6922\n",
      "Epoch 1945/2500\n",
      "892/892 [==============================] - 0s 169us/step - loss: 2428.9373 - val_loss: 2795.3992\n",
      "Epoch 1946/2500\n",
      "892/892 [==============================] - 0s 213us/step - loss: 2428.8841 - val_loss: 2795.3003\n",
      "Epoch 1947/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2429.0313 - val_loss: 2796.5906\n",
      "Epoch 1948/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2429.1920 - val_loss: 2797.2026\n",
      "Epoch 1949/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2428.8459 - val_loss: 2796.8955\n",
      "Epoch 1950/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2430.4555 - val_loss: 2793.3834\n",
      "Epoch 1951/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2429.3773 - val_loss: 2796.3062\n",
      "Epoch 1952/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2428.9913 - val_loss: 2795.7652\n",
      "Epoch 1953/2500\n",
      "892/892 [==============================] - 0s 177us/step - loss: 2429.0657 - val_loss: 2796.9030\n",
      "Epoch 1954/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2428.8591 - val_loss: 2795.9781\n",
      "Epoch 1955/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2429.1263 - val_loss: 2796.0747\n",
      "Epoch 1956/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2429.2876 - val_loss: 2795.8487\n",
      "Epoch 1957/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2429.5391 - val_loss: 2797.5718\n",
      "Epoch 1958/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2430.0996 - val_loss: 2794.5453\n",
      "Epoch 1959/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2430.5273 - val_loss: 2798.4145\n",
      "Epoch 1960/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2429.8023 - val_loss: 2793.5304\n",
      "Epoch 1961/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2429.4588 - val_loss: 2797.7660\n",
      "Epoch 1962/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2429.4524 - val_loss: 2794.9292\n",
      "Epoch 1963/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2428.6275 - val_loss: 2795.6511\n",
      "Epoch 1964/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2429.6596 - val_loss: 2796.3847\n",
      "Epoch 1965/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2429.1403 - val_loss: 2796.3517\n",
      "Epoch 1966/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2429.3800 - val_loss: 2797.6587\n",
      "Epoch 1967/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2429.0879 - val_loss: 2796.2301\n",
      "Epoch 1968/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2429.8627 - val_loss: 2793.8565\n",
      "Epoch 1969/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2428.5848 - val_loss: 2796.0018\n",
      "Epoch 1970/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2429.4563 - val_loss: 2798.6746\n",
      "Epoch 1971/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2429.8314 - val_loss: 2797.5685\n",
      "Epoch 1972/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2429.0744 - val_loss: 2796.0044\n",
      "Epoch 1973/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2429.6291 - val_loss: 2794.4469\n",
      "Epoch 1974/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2428.7921 - val_loss: 2796.2013\n",
      "Epoch 1975/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2428.4724 - val_loss: 2795.6571\n",
      "Epoch 1976/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2428.8325 - val_loss: 2795.6609\n",
      "Epoch 1977/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2429.2682 - val_loss: 2795.1538\n",
      "Epoch 1978/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2429.0939 - val_loss: 2795.6583\n",
      "Epoch 1979/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2429.0090 - val_loss: 2797.5751\n",
      "Epoch 1980/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2428.4327 - val_loss: 2796.4774\n",
      "Epoch 1981/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2428.7589 - val_loss: 2796.2827\n",
      "Epoch 1982/2500\n",
      "892/892 [==============================] - 0s 185us/step - loss: 2428.5297 - val_loss: 2796.1651\n",
      "Epoch 1983/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2428.5109 - val_loss: 2796.4383\n",
      "Epoch 1984/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2429.3037 - val_loss: 2794.8854\n",
      "Epoch 1985/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2428.8107 - val_loss: 2796.6307\n",
      "Epoch 1986/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2428.9354 - val_loss: 2797.2829\n",
      "Epoch 1987/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2428.4846 - val_loss: 2796.4476\n",
      "Epoch 1988/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2428.8205 - val_loss: 2796.2878\n",
      "Epoch 1989/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2428.8093 - val_loss: 2796.2109\n",
      "Epoch 1990/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2428.6329 - val_loss: 2796.5341\n",
      "Epoch 1991/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2429.7127 - val_loss: 2794.1367\n",
      "Epoch 1992/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2428.1636 - val_loss: 2796.4943\n",
      "Epoch 1993/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2428.4464 - val_loss: 2796.9804\n",
      "Epoch 1994/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2429.6681 - val_loss: 2797.8562\n",
      "Epoch 1995/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2428.2789 - val_loss: 2795.2473\n",
      "Epoch 1996/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2428.5966 - val_loss: 2794.8698\n",
      "Epoch 1997/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2429.1163 - val_loss: 2796.5020\n",
      "Epoch 1998/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2428.6917 - val_loss: 2798.0102\n",
      "Epoch 1999/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2430.4781 - val_loss: 2793.9260\n",
      "Epoch 2000/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2428.2504 - val_loss: 2796.6269\n",
      "Epoch 2001/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2428.7693 - val_loss: 2797.1991\n",
      "Epoch 2002/2500\n",
      "892/892 [==============================] - 0s 138us/step - loss: 2428.8262 - val_loss: 2795.5238\n",
      "Epoch 2003/2500\n",
      "892/892 [==============================] - 0s 173us/step - loss: 2429.4362 - val_loss: 2797.1078\n",
      "Epoch 2004/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2428.6118 - val_loss: 2795.3067\n",
      "Epoch 2005/2500\n",
      "892/892 [==============================] - 0s 187us/step - loss: 2429.6312 - val_loss: 2796.5171\n",
      "Epoch 2006/2500\n",
      "892/892 [==============================] - 0s 143us/step - loss: 2428.1391 - val_loss: 2795.1641\n",
      "Epoch 2007/2500\n",
      "892/892 [==============================] - 0s 162us/step - loss: 2428.6919 - val_loss: 2797.2698\n",
      "Epoch 2008/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2428.1028 - val_loss: 2796.2995\n",
      "Epoch 2009/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2428.3822 - val_loss: 2795.2077\n",
      "Epoch 2010/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2428.0379 - val_loss: 2795.8963\n",
      "Epoch 2011/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2429.4617 - val_loss: 2798.3020\n",
      "Epoch 2012/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2428.0721 - val_loss: 2795.7131\n",
      "Epoch 2013/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2428.6130 - val_loss: 2796.5327\n",
      "Epoch 2014/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2428.0799 - val_loss: 2795.2035\n",
      "Epoch 2015/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2428.9078 - val_loss: 2796.4817\n",
      "Epoch 2016/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2428.2836 - val_loss: 2794.9522\n",
      "Epoch 2017/2500\n",
      "892/892 [==============================] - 0s 135us/step - loss: 2428.6826 - val_loss: 2795.1235\n",
      "Epoch 2018/2500\n",
      "892/892 [==============================] - 0s 141us/step - loss: 2428.0998 - val_loss: 2796.7165\n",
      "Epoch 2019/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2429.3189 - val_loss: 2798.2263\n",
      "Epoch 2020/2500\n",
      "892/892 [==============================] - 0s 165us/step - loss: 2428.3083 - val_loss: 2796.0571\n",
      "Epoch 2021/2500\n",
      "892/892 [==============================] - 0s 166us/step - loss: 2428.8313 - val_loss: 2796.6411\n",
      "Epoch 2022/2500\n",
      "892/892 [==============================] - 0s 150us/step - loss: 2428.4324 - val_loss: 2795.5528\n",
      "Epoch 2023/2500\n",
      "892/892 [==============================] - 0s 182us/step - loss: 2428.2667 - val_loss: 2795.7386\n",
      "Epoch 2024/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2429.5203 - val_loss: 2797.2348\n",
      "Epoch 2025/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2428.9182 - val_loss: 2794.4435\n",
      "Epoch 2026/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2427.8974 - val_loss: 2796.6032\n",
      "Epoch 2027/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2429.1216 - val_loss: 2795.0520\n",
      "Epoch 2028/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2427.8534 - val_loss: 2796.3759\n",
      "Epoch 2029/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2428.3058 - val_loss: 2796.4527\n",
      "Epoch 2030/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2427.9498 - val_loss: 2796.1418\n",
      "Epoch 2031/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2429.0042 - val_loss: 2798.6349\n",
      "Epoch 2032/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2428.5241 - val_loss: 2794.4092\n",
      "Epoch 2033/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2427.9815 - val_loss: 2794.8585\n",
      "Epoch 2034/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2428.0890 - val_loss: 2797.2826\n",
      "Epoch 2035/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2428.1359 - val_loss: 2797.7854\n",
      "Epoch 2036/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2429.3459 - val_loss: 2797.9084\n",
      "Epoch 2037/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2428.1864 - val_loss: 2795.9167\n",
      "Epoch 2038/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2427.7994 - val_loss: 2794.8976\n",
      "Epoch 2039/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2428.3832 - val_loss: 2794.3757\n",
      "Epoch 2040/2500\n",
      "892/892 [==============================] - 0s 224us/step - loss: 2429.5940 - val_loss: 2795.0639\n",
      "Epoch 2041/2500\n",
      "892/892 [==============================] - 0s 173us/step - loss: 2428.2216 - val_loss: 2795.5447\n",
      "Epoch 2042/2500\n",
      "892/892 [==============================] - 0s 202us/step - loss: 2427.6424 - val_loss: 2796.4694\n",
      "Epoch 2043/2500\n",
      "892/892 [==============================] - 0s 233us/step - loss: 2429.4899 - val_loss: 2799.1997\n",
      "Epoch 2044/2500\n",
      "892/892 [==============================] - 0s 165us/step - loss: 2430.1309 - val_loss: 2794.3875\n",
      "Epoch 2045/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2428.6807 - val_loss: 2796.0250\n",
      "Epoch 2046/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2427.7456 - val_loss: 2796.8266\n",
      "Epoch 2047/2500\n",
      "892/892 [==============================] - 0s 130us/step - loss: 2428.7360 - val_loss: 2797.6594\n",
      "Epoch 2048/2500\n",
      "892/892 [==============================] - 0s 146us/step - loss: 2428.0000 - val_loss: 2795.6642\n",
      "Epoch 2049/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2428.1586 - val_loss: 2795.6989\n",
      "Epoch 2050/2500\n",
      "892/892 [==============================] - 0s 155us/step - loss: 2427.5247 - val_loss: 2795.5684\n",
      "Epoch 2051/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2427.5611 - val_loss: 2796.6769\n",
      "Epoch 2052/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2427.7319 - val_loss: 2795.7562\n",
      "Epoch 2053/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2428.0480 - val_loss: 2795.7832\n",
      "Epoch 2054/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2429.1083 - val_loss: 2798.0743\n",
      "Epoch 2055/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2428.5622 - val_loss: 2795.1116\n",
      "Epoch 2056/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2427.5274 - val_loss: 2796.2688\n",
      "Epoch 2057/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2427.8184 - val_loss: 2797.7471\n",
      "Epoch 2058/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2427.4741 - val_loss: 2796.9169\n",
      "Epoch 2059/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2428.3361 - val_loss: 2797.9377\n",
      "Epoch 2060/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2427.5917 - val_loss: 2796.7927\n",
      "Epoch 2061/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2427.9654 - val_loss: 2795.1268\n",
      "Epoch 2062/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2427.8552 - val_loss: 2794.7477\n",
      "Epoch 2063/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2427.8127 - val_loss: 2795.3885\n",
      "Epoch 2064/2500\n",
      "892/892 [==============================] - 0s 118us/step - loss: 2428.0246 - val_loss: 2796.6332\n",
      "Epoch 2065/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2427.5949 - val_loss: 2796.8896\n",
      "Epoch 2066/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2427.4325 - val_loss: 2795.5489\n",
      "Epoch 2067/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2427.3968 - val_loss: 2796.3273\n",
      "Epoch 2068/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2428.4096 - val_loss: 2795.8614\n",
      "Epoch 2069/2500\n",
      "892/892 [==============================] - 0s 151us/step - loss: 2427.4103 - val_loss: 2796.1199\n",
      "Epoch 2070/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2427.6053 - val_loss: 2796.4751\n",
      "Epoch 2071/2500\n",
      "892/892 [==============================] - 0s 165us/step - loss: 2427.4423 - val_loss: 2796.7147\n",
      "Epoch 2072/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2427.5770 - val_loss: 2796.9550\n",
      "Epoch 2073/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2427.9933 - val_loss: 2795.1177\n",
      "Epoch 2074/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2427.3191 - val_loss: 2795.5709\n",
      "Epoch 2075/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2428.0457 - val_loss: 2797.4043\n",
      "Epoch 2076/2500\n",
      "892/892 [==============================] - 0s 135us/step - loss: 2428.7882 - val_loss: 2794.2449\n",
      "Epoch 2077/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2428.1564 - val_loss: 2795.5059\n",
      "Epoch 2078/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2427.6565 - val_loss: 2796.3530\n",
      "Epoch 2079/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2427.8937 - val_loss: 2798.8143\n",
      "Epoch 2080/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2427.2578 - val_loss: 2796.3292\n",
      "Epoch 2081/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2428.1690 - val_loss: 2795.8366\n",
      "Epoch 2082/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2427.4059 - val_loss: 2795.6243\n",
      "Epoch 2083/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2428.0885 - val_loss: 2793.9137\n",
      "Epoch 2084/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2428.0642 - val_loss: 2798.6161\n",
      "Epoch 2085/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2427.1248 - val_loss: 2796.4559\n",
      "Epoch 2086/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2428.0994 - val_loss: 2797.9965\n",
      "Epoch 2087/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2426.9337 - val_loss: 2795.4384\n",
      "Epoch 2088/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2427.3223 - val_loss: 2795.2172\n",
      "Epoch 2089/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2427.5629 - val_loss: 2794.6429\n",
      "Epoch 2090/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2427.3847 - val_loss: 2796.3638\n",
      "Epoch 2091/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2428.6856 - val_loss: 2797.8389\n",
      "Epoch 2092/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2427.9861 - val_loss: 2794.4987\n",
      "Epoch 2093/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2429.2729 - val_loss: 2798.4038\n",
      "Epoch 2094/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2427.1152 - val_loss: 2796.2889\n",
      "Epoch 2095/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2427.3134 - val_loss: 2796.4296\n",
      "Epoch 2096/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2427.8608 - val_loss: 2797.4590\n",
      "Epoch 2097/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2427.2998 - val_loss: 2795.8144\n",
      "Epoch 2098/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2427.3781 - val_loss: 2793.9869\n",
      "Epoch 2099/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2427.5418 - val_loss: 2796.3303\n",
      "Epoch 2100/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2427.5097 - val_loss: 2797.7151\n",
      "Epoch 2101/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2426.9041 - val_loss: 2795.4097\n",
      "Epoch 2102/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2427.1422 - val_loss: 2794.9646\n",
      "Epoch 2103/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2427.3803 - val_loss: 2795.8922\n",
      "Epoch 2104/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2427.1347 - val_loss: 2796.2575\n",
      "Epoch 2105/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2427.0363 - val_loss: 2795.6547\n",
      "Epoch 2106/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2427.1092 - val_loss: 2797.2963\n",
      "Epoch 2107/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2427.1501 - val_loss: 2796.4860\n",
      "Epoch 2108/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2427.3849 - val_loss: 2795.1852\n",
      "Epoch 2109/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2427.7317 - val_loss: 2796.1324\n",
      "Epoch 2110/2500\n",
      "892/892 [==============================] - 0s 114us/step - loss: 2427.4839 - val_loss: 2798.2623\n",
      "Epoch 2111/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2426.7873 - val_loss: 2796.2514\n",
      "Epoch 2112/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2426.9445 - val_loss: 2796.7331\n",
      "Epoch 2113/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2428.3386 - val_loss: 2795.9909\n",
      "Epoch 2114/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2427.7527 - val_loss: 2794.1755\n",
      "Epoch 2115/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2426.8200 - val_loss: 2794.6551\n",
      "Epoch 2116/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2427.3055 - val_loss: 2797.6976\n",
      "Epoch 2117/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2426.8695 - val_loss: 2797.5224\n",
      "Epoch 2118/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2427.2295 - val_loss: 2797.1547\n",
      "Epoch 2119/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2426.7087 - val_loss: 2796.8120\n",
      "Epoch 2120/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2428.4897 - val_loss: 2794.3857\n",
      "Epoch 2121/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2426.9706 - val_loss: 2797.1926\n",
      "Epoch 2122/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2427.9489 - val_loss: 2795.4195\n",
      "Epoch 2123/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2426.7804 - val_loss: 2797.3661\n",
      "Epoch 2124/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2427.0096 - val_loss: 2797.0124\n",
      "Epoch 2125/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2427.0726 - val_loss: 2797.5130\n",
      "Epoch 2126/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2427.4115 - val_loss: 2795.9087\n",
      "Epoch 2127/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2426.8193 - val_loss: 2796.1872\n",
      "Epoch 2128/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2426.9832 - val_loss: 2795.5197\n",
      "Epoch 2129/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2427.0545 - val_loss: 2797.7439\n",
      "Epoch 2130/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2426.8661 - val_loss: 2796.9413\n",
      "Epoch 2131/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2427.7853 - val_loss: 2797.1103\n",
      "Epoch 2132/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2427.3459 - val_loss: 2795.0936\n",
      "Epoch 2133/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2427.0246 - val_loss: 2797.1941\n",
      "Epoch 2134/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2426.9302 - val_loss: 2796.0856\n",
      "Epoch 2135/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2427.4180 - val_loss: 2795.3154\n",
      "Epoch 2136/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2427.0873 - val_loss: 2796.8380\n",
      "Epoch 2137/2500\n",
      "892/892 [==============================] - 0s 147us/step - loss: 2426.8836 - val_loss: 2796.4805\n",
      "Epoch 2138/2500\n",
      "892/892 [==============================] - 0s 126us/step - loss: 2427.0047 - val_loss: 2797.0109\n",
      "Epoch 2139/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2427.4406 - val_loss: 2797.5605\n",
      "Epoch 2140/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2428.9489 - val_loss: 2792.9839\n",
      "Epoch 2141/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2427.2378 - val_loss: 2796.4002\n",
      "Epoch 2142/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2427.2725 - val_loss: 2797.9817\n",
      "Epoch 2143/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2426.8405 - val_loss: 2795.2057\n",
      "Epoch 2144/2500\n",
      "892/892 [==============================] - 0s 121us/step - loss: 2427.2960 - val_loss: 2797.0888\n",
      "Epoch 2145/2500\n",
      "892/892 [==============================] - 0s 115us/step - loss: 2426.6506 - val_loss: 2795.4181\n",
      "Epoch 2146/2500\n",
      "892/892 [==============================] - 0s 125us/step - loss: 2428.0283 - val_loss: 2798.0612\n",
      "Epoch 2147/2500\n",
      "892/892 [==============================] - 0s 167us/step - loss: 2428.6324 - val_loss: 2793.4199\n",
      "Epoch 2148/2500\n",
      "892/892 [==============================] - 0s 174us/step - loss: 2426.5331 - val_loss: 2796.3057\n",
      "Epoch 2149/2500\n",
      "892/892 [==============================] - 0s 156us/step - loss: 2426.4153 - val_loss: 2796.4980\n",
      "Epoch 2150/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2426.8911 - val_loss: 2796.8570\n",
      "Epoch 2151/2500\n",
      "892/892 [==============================] - 0s 189us/step - loss: 2426.7588 - val_loss: 2795.9649\n",
      "Epoch 2152/2500\n",
      "892/892 [==============================] - 0s 171us/step - loss: 2428.0636 - val_loss: 2798.3863\n",
      "Epoch 2153/2500\n",
      "892/892 [==============================] - 0s 134us/step - loss: 2426.8556 - val_loss: 2795.8625\n",
      "Epoch 2154/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2426.6642 - val_loss: 2795.6935\n",
      "Epoch 2155/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2426.6957 - val_loss: 2795.1941\n",
      "Epoch 2156/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2426.9635 - val_loss: 2796.7603\n",
      "Epoch 2157/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2427.9153 - val_loss: 2797.1738\n",
      "Epoch 2158/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2426.4983 - val_loss: 2795.5467\n",
      "Epoch 2159/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2426.5402 - val_loss: 2794.4807\n",
      "Epoch 2160/2500\n",
      "892/892 [==============================] - 0s 124us/step - loss: 2426.5676 - val_loss: 2795.8496\n",
      "Epoch 2161/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2426.6984 - val_loss: 2797.0717\n",
      "Epoch 2162/2500\n",
      "892/892 [==============================] - 0s 126us/step - loss: 2426.8148 - val_loss: 2795.5021\n",
      "Epoch 2163/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2426.4258 - val_loss: 2795.6477\n",
      "Epoch 2164/2500\n",
      "892/892 [==============================] - 0s 122us/step - loss: 2427.0310 - val_loss: 2799.3201\n",
      "Epoch 2165/2500\n",
      "892/892 [==============================] - 0s 132us/step - loss: 2426.3751 - val_loss: 2797.4408\n",
      "Epoch 2166/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2426.4535 - val_loss: 2796.4145\n",
      "Epoch 2167/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2427.2473 - val_loss: 2794.5387\n",
      "Epoch 2168/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2426.8884 - val_loss: 2797.6143\n",
      "Epoch 2169/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2426.4789 - val_loss: 2797.6298\n",
      "Epoch 2170/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2426.6760 - val_loss: 2795.4580\n",
      "Epoch 2171/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2427.3681 - val_loss: 2797.5441\n",
      "Epoch 2172/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2428.1464 - val_loss: 2794.3885\n",
      "Epoch 2173/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2426.8075 - val_loss: 2797.4306\n",
      "Epoch 2174/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2426.3298 - val_loss: 2797.4053\n",
      "Epoch 2175/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2426.4170 - val_loss: 2796.7134\n",
      "Epoch 2176/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2426.1358 - val_loss: 2795.3401\n",
      "Epoch 2177/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2426.9836 - val_loss: 2798.0140\n",
      "Epoch 2178/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2427.8708 - val_loss: 2798.2289\n",
      "Epoch 2179/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2426.0978 - val_loss: 2796.1180\n",
      "Epoch 2180/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2427.0142 - val_loss: 2797.8034\n",
      "Epoch 2181/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2425.8325 - val_loss: 2796.4538\n",
      "Epoch 2182/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2426.5856 - val_loss: 2794.9810\n",
      "Epoch 2183/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2427.0534 - val_loss: 2796.2059\n",
      "Epoch 2184/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2426.1146 - val_loss: 2795.8910\n",
      "Epoch 2185/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2426.5438 - val_loss: 2796.5343\n",
      "Epoch 2186/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2426.2699 - val_loss: 2794.5192\n",
      "Epoch 2187/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2427.1066 - val_loss: 2794.7015\n",
      "Epoch 2188/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2426.0723 - val_loss: 2795.8565\n",
      "Epoch 2189/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2425.9969 - val_loss: 2798.4722\n",
      "Epoch 2190/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2426.3476 - val_loss: 2796.9531\n",
      "Epoch 2191/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2425.9816 - val_loss: 2797.2298\n",
      "Epoch 2192/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2426.2794 - val_loss: 2797.8887\n",
      "Epoch 2193/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2426.7147 - val_loss: 2796.4410\n",
      "Epoch 2194/2500\n",
      "892/892 [==============================] - 0s 111us/step - loss: 2427.1963 - val_loss: 2794.5076\n",
      "Epoch 2195/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2427.0136 - val_loss: 2797.6143\n",
      "Epoch 2196/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2426.1598 - val_loss: 2797.1342\n",
      "Epoch 2197/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2426.1308 - val_loss: 2796.3030\n",
      "Epoch 2198/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2426.1019 - val_loss: 2797.3987\n",
      "Epoch 2199/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2425.9689 - val_loss: 2796.9561\n",
      "Epoch 2200/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2427.3354 - val_loss: 2793.5912\n",
      "Epoch 2201/2500\n",
      "892/892 [==============================] - 0s 128us/step - loss: 2425.9240 - val_loss: 2796.8176\n",
      "Epoch 2202/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2426.7011 - val_loss: 2797.8310\n",
      "Epoch 2203/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2426.2001 - val_loss: 2796.4136\n",
      "Epoch 2204/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2426.6118 - val_loss: 2796.1113\n",
      "Epoch 2205/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2426.0026 - val_loss: 2797.1019\n",
      "Epoch 2206/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2426.5853 - val_loss: 2796.4223\n",
      "Epoch 2207/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2426.2385 - val_loss: 2797.1604\n",
      "Epoch 2208/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2426.9153 - val_loss: 2795.9364\n",
      "Epoch 2209/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2425.8959 - val_loss: 2796.3227\n",
      "Epoch 2210/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2426.1178 - val_loss: 2797.3303\n",
      "Epoch 2211/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2425.9934 - val_loss: 2795.2953\n",
      "Epoch 2212/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2426.0288 - val_loss: 2795.3284\n",
      "Epoch 2213/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2426.0934 - val_loss: 2797.8067\n",
      "Epoch 2214/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2425.7252 - val_loss: 2796.6685\n",
      "Epoch 2215/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2428.3853 - val_loss: 2797.6071\n",
      "Epoch 2216/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2425.9618 - val_loss: 2795.4958\n",
      "Epoch 2217/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2425.7874 - val_loss: 2795.1979\n",
      "Epoch 2218/2500\n",
      "892/892 [==============================] - 0s 144us/step - loss: 2425.8513 - val_loss: 2796.0457\n",
      "Epoch 2219/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2426.3718 - val_loss: 2794.4385\n",
      "Epoch 2220/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2428.4627 - val_loss: 2798.3493\n",
      "Epoch 2221/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2427.1148 - val_loss: 2796.9686\n",
      "Epoch 2222/2500\n",
      "892/892 [==============================] - 0s 125us/step - loss: 2425.6401 - val_loss: 2796.7540\n",
      "Epoch 2223/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2426.1909 - val_loss: 2794.7550\n",
      "Epoch 2224/2500\n",
      "892/892 [==============================] - 0s 169us/step - loss: 2426.6225 - val_loss: 2798.1709\n",
      "Epoch 2225/2500\n",
      "892/892 [==============================] - 0s 135us/step - loss: 2425.9717 - val_loss: 2796.5245\n",
      "Epoch 2226/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2426.2150 - val_loss: 2795.4565\n",
      "Epoch 2227/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2425.9652 - val_loss: 2798.1311\n",
      "Epoch 2228/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2427.3017 - val_loss: 2796.4044\n",
      "Epoch 2229/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2426.6567 - val_loss: 2796.6244\n",
      "Epoch 2230/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2425.7052 - val_loss: 2797.1133\n",
      "Epoch 2231/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2425.4964 - val_loss: 2795.9991\n",
      "Epoch 2232/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2426.0716 - val_loss: 2795.4877\n",
      "Epoch 2233/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2425.6508 - val_loss: 2796.8531\n",
      "Epoch 2234/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2426.3767 - val_loss: 2797.1629\n",
      "Epoch 2235/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2426.2126 - val_loss: 2797.7288\n",
      "Epoch 2236/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2426.1046 - val_loss: 2797.8628\n",
      "Epoch 2237/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2426.0084 - val_loss: 2794.7803\n",
      "Epoch 2238/2500\n",
      "892/892 [==============================] - 0s 117us/step - loss: 2425.8644 - val_loss: 2796.7556\n",
      "Epoch 2239/2500\n",
      "892/892 [==============================] - 0s 123us/step - loss: 2425.6750 - val_loss: 2795.7311\n",
      "Epoch 2240/2500\n",
      "892/892 [==============================] - 0s 193us/step - loss: 2425.7708 - val_loss: 2796.7162\n",
      "Epoch 2241/2500\n",
      "892/892 [==============================] - 0s 163us/step - loss: 2425.4534 - val_loss: 2796.7968\n",
      "Epoch 2242/2500\n",
      "892/892 [==============================] - 0s 132us/step - loss: 2425.9935 - val_loss: 2795.4175\n",
      "Epoch 2243/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2425.3455 - val_loss: 2797.6892\n",
      "Epoch 2244/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2425.5611 - val_loss: 2796.5321\n",
      "Epoch 2245/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2426.1799 - val_loss: 2798.9761\n",
      "Epoch 2246/2500\n",
      "892/892 [==============================] - 0s 145us/step - loss: 2425.3343 - val_loss: 2797.4238\n",
      "Epoch 2247/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2426.3009 - val_loss: 2796.3802\n",
      "Epoch 2248/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2425.9681 - val_loss: 2797.1110\n",
      "Epoch 2249/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2425.2687 - val_loss: 2796.7487\n",
      "Epoch 2250/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2425.6274 - val_loss: 2795.9078\n",
      "Epoch 2251/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2425.8134 - val_loss: 2795.4208\n",
      "Epoch 2252/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2426.4852 - val_loss: 2799.8315\n",
      "Epoch 2253/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2426.1786 - val_loss: 2798.0911\n",
      "Epoch 2254/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2425.6055 - val_loss: 2796.5954\n",
      "Epoch 2255/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2425.5515 - val_loss: 2795.7203\n",
      "Epoch 2256/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2425.6516 - val_loss: 2795.9243\n",
      "Epoch 2257/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2425.5365 - val_loss: 2797.0686\n",
      "Epoch 2258/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2425.1041 - val_loss: 2796.7560\n",
      "Epoch 2259/2500\n",
      "892/892 [==============================] - 0s 107us/step - loss: 2425.2122 - val_loss: 2796.8325\n",
      "Epoch 2260/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2425.9284 - val_loss: 2795.3818\n",
      "Epoch 2261/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2425.4730 - val_loss: 2796.9813\n",
      "Epoch 2262/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2425.3830 - val_loss: 2797.4781\n",
      "Epoch 2263/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2426.1400 - val_loss: 2796.0461\n",
      "Epoch 2264/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2425.5546 - val_loss: 2795.8330\n",
      "Epoch 2265/2500\n",
      "892/892 [==============================] - 0s 133us/step - loss: 2425.1654 - val_loss: 2798.9266\n",
      "Epoch 2266/2500\n",
      "892/892 [==============================] - 0s 116us/step - loss: 2426.0386 - val_loss: 2799.1795\n",
      "Epoch 2267/2500\n",
      "892/892 [==============================] - 0s 110us/step - loss: 2426.2015 - val_loss: 2795.5975\n",
      "Epoch 2268/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2426.3849 - val_loss: 2796.5765\n",
      "Epoch 2269/2500\n",
      "892/892 [==============================] - 0s 119us/step - loss: 2425.3477 - val_loss: 2796.2344\n",
      "Epoch 2270/2500\n",
      "892/892 [==============================] - 0s 127us/step - loss: 2425.2299 - val_loss: 2795.8063\n",
      "Epoch 2271/2500\n",
      "892/892 [==============================] - 0s 166us/step - loss: 2425.0944 - val_loss: 2797.5493\n",
      "Epoch 2272/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2425.5517 - val_loss: 2797.5218\n",
      "Epoch 2273/2500\n",
      "892/892 [==============================] - 0s 175us/step - loss: 2425.6905 - val_loss: 2798.4762\n",
      "Epoch 2274/2500\n",
      "892/892 [==============================] - 0s 131us/step - loss: 2426.0672 - val_loss: 2797.4858\n",
      "Epoch 2275/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2425.9681 - val_loss: 2794.1519\n",
      "Epoch 2276/2500\n",
      "892/892 [==============================] - 0s 106us/step - loss: 2425.2073 - val_loss: 2796.5985\n",
      "Epoch 2277/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2426.4552 - val_loss: 2794.4287\n",
      "Epoch 2278/2500\n",
      "892/892 [==============================] - 0s 113us/step - loss: 2425.3370 - val_loss: 2796.8150\n",
      "Epoch 2279/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2425.1027 - val_loss: 2797.4703\n",
      "Epoch 2280/2500\n",
      "892/892 [==============================] - 0s 96us/step - loss: 2425.3504 - val_loss: 2796.0327\n",
      "Epoch 2281/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2425.3676 - val_loss: 2795.8051\n",
      "Epoch 2282/2500\n",
      "892/892 [==============================] - 0s 129us/step - loss: 2425.7787 - val_loss: 2795.7110\n",
      "Epoch 2283/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2425.4748 - val_loss: 2797.4874\n",
      "Epoch 2284/2500\n",
      "892/892 [==============================] - 0s 164us/step - loss: 2426.2792 - val_loss: 2798.0393\n",
      "Epoch 2285/2500\n",
      "892/892 [==============================] - 0s 126us/step - loss: 2424.9899 - val_loss: 2796.7783\n",
      "Epoch 2286/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2425.1472 - val_loss: 2796.3120\n",
      "Epoch 2287/2500\n",
      "892/892 [==============================] - 0s 95us/step - loss: 2425.8330 - val_loss: 2794.9141\n",
      "Epoch 2288/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2425.3764 - val_loss: 2796.2461\n",
      "Epoch 2289/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2425.2244 - val_loss: 2797.5075\n",
      "Epoch 2290/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2425.0574 - val_loss: 2795.9048\n",
      "Epoch 2291/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2425.1718 - val_loss: 2795.5278\n",
      "Epoch 2292/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2425.7296 - val_loss: 2798.4249\n",
      "Epoch 2293/2500\n",
      "892/892 [==============================] - 0s 108us/step - loss: 2424.6939 - val_loss: 2795.9799\n",
      "Epoch 2294/2500\n",
      "892/892 [==============================] - 0s 135us/step - loss: 2425.1322 - val_loss: 2797.2341\n",
      "Epoch 2295/2500\n",
      "892/892 [==============================] - 0s 154us/step - loss: 2425.4758 - val_loss: 2796.0875\n",
      "Epoch 2296/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2425.2260 - val_loss: 2795.0301\n",
      "Epoch 2297/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2424.6957 - val_loss: 2797.1152\n",
      "Epoch 2298/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2425.9572 - val_loss: 2795.6002\n",
      "Epoch 2299/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2426.2711 - val_loss: 2798.3542\n",
      "Epoch 2300/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2425.2727 - val_loss: 2797.6560\n",
      "Epoch 2301/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2424.8639 - val_loss: 2795.9643\n",
      "Epoch 2302/2500\n",
      "892/892 [==============================] - 0s 102us/step - loss: 2425.5482 - val_loss: 2796.5687\n",
      "Epoch 2303/2500\n",
      "892/892 [==============================] - 0s 93us/step - loss: 2425.9559 - val_loss: 2795.4183\n",
      "Epoch 2304/2500\n",
      "892/892 [==============================] - 0s 97us/step - loss: 2425.0850 - val_loss: 2797.6829\n",
      "Epoch 2305/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2425.0356 - val_loss: 2798.3880\n",
      "Epoch 2306/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2424.8604 - val_loss: 2797.2063\n",
      "Epoch 2307/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2425.3923 - val_loss: 2797.4828\n",
      "Epoch 2308/2500\n",
      "892/892 [==============================] - 0s 120us/step - loss: 2425.1496 - val_loss: 2797.6583\n",
      "Epoch 2309/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2424.8422 - val_loss: 2795.3954\n",
      "Epoch 2310/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2425.0286 - val_loss: 2796.0762\n",
      "Epoch 2311/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2425.5094 - val_loss: 2798.7032\n",
      "Epoch 2312/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2424.4379 - val_loss: 2796.3647\n",
      "Epoch 2313/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2425.1723 - val_loss: 2794.8107\n",
      "Epoch 2314/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2425.6531 - val_loss: 2798.4041\n",
      "Epoch 2315/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2424.7233 - val_loss: 2796.1221\n",
      "Epoch 2316/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2424.9284 - val_loss: 2796.8559\n",
      "Epoch 2317/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2425.6842 - val_loss: 2795.2118\n",
      "Epoch 2318/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2424.6069 - val_loss: 2796.4357\n",
      "Epoch 2319/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2425.6757 - val_loss: 2798.9261\n",
      "Epoch 2320/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2424.8273 - val_loss: 2797.8093\n",
      "Epoch 2321/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2425.4943 - val_loss: 2798.3033\n",
      "Epoch 2322/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2425.2485 - val_loss: 2796.5033\n",
      "Epoch 2323/2500\n",
      "892/892 [==============================] - 0s 69us/step - loss: 2424.5154 - val_loss: 2795.8588\n",
      "Epoch 2324/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2425.0611 - val_loss: 2795.8077\n",
      "Epoch 2325/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2425.0386 - val_loss: 2797.8208\n",
      "Epoch 2326/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2425.3280 - val_loss: 2796.7228\n",
      "Epoch 2327/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2424.6754 - val_loss: 2795.9444\n",
      "Epoch 2328/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2424.5822 - val_loss: 2795.9309\n",
      "Epoch 2329/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2425.3248 - val_loss: 2798.6184\n",
      "Epoch 2330/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2425.9256 - val_loss: 2798.9981\n",
      "Epoch 2331/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2424.5215 - val_loss: 2796.0392\n",
      "Epoch 2332/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2424.4533 - val_loss: 2796.0486\n",
      "Epoch 2333/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2424.4492 - val_loss: 2796.8079\n",
      "Epoch 2334/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2424.5243 - val_loss: 2796.9969\n",
      "Epoch 2335/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2424.5796 - val_loss: 2797.0894\n",
      "Epoch 2336/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2425.2177 - val_loss: 2795.5903\n",
      "Epoch 2337/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2424.8151 - val_loss: 2798.2156\n",
      "Epoch 2338/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2424.6585 - val_loss: 2796.0488\n",
      "Epoch 2339/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2424.7842 - val_loss: 2796.0393\n",
      "Epoch 2340/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2425.0980 - val_loss: 2796.1288\n",
      "Epoch 2341/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2425.4903 - val_loss: 2795.1625\n",
      "Epoch 2342/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2424.4677 - val_loss: 2798.7711\n",
      "Epoch 2343/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2425.3698 - val_loss: 2798.7271\n",
      "Epoch 2344/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2424.2222 - val_loss: 2797.4290\n",
      "Epoch 2345/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2424.7946 - val_loss: 2795.7328\n",
      "Epoch 2346/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2425.1189 - val_loss: 2798.1930\n",
      "Epoch 2347/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2424.4631 - val_loss: 2797.6495\n",
      "Epoch 2348/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2424.9547 - val_loss: 2797.0426\n",
      "Epoch 2349/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2424.9477 - val_loss: 2794.7437\n",
      "Epoch 2350/2500\n",
      "892/892 [==============================] - 0s 100us/step - loss: 2424.2911 - val_loss: 2796.4667\n",
      "Epoch 2351/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2424.4600 - val_loss: 2797.1609\n",
      "Epoch 2352/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2425.3045 - val_loss: 2796.5921\n",
      "Epoch 2353/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2424.3556 - val_loss: 2796.9958\n",
      "Epoch 2354/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2424.3449 - val_loss: 2798.2198\n",
      "Epoch 2355/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2424.3156 - val_loss: 2797.9270\n",
      "Epoch 2356/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2424.6831 - val_loss: 2797.5803\n",
      "Epoch 2357/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2424.3957 - val_loss: 2797.0570\n",
      "Epoch 2358/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2424.4036 - val_loss: 2795.6072\n",
      "Epoch 2359/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2424.4589 - val_loss: 2797.7821\n",
      "Epoch 2360/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2424.6768 - val_loss: 2796.3624\n",
      "Epoch 2361/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2424.4668 - val_loss: 2796.4339\n",
      "Epoch 2362/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2424.1092 - val_loss: 2797.1553\n",
      "Epoch 2363/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2424.8291 - val_loss: 2799.1086\n",
      "Epoch 2364/2500\n",
      "892/892 [==============================] - 0s 105us/step - loss: 2424.1170 - val_loss: 2797.9555\n",
      "Epoch 2365/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2424.3359 - val_loss: 2796.8867\n",
      "Epoch 2366/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2424.2921 - val_loss: 2795.4031\n",
      "Epoch 2367/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2424.3582 - val_loss: 2797.9319\n",
      "Epoch 2368/2500\n",
      "892/892 [==============================] - 0s 48us/step - loss: 2424.4631 - val_loss: 2796.8314\n",
      "Epoch 2369/2500\n",
      "892/892 [==============================] - 0s 74us/step - loss: 2424.1458 - val_loss: 2796.9124\n",
      "Epoch 2370/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2424.5983 - val_loss: 2798.7850\n",
      "Epoch 2371/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2424.4105 - val_loss: 2796.2558\n",
      "Epoch 2372/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2424.3964 - val_loss: 2797.8730\n",
      "Epoch 2373/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2424.9763 - val_loss: 2799.4208\n",
      "Epoch 2374/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2424.9219 - val_loss: 2796.3274\n",
      "Epoch 2375/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2424.1373 - val_loss: 2795.9895\n",
      "Epoch 2376/2500\n",
      "892/892 [==============================] - 0s 98us/step - loss: 2424.3004 - val_loss: 2795.8986\n",
      "Epoch 2377/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2424.9614 - val_loss: 2798.4533\n",
      "Epoch 2378/2500\n",
      "892/892 [==============================] - 0s 73us/step - loss: 2424.2003 - val_loss: 2798.0187\n",
      "Epoch 2379/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2424.1046 - val_loss: 2796.7610\n",
      "Epoch 2380/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2424.7845 - val_loss: 2796.0864\n",
      "Epoch 2381/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2424.1214 - val_loss: 2797.1577\n",
      "Epoch 2382/2500\n",
      "892/892 [==============================] - 0s 101us/step - loss: 2424.0195 - val_loss: 2796.0711\n",
      "Epoch 2383/2500\n",
      "892/892 [==============================] - 0s 112us/step - loss: 2424.2508 - val_loss: 2797.9908\n",
      "Epoch 2384/2500\n",
      "892/892 [==============================] - 0s 188us/step - loss: 2423.9882 - val_loss: 2795.6495\n",
      "Epoch 2385/2500\n",
      "892/892 [==============================] - 0s 185us/step - loss: 2424.5732 - val_loss: 2798.5519\n",
      "Epoch 2386/2500\n",
      "892/892 [==============================] - 0s 109us/step - loss: 2423.8446 - val_loss: 2797.2564\n",
      "Epoch 2387/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2424.6127 - val_loss: 2797.5245\n",
      "Epoch 2388/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2424.1769 - val_loss: 2798.0847\n",
      "Epoch 2389/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2424.4567 - val_loss: 2795.5487\n",
      "Epoch 2390/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2423.8628 - val_loss: 2796.8775\n",
      "Epoch 2391/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2424.3371 - val_loss: 2796.2763\n",
      "Epoch 2392/2500\n",
      "892/892 [==============================] - 0s 92us/step - loss: 2423.9248 - val_loss: 2797.8101\n",
      "Epoch 2393/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2424.1031 - val_loss: 2798.7154\n",
      "Epoch 2394/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2424.6350 - val_loss: 2796.4876\n",
      "Epoch 2395/2500\n",
      "892/892 [==============================] - 0s 149us/step - loss: 2424.6595 - val_loss: 2795.9239\n",
      "Epoch 2396/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2424.5616 - val_loss: 2799.0893\n",
      "Epoch 2397/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2424.4187 - val_loss: 2795.6214\n",
      "Epoch 2398/2500\n",
      "892/892 [==============================] - 0s 84us/step - loss: 2424.7786 - val_loss: 2795.2184\n",
      "Epoch 2399/2500\n",
      "892/892 [==============================] - 0s 90us/step - loss: 2424.4709 - val_loss: 2798.7940\n",
      "Epoch 2400/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2423.7517 - val_loss: 2796.9400\n",
      "Epoch 2401/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2424.1089 - val_loss: 2797.7307\n",
      "Epoch 2402/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2423.9348 - val_loss: 2795.8914\n",
      "Epoch 2403/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2424.2109 - val_loss: 2796.4540\n",
      "Epoch 2404/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2424.2296 - val_loss: 2797.4334\n",
      "Epoch 2405/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2424.6247 - val_loss: 2796.6334\n",
      "Epoch 2406/2500\n",
      "892/892 [==============================] - 0s 88us/step - loss: 2424.8300 - val_loss: 2796.0762\n",
      "Epoch 2407/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2425.4674 - val_loss: 2798.1258\n",
      "Epoch 2408/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2423.8475 - val_loss: 2796.3863\n",
      "Epoch 2409/2500\n",
      "892/892 [==============================] - 0s 72us/step - loss: 2424.1867 - val_loss: 2796.2521\n",
      "Epoch 2410/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2424.6193 - val_loss: 2798.5805\n",
      "Epoch 2411/2500\n",
      "892/892 [==============================] - 0s 99us/step - loss: 2424.0562 - val_loss: 2795.2911\n",
      "Epoch 2412/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2423.9524 - val_loss: 2796.6515\n",
      "Epoch 2413/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2424.4541 - val_loss: 2798.7274\n",
      "Epoch 2414/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2424.6129 - val_loss: 2795.0050\n",
      "Epoch 2415/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2423.9466 - val_loss: 2797.1711\n",
      "Epoch 2416/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2423.6938 - val_loss: 2796.6546\n",
      "Epoch 2417/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2424.3848 - val_loss: 2798.5753\n",
      "Epoch 2418/2500\n",
      "892/892 [==============================] - 0s 71us/step - loss: 2423.6519 - val_loss: 2796.5794\n",
      "Epoch 2419/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2424.0058 - val_loss: 2796.1100\n",
      "Epoch 2420/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2423.5742 - val_loss: 2795.9151\n",
      "Epoch 2421/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2424.1936 - val_loss: 2798.4341\n",
      "Epoch 2422/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2423.8516 - val_loss: 2797.6931\n",
      "Epoch 2423/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2423.6169 - val_loss: 2797.0644\n",
      "Epoch 2424/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2424.3406 - val_loss: 2795.2312\n",
      "Epoch 2425/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2424.4597 - val_loss: 2797.2271\n",
      "Epoch 2426/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2423.5924 - val_loss: 2798.6934\n",
      "Epoch 2427/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2423.4419 - val_loss: 2798.2249\n",
      "Epoch 2428/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2423.8036 - val_loss: 2796.1473\n",
      "Epoch 2429/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2425.4689 - val_loss: 2799.7809\n",
      "Epoch 2430/2500\n",
      "892/892 [==============================] - 0s 50us/step - loss: 2424.3157 - val_loss: 2794.5760\n",
      "Epoch 2431/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 2424.3852 - val_loss: 2796.9781\n",
      "Epoch 2432/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2423.5461 - val_loss: 2797.0711\n",
      "Epoch 2433/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2423.8693 - val_loss: 2796.2522\n",
      "Epoch 2434/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2424.5027 - val_loss: 2795.3025\n",
      "Epoch 2435/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2424.2267 - val_loss: 2799.1059\n",
      "Epoch 2436/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2424.4333 - val_loss: 2796.7658\n",
      "Epoch 2437/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2424.0819 - val_loss: 2796.0824\n",
      "Epoch 2438/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2423.4205 - val_loss: 2798.3050\n",
      "Epoch 2439/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2423.6269 - val_loss: 2797.7469\n",
      "Epoch 2440/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2423.4596 - val_loss: 2797.9466\n",
      "Epoch 2441/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2423.8346 - val_loss: 2798.5933\n",
      "Epoch 2442/2500\n",
      "892/892 [==============================] - 0s 70us/step - loss: 2424.8268 - val_loss: 2796.5412\n",
      "Epoch 2443/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2424.1413 - val_loss: 2795.7503\n",
      "Epoch 2444/2500\n",
      "892/892 [==============================] - 0s 65us/step - loss: 2422.9844 - val_loss: 2797.6370\n",
      "Epoch 2445/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2424.4827 - val_loss: 2798.8609\n",
      "Epoch 2446/2500\n",
      "892/892 [==============================] - 0s 91us/step - loss: 2423.2758 - val_loss: 2798.2542\n",
      "Epoch 2447/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2423.6680 - val_loss: 2797.5718\n",
      "Epoch 2448/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2424.5275 - val_loss: 2798.4174\n",
      "Epoch 2449/2500\n",
      "892/892 [==============================] - 0s 79us/step - loss: 2423.4274 - val_loss: 2795.1377\n",
      "Epoch 2450/2500\n",
      "892/892 [==============================] - 0s 68us/step - loss: 2423.4615 - val_loss: 2795.8795\n",
      "Epoch 2451/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2423.5145 - val_loss: 2798.3126\n",
      "Epoch 2452/2500\n",
      "892/892 [==============================] - 0s 76us/step - loss: 2424.3747 - val_loss: 2798.2463\n",
      "Epoch 2453/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2423.5901 - val_loss: 2798.7381\n",
      "Epoch 2454/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2423.7897 - val_loss: 2798.3882\n",
      "Epoch 2455/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2423.6435 - val_loss: 2798.7831\n",
      "Epoch 2456/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2423.7945 - val_loss: 2794.9557\n",
      "Epoch 2457/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2423.6255 - val_loss: 2797.3358\n",
      "Epoch 2458/2500\n",
      "892/892 [==============================] - 0s 82us/step - loss: 2423.5406 - val_loss: 2797.7896\n",
      "Epoch 2459/2500\n",
      "892/892 [==============================] - 0s 75us/step - loss: 2423.3658 - val_loss: 2796.2571\n",
      "Epoch 2460/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2423.1259 - val_loss: 2797.4499\n",
      "Epoch 2461/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2423.4966 - val_loss: 2797.4186\n",
      "Epoch 2462/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2423.6658 - val_loss: 2797.3604\n",
      "Epoch 2463/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2423.2293 - val_loss: 2797.1942\n",
      "Epoch 2464/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 2423.3344 - val_loss: 2796.7811\n",
      "Epoch 2465/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2423.1229 - val_loss: 2797.1522\n",
      "Epoch 2466/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 2423.6370 - val_loss: 2797.0875\n",
      "Epoch 2467/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2423.1684 - val_loss: 2797.0956\n",
      "Epoch 2468/2500\n",
      "892/892 [==============================] - 0s 60us/step - loss: 2423.0172 - val_loss: 2797.1637\n",
      "Epoch 2469/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2423.7253 - val_loss: 2796.0985\n",
      "Epoch 2470/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2423.6163 - val_loss: 2798.2111\n",
      "Epoch 2471/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2423.6562 - val_loss: 2799.4089\n",
      "Epoch 2472/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2422.8649 - val_loss: 2796.4135\n",
      "Epoch 2473/2500\n",
      "892/892 [==============================] - 0s 63us/step - loss: 2423.3761 - val_loss: 2796.7865\n",
      "Epoch 2474/2500\n",
      "892/892 [==============================] - 0s 78us/step - loss: 2423.4660 - val_loss: 2797.1226\n",
      "Epoch 2475/2500\n",
      "892/892 [==============================] - 0s 62us/step - loss: 2423.2124 - val_loss: 2796.0879\n",
      "Epoch 2476/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2423.3134 - val_loss: 2796.4600\n",
      "Epoch 2477/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2423.7088 - val_loss: 2798.1743\n",
      "Epoch 2478/2500\n",
      "892/892 [==============================] - 0s 80us/step - loss: 2423.2423 - val_loss: 2797.2315\n",
      "Epoch 2479/2500\n",
      "892/892 [==============================] - 0s 89us/step - loss: 2423.6379 - val_loss: 2799.0862\n",
      "Epoch 2480/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2424.5241 - val_loss: 2795.9372\n",
      "Epoch 2481/2500\n",
      "892/892 [==============================] - 0s 83us/step - loss: 2422.8466 - val_loss: 2798.3376\n",
      "Epoch 2482/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2423.3890 - val_loss: 2798.4979\n",
      "Epoch 2483/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2423.3162 - val_loss: 2796.6055\n",
      "Epoch 2484/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2423.4656 - val_loss: 2796.5714\n",
      "Epoch 2485/2500\n",
      "892/892 [==============================] - 0s 103us/step - loss: 2423.2784 - val_loss: 2797.3745\n",
      "Epoch 2486/2500\n",
      "892/892 [==============================] - 0s 86us/step - loss: 2423.9310 - val_loss: 2797.2359\n",
      "Epoch 2487/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2423.1919 - val_loss: 2798.9768\n",
      "Epoch 2488/2500\n",
      "892/892 [==============================] - 0s 81us/step - loss: 2423.6706 - val_loss: 2796.2441\n",
      "Epoch 2489/2500\n",
      "892/892 [==============================] - 0s 87us/step - loss: 2423.5921 - val_loss: 2795.7057\n",
      "Epoch 2490/2500\n",
      "892/892 [==============================] - 0s 77us/step - loss: 2423.2187 - val_loss: 2797.7900\n",
      "Epoch 2491/2500\n",
      "892/892 [==============================] - 0s 94us/step - loss: 2423.0944 - val_loss: 2797.3633\n",
      "Epoch 2492/2500\n",
      "892/892 [==============================] - 0s 104us/step - loss: 2423.0190 - val_loss: 2797.0723\n",
      "Epoch 2493/2500\n",
      "892/892 [==============================] - 0s 85us/step - loss: 2422.7902 - val_loss: 2797.7181\n",
      "Epoch 2494/2500\n",
      "892/892 [==============================] - 0s 58us/step - loss: 2422.7797 - val_loss: 2797.0291\n",
      "Epoch 2495/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2423.9321 - val_loss: 2797.0731\n",
      "Epoch 2496/2500\n",
      "892/892 [==============================] - 0s 61us/step - loss: 2423.7099 - val_loss: 2799.9200\n",
      "Epoch 2497/2500\n",
      "892/892 [==============================] - 0s 66us/step - loss: 2422.7635 - val_loss: 2797.7567\n",
      "Epoch 2498/2500\n",
      "892/892 [==============================] - 0s 59us/step - loss: 2422.7448 - val_loss: 2797.1382\n",
      "Epoch 2499/2500\n",
      "892/892 [==============================] - 0s 67us/step - loss: 2423.2777 - val_loss: 2798.1938\n",
      "Epoch 2500/2500\n",
      "892/892 [==============================] - 0s 64us/step - loss: 2423.0056 - val_loss: 2797.7599\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train, y_train, validation_data = (X_test, y_test),epochs=2500, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('model1_dnn_normal.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEFCAYAAAAL/efAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFP4iIg6k8BcRcaD/D8WjAnipNNIeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27054dbbe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"loss\", \"val_loss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEFCAYAAADt1CyEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27054ea17f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.legend([\"loss\", \"val_loss\"])\n",
    "plt.title(\"Loss\")\n",
    "plt.ylim([2000, 3500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL9JREFUeJzt2rsJAlEQQFEVsQIFE63BAsRcbEBN\nDAQT+4/XCgTxgh84J93H7C5cJnrjYRiGEbxp8u0P4L8JiERAJAIiERCJgEgERCIgEgGRTD/xkvnh\nlmdcd9s8437Z5xnr5eKlc7/yz5vVLM84nY9Pn9lAJAIiERCJgEgERCIgEgGRCIhk7EorhQ1EIiAS\nAZEIiERAJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiJ5AGD2Dk3xanHn\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc2bbe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL9JREFUeJzt2r0JwlAUgFEj7mDnINYO4A6COwji\nDAEdU6zs4wSC+IE/cE6bx00CH7d6wzRN0wzeNP/2B/DfBEQiIBIBkQiIREAkAiIREImASBafeMn1\nds8zVptjnrHfLvOM8XR46dyv/PNunUfMzpfx6TMbiERAJAIiERCJgEgERCIgEgGRDK60UthAJAIi\nERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiCSB3JkFE0374NK\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bd4b6400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAALxJREFUeJzt2ssJwlAQQFEjrkRbECSkDYuwCHFj\nEanCcmMFgnjBD5yzzWOSwGVWb1iWZVnBm9bf/gD+m4BIBEQiIBIBkQiIREAkAiIREMnmEy85HMc8\n43S+5Bn3+ZZn7Hfbl879yj/P13OeMU7T02c2EImASAREIiASAZEIiERAJAIiGVxppbCBSAREIiAS\nAZEIiERAJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkDzmmDk0UzotFAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bbde06a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL9JREFUeJzt2r0JwlAUgFEVF7GzsLKwcQs7OxcR\ncZBs4mYR4gSC+IE/cE6bx00CH7d682maphm8afHtD+C/CYhEQCQCIhEQiYBIBEQiIBIBkSw/8ZL7\nOOYZm+06z9jvVnnGMNxeOvcr/3w8nPKMy/X89JkNRCIgEgGRCIhEQCQCIhEQiYBI5q60UthAJAIi\nERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiCSB1vQFE0A7VGq\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bbedbe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAALtJREFUeJzt2jkKwlAUQFEjLsDGwqFzn25K3Yjo\nYiRFXIEgXnCAc9p8XhK4vOoP0zRNM3jT/NsfwH8TEImASAREIiASAZEIiERAJAIiWXziJYfdPs84\nrZZ5xvF8yjO2m/VL537ln2/XS55xH8enz2wgEgGRCIhEQCQCIhEQiYBIBEQyuNJKYQORCIhEQCQC\nIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiERAJAIiERCJgEgeFZoTTdX8cIUAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218ba83c550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL5JREFUeJzt2rkJAlEUQFFHbMdUxGbMNbIIqzEQ\nM+txqWOsQBAvuMA56XzezMDlRX8Yx3GcwJum3/4A/puASAREIiASAZEIiERAJAIiERDJ7BMvuZ/X\necYw3+cZi+Uqz7hdLy+d+5V/3m52ecbxdHj6zAYiERCJgEgERCIgEgGRCIhEQCSDK60UNhCJgEgE\nRCIgEgGRCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiOQBGQUUTTdqfR8A\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc33e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAMBJREFUeJzt2r0JwlAUgFEV97JyBdv0Ng5g4wSW\nggNYphHE8eIEgviBP3BOm8dNAh+3evNpmqYZvGnx7Q/gvwmIREAkAiIREImASAREIiASAZEsP/GS\n8XLMM1brTZ6xP+zyjPNpfOncr/zzMGzzjNv9+vSZDUQiIBIBkQiIREAkAiIREImASOautFLYQCQC\nIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiERAJAIiERCJgEgERCIgkgdBZRRNAkh8\nkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc1db128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAALhJREFUeJzt2ssJwkAUQFEV2xBxa9qKXYhWJWh1\nYwWCeMUPnLOd8GYCl1mELMcYYwEvWn37APw3AZEIiERAJAIiERCJgEgERCIgkvUnNnnHx+7tZpdn\nXG+XPGM/TU899yvvPB/mPON0Pj5ccwORCIhEQCQCIhEQiYBIBEQiIJKlX1op3EAkAiIREImASARE\nIiASAZEIiERAJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIJI7tiAUTROh/9QAAAAASUVO\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bbd873c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL9JREFUeJzt2rsJAlEQQFEVOzMzETQVE+3JRLAI\nDTS2BAtaKxDEC37gnHQfs7twmeiNh2EYRvCmybc/gP8mIBIBkQiIREAkAiIREImASAREMv3ES/aH\nY56xWszzjNlyl2fcb6eXzv3KP2/W2zzjcj0/fWYDkQiIREAkAiIREImASAREIiCSsSutFDYQiYBI\nBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiERAJAIiERCJgEgERCIgEgGRCIjkAR1FFE3gLBQn\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bbf6ab38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL9JREFUeJzt2j0OAVEUgFHEAvQUQqeQKDRo1XZi\nA5ZhwWMFEvElfpJz2nm5M5N8udUbD8MwjOBNk29/AP9NQCQCIhEQiYBIBEQiIBIBkQiIZPqJl6y3\nuzzjfjvnGZv9Nc9YrhYvnfuVf57NL3nG4XR8+swGIhEQiYBIBEQiIBIBkQiIREAkY1daKWwgEgGR\nCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiERAJAIiERDJA2tSDk0JKfAZ\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc33e080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAMBJREFUeJzt2ssJwlAQQFEjVqIWYAEWZA0W4Na9\nNQg2YAHRpmIFgnjBD5yzzWOSwGVWb5imaZrBm+bf/gD+m4BIBEQiIBIBkQiIREAkAiIREMniEy/Z\n7Y95xvl0yDPG6yXPWG+2L537lX++38Y8Y7laPX1mA5EIiERAJAIiERCJgEgERCIgksGVVgobiERA\nJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiIREImASARE8gAYnBRNR0jF\npgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc4ac8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAALtJREFUeJzt2jsKwlAQQFEj9oKW4kaiVlq6EAtx\nJ645rkAQL/iBc9o8JglcpnrDNE3TDN40//YH8N8ERCIgEgGRCIhEQCQCIhEQiYBIFp94yfm0yzMu\nh22ecbje84zlevPSuV/559XxlmeM+/HpMxuIREAkAiIREImASAREIiASAZEMrrRS2EAkAiIREImA\nSAREIiASAZEIiERAJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIJIHdOIOTWKwEskAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc3ca2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAMFJREFUeJzt2ssJwlAQQFEjViCW4EqwEKuwE8uw\nEgsQ3IgVKBYTKxDEC37gnG0ekwQus3rDOI7jBN40/fYH8N8ERCIgEgGRCIhEQCQCIhEQiYBIZp94\nyWa7yzMuh32ecTyd84z1avnSuV/55/vtmmfMF4unz2wgEgGRCIhEQCQCIhEQiYBIBEQyuNJKYQOR\nCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiERAJAIiERCJgEgeAPgUTX8x\nFdsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc2cbdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAALtJREFUeJzt2ssJwlAQQFEVO1FsyJQi2owIVqII\nLlJVrEAQL/iBc7Z5TBK4zOrNp2maZvCmxbc/gP8mIBIBkQiIREAkAiIREImASAREsvzES1brTZ4x\njvc843q55RnDsH3p3K/88+l4zjP2h93TZzYQiYBIBEQiIBIBkQiIREAkAiKZu9JKYQORCIhEQCQC\nIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiERAJAIiERCJgEge74YUTTwz7ZMAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bbba9438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL9JREFUeJzt2r0JwlAUgFEVd3cCa0EEWx1ABDsb\nG+ts4AoSJxDED/yBc9o8bhL4uNWbjuM4TuBNs29/AP9NQCQCIhEQiYBIBEQiIBIBkQiIZP6JlwyX\nQ55x22/zjMXpmmfsjueXzv3KPy+He56x3qyePrOBSAREIiASAZEIiERAJAIiERDJ1JVWChuIREAk\nAiIREImASAREIiASAZEIiERAJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBETyAPHQGk35gW1T\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc499ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAALpJREFUeJzt2jkKwlAUQFFj3KNg74CtpSsTtLey\ndDtxBYJ4wQHOaQMvyefyqj9M0zTN4E3zb38A/01AJAIiERCJgEgERCIgEgGRCIhk8YmX3G+nPGN/\nOOYZl/M1zxjH147sV/55tdzmGZvd+ukzG4hEQCQCIhEQiYBIBEQiIBIBkQyutFLYQCQCIhEQiYBI\nBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiERAJAIiERCJgEgERCIgkgfDaBRNPGgyqwAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc30e2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL5JREFUeJzt2ssJwlAQQFEj9iEWYXciNiBpSWxA\nV+5EsJFYgSBe8APnbPOYJHCZ1RumaZpm8Kb5tz+A/yYgEgGRCIhEQCQCIhEQiYBIBESy+MRLLudD\nnrEfxzzjcDzlGdfb/aVzv/LPy9U6z9juNk+f2UAkAiIREImASAREIiASAZEIiGRwpZXCBiIREImA\nSAREIiASAZEIiERAJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIBIBkTwA2V8XTZEdZ1wA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc427080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAL5JREFUeJzt2rsJAlEQQFEV+zAS6xNNFcsw08RA\nsAYDLWytQBAv+IFz0n3M7sJlojcehmEYwZsm3/4A/puASAREIiASAZEIiERAJAIiERDJ9BMvmc0X\necbtes8zDsd9nrHbbF869yv/fDmf8ozVevn0mQ1EIiASAZEIiERAJAIiERCJgEjGrrRS2EAkAiIR\nEImASAREIiASAZEIiERAJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIJIHwyoUTe+hSCUA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc307eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAMBJREFUeJzt2j0OAVEUgFHEvhQqC1AqdAoKQmIN\nNqjXWsRYgUR8iZ/knHZe7swkX271xsMwDCN40+TbH8B/ExCJgEgERCIgEgGRCIhEQCQCIpl+4iW3\n6z3PmC3mecZyc8ozLufVS+d+5Z9323WecTjunz6zgUgERCIgEgGRCIhEQCQCIhEQydiVVgobiERA\nJAIiERCJgEgERCIgEgGRCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiIREImASARE8gBfzBRNLKw3\ntwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc38a1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAAqCAYAAABcF9KrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAMFJREFUeJzt2rsJAlEQQFEVmzC2Fa3AIhQ7EQzF\nyArswVzLsYK1AkG84AfOSfcxuwuXid54GIZhBG+afPsD+G8CIhEQiYBIBEQiIBIBkQiIREAk00+8\n5Ho55xn7wynP2M3uecb8eHvp3K/882K5yjM22/XTZzYQiYBIBEQiIBIBkQiIREAkAiIZu9JKYQOR\nCIhEQCQCIhEQiYBIBEQiIBIBkQiIREAkAiIREImASAREIiASAZEIiERAJAIiERCJgEgel1kVTbbJ\nd4YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218ba695e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in range(20):\n",
    "    input_color = np.hstack([color1[idx], color2[idx]]).reshape(1,6) /256\n",
    "    color_3_pre = model1.predict(input_color)[0]\n",
    "\n",
    "    true = [color1[idx], color2[idx], color3[idx]]\n",
    "    predict = [color1[idx], color2[idx], color_3_pre]\n",
    "    \n",
    "    bar_true = plot_colors([0.33, 0.33, 0.33], true, h=50, w=300)\n",
    "    bar_predict = plot_colors([0.33, 0.33, 0.33], predict, h=50, w=300)\n",
    "    \n",
    "    plt.figure(figsize = (2, 1))  \n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(bar_true)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(bar_predict)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColorMind 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://colormind.io/api/\"\n",
    "data = {\n",
    "    'model' : \"default\",\n",
    "    'input' : [[44, 43, 44], [90, 83, 82], \"N\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAAvCAYAAAAVbkUjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAXJJREFUeJzt3T1KA1EYhtEbfyq1C0IqEbG0tJM0\nViGl2AiSQrCxSGenlm7Gzo25g+sGRvLCwCfGczbwho8pnkwzk957bwAAbLTz2z8AAOCvEE4AACHh\nBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AAKG9ipHp8rFipj3Mr0p2WmvtabUo2TmZ\nHbexX8XZxvtPV/OSnefZqfsPeF1+lewcXqxH3X8bb/9yWfN/9+j63rM/4O3ms2Tn4PzD/Qe83+6X\n7Oye3f14f2+cAABCwgkAICScAABCwgkAICScAABCwgkAICScAABCwgkAICScAABCwgkAICScAABC\nwgkAIDTpY78iCADwT3jjBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4A\nACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AAKFv\n5FwuVysmlSoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1968e780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAAvCAYAAAAVbkUjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAXlJREFUeJzt3TEuBHEYxuH/WoXEAXTbOoDCBXQa\nV1BwCJHo9BQqByDRinYrCoVoxQEUotra3wVGvMkkn1jPc4F38mWKX6aZSe+9NwAAfrTy2w8AAPBX\nCCcAgJBwAgAICScAgJBwAgAICScAgJBwAgAICScAgJBwAgAICScAgNBqxcj7x6Jips12jkt2Wmvt\ncG+jZOfs5KiN/SvOMt7/YO22ZOf84dX9B0w310t2Fleno+6/jLffb3clOxdPL979Aduf9yU78+dH\n9x9ws3VdsrN7+fbt/X1xAgAICScAgJBwAgAICScAgJBwAgAICScAgJBwAgAICScAgJBwAgAICScA\ngJBwAgAICScAgNCkj/2LIADAP+GLEwBASDgBAISEEwBASDgBAISEEwBASDgBAISEEwBASDgBAISE\nEwBASDgBAISEEwBASDgBAISEEwBASDgBAISEEwBASDgBAISEEwBASDgBAISEEwBASDgBAISEEwBA\nSDgBAIS+ANbVQ1cgFdIbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1b5ac630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAAvCAYAAAAVbkUjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAWJJREFUeJzt3TEuhEEYgOF/RaIQFBqVBM2eQqPU\nKRxhO63eFRR0OidwBidwoN8BbOJNNhms57nAN/kyxZtpZjHP8zwBAPCtnZ8+AADAXyGcAAAi4QQA\nEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAAKLdEUNOzy5GjJkub1ZD5kzTND0/3A2Zc3iw\nP236K8427v/l/nbInL2Tc/tf4/H6Y8ic46vXjfa/jbt3978auf+n1fuQOUfLN/tf4zfcfy9OAACR\ncAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAACixbzpL4IAAP+E\nFycAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCI\nhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBA9AmTFS5X0Aoo/QAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1b0023c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAAvCAYAAAAVbkUjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAW5JREFUeJzt3TFKA2EUhdEXcQuxF4RYWFnYZC0h\nbVrb4FqyA8ENiK32WUOKtEkX4XcDihcGfsl4zgbecJniY5qZtNZaAQDwq4u/fgAAgHMhnAAAQsIJ\nACAknAAAQsIJACAknAAAQsIJACAknAAAQsIJACAknAAAQpc9jnyeTj3O1N39bZc7VVXzh+sudzab\n1xr6V5wx7v9U0y53FtsP+39jdXPscufxZT9o/zFuv5z12X79PGz7qnHufy7vftU49z9Mr7rc2b29\n/7i/L04AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQEk4AACHhBAAQ\nmrShfxEEAPgnfHECAAgJJwCAkHACAAgJJwCAkHACAAgJJwCAkHACAAgJJwCAkHACAAgJJwCAkHAC\nAAgJJwCAkHACAAgJJwCAkHACAAgJJwCAkHACAAgJJwCAkHACAAgJJwCAkHACAAgJJwCA0Bcze0JX\n0XjLSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef1b0ee2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAAvCAYAAAAVbkUjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAAXZJREFUeJzt3b1JQ2EYhuEv4gA2gpjgD04gOII7\npLBzABsrGydwBMEBhGAw4izWgljYWn8ucMQHDrxivK4FnpyXFDenOZPee28AAPxo47d/AADAXyGc\nAABCwgkAICScAABCwgkAICScAABCwgkAICScAABCwgkAICScAABCmxUj17Ojipm22t4q2Wmttcen\nVcnObLrbxn4VZx3v/3pzWbLzcXrm/gOe558lOztXL6Puv463X97VPNPe8b3//oCH87eSnf2Ld/cf\nsLg9KNk5PFl8e39vnAAAQsIJACAknAAAQsIJACAknAAAQsIJACAknAAAQsIJACAknAAAQsIJACAk\nnAAAQsIJACA06WO/IggA8E944wQAEBJOAAAh4QQAEBJOAAAh4QQAEBJOAAAh4QQAEBJOAAAh4QQA\nEBJOAAAh4QQAEBJOAAAh4QQAEBJOAAAh4QQAEBJOAAAh4QQAEBJOAAAh4QQAEBJOAAAh4QQAEBJO\nAAChL5YRNVezKeCwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ef195c47b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    input_color = [color1[idx].tolist(), color2[idx].tolist(), \"N\"]\n",
    "    data = {\n",
    "    'model' : \"default\",\n",
    "    'input' : input_color}\n",
    "    \n",
    "    plt.figure(figsize = (10, 1)) \n",
    "    \n",
    "    \n",
    "    true = [color1[idx], color2[idx], color3[idx]]\n",
    "    bar_true = plot_colors([0.33, 0.33, 0.33], true, h=50, w=300)\n",
    "    plt.subplot(1, 6, 1)\n",
    "    plt.imshow(bar_true)\n",
    "    plt.axis('off')\n",
    "\n",
    "    for rand_idx in range(5):\n",
    "        response = requests.post(url, data = json.dumps(data))\n",
    "        color_3_pre = np.array(response.json()[\"result\"][2])\n",
    "        predict = [color1[idx], color2[idx], color_3_pre]\n",
    "        bar_predict = plot_colors([0.33, 0.33, 0.33], predict, h=50, w=300)\n",
    "\n",
    "        plt.subplot(1, 6, 2 + rand_idx)\n",
    "        plt.imshow(bar_predict)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 색깔 추천 (컬러마인드에서 예측한 값과 가까운 값 , 예측한 값, 컬러마인드 나머지1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  19.32861825,   74.59376488,  101.89400857]), array([ 118.23544433,  136.80332436,  161.93016993]), array([ 225.54908553,  231.38313609,  234.0563475 ])]\n",
      "[array([250, 184, 149]), array([ 166.87835693,  165.21559143,  167.00437927], dtype=float32), array([253, 221, 166])]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAAtCAYAAABh2+rHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAASVJREFUeJzt3TFKA1EUQNGXYFyBgo0WriALEFsR\na8HYWAgWug3BDdvHFVgYHOXCOe3w3h9+cZluVvv9fj8ApKz/+wUA+DnxBggSb4Ag8QYIEm+AIPEG\nCBJvgCDxBggSb4Cgo7845OTuZdH9z9dXi+5/e7pddP/MzMXZ6cGz9ft9334uun9mZnPzetBc/W63\n58eL7p+Z2T0+HDRXv9uP+82i+2dm1pe7758tfjoAv068AYLEGyBIvAGCxBsgSLwBgsQbIEi8AYLE\nGyBIvAGCxBsgaOXv8QA9vrwBgsQbIEi8AYLEGyBIvAGCxBsgSLwBgsQbIEi8AYLEGyBIvAGCxBsg\nSLwBgsQbIEi8AYLEGyBIvAGCxBsgSLwBgsQbIEi8AYLEGyDoC6jSFVOHE7inAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x218bc066198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx =0\n",
    "input_color = np.hstack([color1[idx], color2[idx]]).reshape(1,6)\n",
    "\n",
    "li_color_3_recom = []\n",
    "\n",
    "# true color values\n",
    "true = [color1[idx], color2[idx], color3[idx]]\n",
    "bar_true = plot_colors([0.33, 0.33, 0.33], true, h=50, w=300)\n",
    "\n",
    "# predict color values using dnn\n",
    "color_3_pre = model1.predict(input_color/256)[0]\n",
    "# get color values using colormind\n",
    "li_color_3_colormind = []\n",
    "request_color = [color1[idx].tolist(), color2[idx].tolist(), \"N\"]\n",
    "data = {\n",
    "'model' : \"default\",\n",
    "'input' : request_color}\n",
    "\n",
    "li_similar = []\n",
    "li_color_3_colormind = []\n",
    "for _ in range(3):\n",
    "    response = requests.post(url, data = json.dumps(data))\n",
    "    color_3_colormind = np.array(response.json()[\"result\"][2])\n",
    "    li_similar.append(np.linalg.norm(color_3_pre - color_3_colormind))\n",
    "    li_color_3_colormind.append(color_3_colormind)\n",
    "    \n",
    "li_idx = np.argsort(np.array(li_similar))\n",
    "# recommend color1\n",
    "li_color_3_recom.append(li_color_3_colormind[li_idx[0]])\n",
    "# recommend color2\n",
    "li_color_3_recom.append(color_3_pre)\n",
    "# recommend color3\n",
    "li_color_3_recom.append(li_color_3_colormind[li_idx[1]])\n",
    "\n",
    "# draw color bar\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(bar_true)\n",
    "plt.axis('off')\n",
    "bar_true = plot_colors([0.33, 0.33, 0.33], true, h=50, w=300)\n",
    "for recom_idx in range(3):\n",
    "    recom = [color1[idx], color2[idx],  li_color_3_recom[recom_idx]]\n",
    "    bar_recom = plot_colors([0.33, 0.33, 0.33], recom, h=50, w=300)\n",
    "    plt.subplot(1, 4, 2+recom_idx)\n",
    "    plt.imshow(bar_recom)\n",
    "    plt.axis('off')\n",
    "\n",
    "print(true)\n",
    "print(li_color_3_recom)\n",
    "\n",
    "\n",
    "# predict = [color1[idx], color2[idx], color_3_pre]\n",
    "# bar_predict = plot_colors([0.33, 0.33, 0.33], predict, h=50, w=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommand_color(color1, color2):\n",
    "    '''\n",
    "    recommand 3 colors\n",
    "    input : main color1(RGB), sub color2(RGB)\n",
    "    output : \n",
    "    recommand color1(RGB, nearest colormind from predict), \n",
    "    recommand color2(RGB, predict), \n",
    "    recommand color3(RGB, second colormind from predict)\n",
    "    '''\n",
    "    input_color = np.hstack([color1, color2]).reshape(1,6)\n",
    "\n",
    "    li_color_3_recom = []\n",
    "    # predict color values using dnn\n",
    "    model = load_model(\"model/model1_dnn_normal.hdf5\")\n",
    "    color_3_pre = model.predict(input_color/256)[0]\n",
    "    \n",
    "    # get color values using colormind\n",
    "    li_color_3_colormind = []\n",
    "    request_color = [color1, color2, \"N\"]\n",
    "    data = {\n",
    "    'model' : \"default\",\n",
    "    'input' : request_color}\n",
    "\n",
    "    li_similar = []\n",
    "    li_color_3_colormind = []\n",
    "    \n",
    "    url = 'http://colormind.io/api/'\n",
    "    for _ in range(3):\n",
    "        response = requests.post(url, data = json.dumps(data))\n",
    "        color_3_colormind = np.array(response.json()[\"result\"][2])\n",
    "        li_similar.append(np.linalg.norm(color_3_pre - color_3_colormind))\n",
    "        li_color_3_colormind.append(color_3_colormind)\n",
    "\n",
    "    li_idx = np.argsort(np.array(li_similar))\n",
    "    \n",
    "    c1 = get_hex(li_color_3_colormind[li_idx[0]])\n",
    "    c2 = get_hex(color_3_pre.astype(int))\n",
    "    c3 = get_hex(li_color_3_colormind[li_idx[1]])\n",
    "  \n",
    "    #li_color_3_recom.append(c1, c2, c3)\n",
    "    return c1, c2, c3\n",
    "\n",
    "# predict = [color1[idx], color2[idx], color_3_pre]\n",
    "# bar_predict = plot_colors([0.33, 0.33, 0.33], predict, h=50, w=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('#fab895', '#a6a5a7', '#f4c767')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommand_color([19.32861825, 74.59376488, 101.89400857], [118.23544433, 136.80332436, 161.93016993])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdex(r, g, b):\n",
    "    return '#%02x%02x%02x' % ( r, g, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hex(color): \n",
    "    return '#%02x%02x%02x' % ( color[0], color[1], color[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
